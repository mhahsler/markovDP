---
output: github_document
bibliography: vignettes/references.bib
---

```{r echo=FALSE, results = 'asis'}
pkg <- "markovDP"

source("https://raw.githubusercontent.com/mhahsler/pkg_helpers/main/pkg_helpers.R")
pkg_title(pkg)
```

## Introduction
A Markov decision process (MDP) [@Bellman1957; @Howard1960] is a discrete-time 
stochastic control process. In each time step, an 
agent can perform actions which affect the system (i.e., may cause
the system state to change). The agent's goal 
is to maximize its expected
future rewards that depend on the sequence of system state and the
agent's actions in the future. Solving the MDP means finding 
the optimal (or at least a good) policy
that guides the agent's actions. 

The `markovDP` package provides the infrastructure to work with MDPs in R. 
It also interfaces to the following popular algorithms:

* Dynamic Programming 
  - __Value Iteration__ [@Bellman1957]
  - __Modified Policy Iteration__ [@Howard1960; @Puterman1978]

* __Linear Programming__ [@Manne1960]

* Termporal Differencing
  - __Q-Learning__ [@Watkins1992]
  - __Sarsa__ [@Sutton1998]
  - __Expected Sarsa__ [@Sutton1998]

These implementations follow the description is [@Russell2020] and 
[@Sutton1998].


```{r echo=FALSE, results = 'asis'}
pkg_citation(pkg, 1)
pkg_install(pkg)
```

## Usage

Solving the simple maze from [@Russell2020].
```{r problem}
library("markovDP")
data("Maze")
Maze
gridworld_plot(Maze)
```

```{r solve}
sol <- solve_MDP(model = Maze)
sol
```

Display the value function.

```{r value_function}
plot_value_function(sol)
```

```{r gridworld_plot}
gridworld_plot(sol)
```


## Acknowledgments

Development of this package was supported in part by 
National Institute of Standards and Technology (NIST) under grant number 
[60NANB17D180](https://www.nist.gov/ctl/pscr/safe-net-integrated-connected-vehicle-computing-platform).

## References
