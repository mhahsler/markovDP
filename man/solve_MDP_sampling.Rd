% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_sampling.R
\name{solve_MDP_sampling}
\alias{solve_MDP_sampling}
\title{Solve MDPs using Random-Sampling}
\usage{
solve_MDP_sampling(
  model,
  method = "q_planning",
  horizon = NULL,
  discount = NULL,
  alpha = function(t, n) min(10/n, 1),
  n = 1000,
  Q = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; Composed of the algorithm family abbreviation and the algorithm
separated by \code{:}. The algorithm families can be found in the See Also
section under "Other solvers". The family abbreviation follows \code{solve_MDP_} in the function name.}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{...}{further parameters are passed on to the solver function.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Solve MDPs via random-sampling. Implemented is Sutton and Barto's one-step
random-sampling one-step tabular Q-planning.
}
\details{
Random-sample one-step tabular Q-planning is a simple, not very effective,
planning method shown as an illustration in Chapter 8 of
Sutton and Barto (2018). It randomly selects a
state/action pair and samples the following state \eqn{s'} to
perform a single a one-step update:

\deqn{Q(s,a) \leftarrow Q(s, a) + \alpha * (R + \gamma \max_a'(Q(s', a')) - Q(s, a))}
}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.
}
\seealso{
Other solver: 
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_SGD}()},
\code{\link{solve_MDP_TD}()}
}
\concept{solver}
