% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/act.R
\name{act}
\alias{act}
\alias{act.MDP}
\alias{act.MDPE}
\title{Perform an Action}
\usage{
act(model, state, action, fast = FALSE, ...)

\method{act}{MDP}(model, state, action = NULL, fast = FALSE, ...)

\method{act}{MDPE}(model, state, action, fast = FALSE, ...)
}
\arguments{
\item{model}{an MDP model.}

\item{state}{the current state.}

\item{action}{the chosen action. If the action is not specified (\code{NULL}) and
the MDP model contains a policy, then the action is chosen according to the
policy.}

\item{fast}{logical; if \code{TRUE} then extra state id to label conversions are avoided.}

\item{...}{if action is unspecified, then the additional parameters are
passed on to \code{action()} to determine the action using the model's policy.}
}
\value{
a names list with
the \code{reward} and the next \code{state_prime}.
}
\description{
Performs an action in a state and returns the new state and reward.
}
\examples{
data(Maze)

act(Maze, "s(1,3)", "right")

# solve the maze and then ask for actions using the policy
sol <- solve_MDP(Maze)
act(sol, "s(1,3)")

# make the policy in sol epsilon-soft and ask 10 times for the action
replicate(10, act(sol, "s(1,3)", epsilon = .2))
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other MDPE: 
\code{\link{MDPE}()},
\code{\link{absorbing_states}()},
\code{\link{sample_MDP.MDPE}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{MDPE}
