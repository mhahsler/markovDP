% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_SGD.R
\name{solve_MDP_SGD}
\alias{solve_MDP_SGD}
\alias{add_linear_approx_Q_function}
\alias{approx_Q_value}
\alias{approx_greedy_action}
\alias{approx_greedy_policy}
\title{Episodic Semi-gradient Sarsa}
\usage{
solve_MDP_SGD(
  model,
  method = "semi_gradient_sarsa",
  horizon = NULL,
  discount = NULL,
  alpha = 0.01,
  epsilon = 0.2,
  n = 1000,
  w = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

add_linear_approx_Q_function(model, state_features = NULL)

approx_Q_value(model, state = NULL, action = NULL, w = NULL)

approx_greedy_action(model, s, w = NULL, epsilon = 0)

approx_greedy_policy(model, w = NULL)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods: \code{'semi_gradient_sarsa'}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{alpha}{step size.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies.}

\item{n}{number of episodes used for learning.}

\item{w}{a weight vector}

\item{...}{further parameters are passed on to the solver function.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}

\item{state}{a state (index or name)}

\item{action}{an action (index or name)}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
MDP control using state-value approximation. Semi-gradient Sarsa for
episodic problems.
}
\details{
\subsection{Linear Approximation}{

The state-action value function is approximated by
\deqn{\hat{q}(s,a) = \boldsymbol{w}^\top\phi(s,a),}

where \eqn{\boldsymbol{w} \in \mathbb{R}^n} is a weight vector
and \eqn{\phi: S \times A \rightarrow \mathbb{R}^n}  is a
feature function that
maps each state-action pair to a feature vector.
The gradient of the state-action function is
\deqn{\nabla \hat{q}(s,a,\boldsymbol{w}) = \phi(s,a).}
}

\subsection{State-action Feature Vector Construction}{

For a small number of actions, we can
follow the construction described by Geramifard et al (2013)
which uses a state feature function \eqn{\phi: S \rightarrow \mathbb{R}^{m}}
to construct the complete state-action feature vector.
Here, we also add an intercept term.
The state-action feature vector has length \eqn{1 + |A| \times m}.
It has the intercept and then one component for each action. All these components
are set to zero and only the active action component is set to \eqn{\phi(s)},
where \eqn{s} is the current state.
For example, for the state feature
vector \eqn{\phi(s) = (3,4)} and action \eqn{a=2} out of three possible
actions \eqn{A = \{1, 2, 3\}}, the complete
state-action feature vector is \eqn{\phi(s,a) = (1,0,0,3,4,0,0)}.
The leading 1 is for the intercept and the zeros represent the two not
chosen actions.

This construction is implemented in \code{add_linear_approx_Q_function()}.
}

\subsection{Helper Functions}{

The following helper functions for using approximation are available:
\itemize{
\item \code{approx_Q_value()} calculates approximate Q values given the weights in
the model or specified weights.
\item \code{approx_greedy_action()} uses approximate Q values given the weights in
the model or specified weights to find the the greedy action for a
state.
\item \code{approx_greedy_policy()} calculates the greedy-policy
for the approximate Q values given the weights in
the model or specified weights.
}
}

\subsection{Episodic Semi-gradient Sarsa}{

The implementation follows the algorithm given in Sutton and Barto (2018).
}
}
\examples{
# Example 1: A maze without walls. The step cost is 1. The start is top-left and 
# the goal (+100 reward) is bottom-right. 
# This is the ideal problem for a linear approximation of the Q-function
# using the x/y location as state features.

m <- gw_random_maze(5, wall_prob = 0)

# construct state features as the x/y coordinates in the gridworld
state_features <- t(gw_s2rc(S(m)))
colnames(state_features) <- c("x", "y")
state_features

m <- add_linear_approx_Q_function(m, state_features)

# constructed state-action features (X) and approximate Q function 
# and gradient
m$approx_Q_function 

sol <- solve_MDP_SGD(m, horizon = 100, n = 100, 
                     alpha = 0.01, epsilon = .7)

gw_plot(sol)
gw_matrix(sol, what = "value")

# learned weights and state values
sol$solution$w
 
# extracting approximate Q-values
approx_greedy_action(sol, "s(4,5)")
approx_Q_value(sol, "s(4,5)", "down")
approx_Q_value(sol)

# extracting a greedy policy using the approximate Q-values
approx_greedy_policy(sol)
 
# Example 2: Stuart Russell's 3x4 Maze using approximation
# The wall and the -1 absorbing state make linear approximation 
# using just the position more difficult.
data(Maze)
gw_plot(Maze)

Maze_approx <- add_linear_approx_Q_function(Maze)

sol <- solve_MDP_SGD(Maze_approx, horizon = 100, n = 100, 
                     alpha = 0.01, epsilon = 0.3)
gw_plot(sol)

}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Alborz Geramifard, Thomas J. Walsh, Stefanie Tellex, Girish Chowdhary, Nicholas Roy, and Jonathan P. How. 2013. A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning. Foundations and Trends in Machine Learning 6(4), December 2013, pp. 375-451. \doi{10.1561/2200000042}
}
\seealso{
Other solver: 
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()},
\code{\link{solve_MDP_sampling}()}
}
\concept{solver}
