% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_APPROX_LAMDA.R
\name{solve_MDP_APPROX_LAMBDA}
\alias{solve_MDP_APPROX_LAMBDA}
\title{Eligibility Trace Algorithms with Linear Function Approximation}
\usage{
solve_MDP_APPROX_LAMBDA(
  model,
  method = "sarsa",
  horizon = NULL,
  discount = NULL,
  lambda = 0.1,
  alpha = schedule_exp(0.2, 0.1),
  epsilon = schedule_exp(1, 0.1),
  n,
  w = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods:
\itemize{
\item \code{'Sarsa'} - True online Sarsa(\eqn{\lambda}).
\item \code{'GTD'} - GTD(\eqn{\lambda}).
}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{lambda}{the trace-decay parameter for the an accumulating trace.}

\item{alpha}{step size (learning rate). A scalar value between 0 and 1 or a
\link{schedule}.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies. A scalar value between 0 and 1 or a
\link{schedule}.}

\item{n}{number of episodes used for learning.}

\item{w}{a weight vector}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}
}
\value{
\code{solve_MDP()} returns an object of class MDP or MDPTF which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Implementation of Sarsa(\eqn{\lambda}) and GTD(\eqn{\lambda}) following Sutton & Barto (2018).
}
\details{
Approximation functions are added to models using
\code{add_linear_approx_Q_function()}.
\subsection{Implemented methods}{
\itemize{
\item \code{'Sarsa'} - True online Sarsa(\eqn{\lambda}).
\item \code{'GTD'} - GTD(\eqn{\lambda}).
}
}

\subsection{Schedules}{
\itemize{
\item epsilon schedule: \code{t} is increased by each processed episode.
\item alpha schedule: \code{t} is increased by each processed episode.
}
}
}
\examples{

# Example 1: A Maze without walls
Maze <- gw_maze_MDP(c(5, 5), start = "s(1,1)", goal = "s(5,5)") 

Maze_approx <- add_linear_approx_Q_function(Maze)
sol <- solve_MDP_APPROX_LAMBDA(Maze_approx, horizon = 100, n = 100,
                     lambda = .1, 
                     alpha = schedule_exp(0.2, .1), 
                     epsilon = schedule_exp(1, .1)
                     )
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol)

# Example 2: Stuart Russell's 3x4 Maze
data(Maze)
Maze_approx <- add_linear_approx_Q_function(Maze, 
                        transformation = transformation_fourier_basis, 
                        order = 2)
sol <- solve_MDP_APPROX_LAMBDA(Maze_approx, horizon = 100, n = 200,
                     lambda = 0.1, 
                     alpha = schedule_exp(0.2, .1), 
                     epsilon = schedule_exp(1, .1))
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol)

}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.
}
\seealso{
Other solver: 
\code{\link{schedule}},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()}

Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{reachable_states}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{start}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other MDPTF: 
\code{\link{MDPTF}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{reachable_states}()},
\code{\link{sample_MDP.MDPTF}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX}()},
\code{\link{start}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{MDPTF}
\concept{solver}
