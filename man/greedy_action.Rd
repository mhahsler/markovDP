% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/greedy.R
\name{greedy_action}
\alias{greedy_action}
\alias{greedy_policy}
\title{Greedy Actions and Policies}
\usage{
greedy_action(x, s, Q = NULL, epsilon = 0, prob = FALSE)

greedy_policy(x)
}
\arguments{
\item{x}{a solved MDP model or a Q matrix.}

\item{s}{a state.}

\item{Q}{an optional Q-matrix.}

\item{epsilon}{an \code{epsilon > 0} applies an epsilon-greedy policy.}

\item{prob}{logical; return a probability distribution over the actions.}
}
\value{
\itemize{
\item \code{greedy_action()} returns the action with the highest q-value
for state \code{s}. If \code{prob = TRUE}, then a vector with
the probability for each action is returned.
\item \code{greedy_policy()} returns a data.frame with the policy.
}

\code{greedy_policy()} returns the greedy policy given \code{Q}.
}
\description{
Extract a greedy policy or select a greedy action
from a solved model or a Q matrix.
}
\examples{
data(Maze)
Maze

# create a random policy and calculate q-values
pi_random <- random_policy(Maze)
pi_random

V <- policy_evaluation(Maze, pi_random)
V

# calculate Q values
Q <- Q_values(Maze, V)
Q

# get the greedy policy form the Q values
pi_greedy <- greedy_policy(Q)
pi_greedy
Maze_with_policy <- add_policy(Maze, pi_greedy)
gw_plot(Maze_with_policy, main = "Maze: Greedy Policy")

# find the greedy/ epsilon-greedy action for the top-left corner state 
greedy_action(Maze, "s(1,1)", Q, epsilon = 0, prob = FALSE)
greedy_action(Maze, "s(1,1)", Q, epsilon = 0, prob = TRUE)
greedy_action(Maze, "s(1,1)", Q, epsilon = .1, prob = TRUE)

# we can also specify a model with a policy and use the internal Q-values 
greedy_action(Maze_with_policy, "s(1,1)", epsilon = .1, prob = TRUE)

}
\references{
Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{reachable_states}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX_LAMBDA}()},
\code{\link{start}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other policy: 
\code{\link{Q_values}()},
\code{\link{action}()},
\code{\link{bellman_update}()},
\code{\link{policy}()},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{reward}()},
\code{\link{value_function}()},
\code{\link{visit_probability}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{policy}
