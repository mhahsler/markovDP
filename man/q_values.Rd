% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/q_values.R
\name{q_values}
\alias{q_values}
\alias{greedy_action}
\alias{greedy_policy}
\title{Q-Values and Greedy Policies}
\usage{
q_values(model, U = NULL)

greedy_action(Q, s, epsilon = 0, prob = FALSE)

greedy_policy(Q)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{U}{a vector with value function representing the state utilities
(expected sum of discounted rewards from that point on).
If \code{model} is a solved model, then the state
utilities are taken from the solution.}

\item{Q}{an action value function with Q-values as a state by action matrix.}

\item{s}{a state.}

\item{epsilon}{an \code{epsilon > 0} applies an epsilon-greedy policy.}

\item{prob}{logical; return a probability distribution over the actions.}
}
\value{
\code{q_values()} returns a state by action matrix specifying the Q-function,
i.e., the action value for executing each action in each state. The Q-values
are calculated from the value function (U) and the transition model.

\code{greedy_action()} returns the action with the highest q-value
for state \code{s}. If \code{prob = TRUE}, then a vector with
the probability for each action is returned.

\code{greedy_policy()} returns the greedy policy given \code{Q}.
}
\description{
Implementation several functions useful to deal with Q-values for MDPs
which maps a state/action pair to a utility value.
}
\details{
Implemented functions are:
\itemize{
\item \code{q_values()} calculates (approximates)
Q-values for a given model and value function using the Bellman
optimality equation:

\deqn{q(s,a) = \sum_{s'} T(s'|s,a) [R(s,a) + \gamma U(s')]}

Q-values are calculated if \eqn{U = U^*}, the optimal value function
otherwise we get an approximation.
Q-values can be used as the input for several other functions.
\item \code{greedy_action()} returns the action with the largest Q-value given a
state.
\item \code{greedy_policy()}
generates a greedy policy using Q-values.
}
}
\examples{
data(Maze)
Maze

# create a random policy and calculate q-values
pi_random <- random_policy(Maze)
u <- policy_evaluation(Maze, pi_random)
q <- q_values(Maze, U = u)

# get the greedy policy form the q-values
pi_greedy <- greedy_policy(q)
pi_greedy
gridworld_plot(add_policy(Maze, pi_greedy), main = "Maze: Greedy Policy")

greedy_action(q, "s(3,1)", epsilon = 0, prob = FALSE)
greedy_action(q, "s(3,1)", epsilon = 0, prob = TRUE)
greedy_action(q, "s(3,1)", epsilon = .1, prob = TRUE)
}
\references{
Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{actions}},
\code{\link{add_policy}()},
\code{\link{bellman_update}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_and_absorbing}},
\code{\link{value_function}()}

Other policy: 
\code{\link{action}()},
\code{\link{add_policy}()},
\code{\link{bellman_update}()},
\code{\link{policy}()},
\code{\link{policy_evaluation}()},
\code{\link{reward}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{policy}
