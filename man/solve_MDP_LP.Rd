% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_LP.R
\name{solve_MDP_LP}
\alias{solve_MDP_LP}
\title{Solve MDPs using Linear Programming}
\usage{
solve_MDP_LP(
  model,
  method = "LP",
  horizon = NULL,
  discount = NULL,
  inf = 1000,
  lpSolve_args = list(),
  ...,
  matrix = NULL,
  continue = FALSE,
  verbose = FALSE,
  progress = NULL
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods: \code{'LP'}}

\item{horizon}{Only infinite-horizon MDPs with \code{horizon = Inf} are supported.}

\item{discount}{only undiscounted MDPs with \code{discount = 1} are supported.}

\item{inf}{value used for infinity when calling \code{lpSolve::lp()}. This
should me much larger than the largest absolute
reward in the model.}

\item{lpSolve_args}{a list with additional arguments passed on to \code{lpSolve::lp()}.}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}

\item{progress}{not supported by this solver.}
}
\value{
\code{solve_MDP()} returns an object of class MDP or MDPTF which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements that depend on the used method. Common
elements are:
\itemize{
\item \code{method} with the name of the used method
\item parameters used.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
}
}
\description{
Solve discounted, infinite horizon MDPs via linear programming.
}
\details{
A linear programming formulation was developed by Manne (1960) and further
described by Puterman (1996). For the
optimal value function, the Bellman equation holds:

\deqn{
 v^*(s) = \max_{a \in \mathcal{A}}\sum_{s' \in  \mathcal{S}} p(s, a, s') [ r(s, a, s') + \gamma v^*(s')]\; \forall a\in \mathcal{A}, s \in \mathcal{S}
}

The maximization problem can reformulate as a minimization with
a linear constraint for each state action pair.
The optimal value function can be found by solving the
following linear program:
\deqn{\text{min} \sum_{s\in S} v(s)}
subject to
\deqn{v(s) \ge \sum_{s' \in \mathcal{S}} p(s, a, s')[r(s, a, s') + \gamma v(s')],\; \forall a\in \mathcal{A}, s \in \mathcal{S}
}

Note:
\itemize{
\item The discounting factor has to be strictly less than 1.
\item The used solver does not support infinity and a sufficiently large
value needs to be used instead (see parameter \code{inf}).
\item Additional parameters to to \code{solve_MDP} are passed on to \code{\link[lpSolve:lp]{lpSolve::lp()}}.
}
}
\examples{
data(Maze)

# we change the discount to 0.9 since LP is only implemented for discounted MDPs.  
maze_solved <- solve_MDP(Maze, discount = 0.9, method = "LP:LP", verbose = TRUE)
maze_solved
policy(maze_solved)

}
\references{
Manne, Alan. 1960. "On the Job-Shop Scheduling Problem." Operations Research 8 (2): 219-23. \doi{10.1287/opre.8.2.219}.

Puterman, Martin L. 1996. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons.
}
\seealso{
Other solver: 
\code{\link{schedule}},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()}
}
\concept{solver}
