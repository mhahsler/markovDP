% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/regret.R
\name{regret}
\alias{regret}
\title{Calculate the Regret of a Policy}
\usage{
regret(policy, benchmark, start = NULL)
}
\arguments{
\item{policy}{a solved MDP containing the policy to calculate the regret for.}

\item{benchmark}{a solved MDP with the (optimal) policy. Regret is calculated relative to this
policy.}

\item{start}{start state distribution. If NULL then the start state of the \code{benchmark} is used.}
}
\value{
the regret as a difference of expected long-term rewards.
}
\description{
Calculates the regret of a policy relative to a benchmark policy.
}
\details{
Regret is defined as \eqn{V^{\pi^*}(s_0) - V^{\pi}(s_0)} with \eqn{V^\pi} representing the expected long-term
state value (represented by the value function) given the policy \eqn{\pi} and the start
state \eqn{s_0}.

Note that for regret usually the optimal policy \eqn{\pi^*} is used as the benchmark.
Since the optimal policy may not be known, regret relative to the best known policy can be used.
}
\examples{
data(Maze)

sol_optimal <- solve_MDP(Maze)
policy(sol_optimal)

# a manual policy (go up and in some squares to the right)
acts <- rep("up", times = length(Maze$states))
names(acts) <- Maze$states
acts[c("s(1,1)", "s(1,2)", "s(1,3)")] <- "right"
U <- policy_evaluation(Maze, manual_policy(Maze, acts))

sol_manual <- add_policy(Maze, manual_policy(Maze, acts, U = TRUE))
policy(sol_manual)

regret(sol_manual, benchmark = sol_optimal)
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{actions}()},
\code{\link{add_policy}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{q_values}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_and_absorbing}},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
