% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/regret.R
\name{regret}
\alias{regret}
\alias{action_discrepancy}
\alias{value_error}
\title{Regret of a Policy and Related Measures}
\usage{
regret(policy, benchmark, start = NULL, ...)

action_discrepancy(policy, benchmark, proportion = FALSE)

value_error(policy, benchmark, type = "RMSVE")
}
\arguments{
\item{policy}{a solved MDP containing the policy to calculate the regret for.}

\item{benchmark}{a solved MDP with the (optimal) policy. Regret is calculated relative to this
policy.}

\item{start}{start state distribution. If NULL then the start state of the \code{benchmark} is used.}

\item{...}{further arguments are passed on to \code{\link[=reward]{reward()}}.}

\item{proportion}{logical; should the action discrepancy be reported as a proportion
of states with a different action.}

\item{type}{type of error root mean square value error (\code{"RMSVE"}), mean square value error (\code{"MSVE"}),
mean absolute value error (\code{"MAVE"}), absolute value error vector (\code{"AVE"}),  value error vector (\code{"VE"}).}
}
\value{
\itemize{
\item \code{regret()} returns the regret as a difference of expected long-term rewards.
\item \code{action_discrepancy()} returns the number or proportion of diverging actions.
\item \code{mean_value_error()} returns the mean squared or absolute difference in the
value function.
}
}
\description{
Calculates the regret and related measures for a policy relative to a benchmark policy.
}
\details{
\subsection{Regret}{

Regret for a policy \eqn{\pi} is defined as
\eqn{V_\pi(s_0) - v_*(s_0)}.
\eqn{V_\pi(s_0)} representing the expected long-term
state value for following policy \eqn{\pi} and the starting
in state \eqn{s_0} (or a start distribution).

Note that for regret, usually the optimal policy \eqn{\pi^*} is used as the benchmark.
Since the optimal policy may not be known, regret relative to the best known policy can be used.

Regret is only valid with converged value functions. This means that either the
solver has converged, or the value function was estimated for the policy using
converged \code{\link[=policy_evaluation]{policy_evaluation()}}.
}

\subsection{Action Discrepancy}{

The action discrepancy is the number of states in the policy for which the
prescribed action in the policy differs. This implementation calculates
the Q matrix for the benchmark to make sure that actions with the same Q value
are considers as correct.
}

\subsection{Root Mean Squared Value Error}{

The root mean value error \eqn{\sqrt{\text{VE}} = \sqrt{||V - v_*||^2}}
is the sum of the squared differences in
state values between the two solution and the optimal value function. For \eqn{v_*}, the
value function of the benchmark solution is used.
}
}
\examples{
data(Maze)

sol_optimal <- solve_MDP(Maze)
policy(sol_optimal)

# a manual policy (go up and in some squares to the right)
acts <- rep("up", times = length(Maze$states))
names(acts) <- Maze$states
acts[c("s(1,1)", "s(1,2)", "s(1,3)")] <- "right"

sol_manual <- add_policy(Maze, manual_policy(Maze, acts, estimate_V = TRUE))
policy(sol_manual)

regret(sol_manual, benchmark = sol_optimal)

action_discrepancy(sol_manual, benchmark = sol_optimal)

value_error(sol_manual, benchmark = sol_optimal, type = "VE")
value_error(sol_manual, benchmark = sol_optimal, type = "MAVE")
value_error(sol_manual, benchmark = sol_optimal, type = "RMSVE")
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
