% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_TD.R
\name{solve_MDP_TD}
\alias{solve_MDP_TD}
\title{Solve MDPs using Tabular Temporal Differencing}
\usage{
solve_MDP_TD(
  model,
  method = "sarsa",
  horizon = NULL,
  discount = NULL,
  alpha = schedule_exp(0.2, 0.01),
  epsilon = schedule_exp(1, 0.1),
  n_step = 1,
  lambda = 0,
  on_policy = TRUE,
  n,
  Q = 0,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods:
\code{"sarsa"}, \code{"q"}, \code{"q_learning"}, or \code{"expected_sarsa"}. Not all methods are available
for all setting s of \code{n_step} and \code{lambda} (see Details).}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{alpha}{step size (learning rate). A scalar value between 0 and 1 or a
\link{schedule}.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies. A scalar value between 0 and 1 or a
\link{schedule}.}

\item{n_step}{number of steps to look ahead for n-step Sarsa.}

\item{lambda}{eligibility trace decay factor for Sarsa(lambda).}

\item{on_policy}{logical; should we learn on-policy (the behavior policy)
vs. off-policy (using importance sampling)? Only used for method \code{"sarsa"}.}

\item{n}{number of episodes used for learning.}

\item{Q}{an initial state-action value matrix. By default an all 0 matrix is
used.}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}
}
\value{
\code{solve_MDP()} returns an object of class MDP or MDPTF which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements that depend on the used method. Common
elements are:
\itemize{
\item \code{method} with the name of the used method
\item parameters used.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
}
}
\description{
Solve MDPs using tabular temporal difference control
methods like q-learning and 1-step, n-step Sarsa, Sarsa(\eqn{\lambda}),
and Q(\eqn{\lambda}).
}
\details{
Implemented are several tabular temporal difference control methods
described in Sutton and Barto (2018).
Note that the MDP transition and reward models are used
for these reinforcement learning methods only to sample from
the environment.

The implementation uses an \eqn{\epsilon}-greedy behavior policy,
where the parameter \code{epsilon} controls the degree of exploration.
The algorithms use a step size parameter \eqn{\alpha} (learning rate).
The \code{epsilon} and the learning rate \code{alpha} can be specified as
fixed numbers between 0 and 1 or as a schedule function where the
value is gradually reduced (see Schedule section below).

All temporal differencing methods us the following update:
\deqn{
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [G_t - Q(S_t,A_t)],
}
where \eqn{G_t} is the target estimate for the given q-value. The different methods below
estimate the target value differently (see 1 and n-step methods below).

If the model has absorbing states to terminate episodes, then no maximal episode length
(\code{horizon}) needs to
be specified. To make sure that the algorithm does finish in a reasonable amount of time,
episodes are stopped after 1000 actions (with a warning). For models without absorbing states,
the episode length has to be specified via \code{horizon}.
\subsection{1-step Methods}{

1-step methods estimate the return by just looking one state ahead.
\itemize{
\item \code{q_learning}: \strong{Q-Learning} (Watkins and Dayan 1992) is an off-policy (learns a greedy target policy)
temporal difference method. We use here an \eqn{\epsilon}-greedy behavior policy.
The update target value is estimated by one-step bootstrapping using the
reward and the value of the state following the current greedy target policy:
\deqn{G_t = R_{t+1} + \gamma \max_a Q(S_{t+1}, a)}
\item \code{sarsa}: \strong{Sarsa} (Rummery and Niranjan 1994) is an on-policy method (behavior and
target policy are the same). We use an \eqn{\epsilon}-greedy policy
and the final \eqn{\epsilon}-greedy policy is converted into a greedy policy.
\eqn{\epsilon} can be lowered over time (see \link{schedule} and parameter \code{continue})
to learn a approximately greedy policy. The target is estimated
as the one-step bootstrap estimate following the current behavior policy:
\deqn{G_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})}
\item \code{expected_sarsa} \strong{Expected Sarsa} (Sutton and Barto 2018) learns the behavior policy
(on-policy learning).
We use Sarsa with an \eqn{\epsilon}-greedy policy which uses the
the expected value under the current policy for the update:
\deqn{G_t = R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a)}

Expected Sarsa moves in the same direction as Sarsa would
move in expectation.
Because it uses the expectation, we can
set the step size \eqn{\alpha} to large values and 1 is common.

The Q-learning algorithm can be seen as a simplification of the
off-policy version of expected Sarsa.
}
}

\subsection{n-step Methods}{

n-step methods use a longer look ahead. The
return is estimated by looking
\eqn{n} time steps ahead (\code{n_step} in the code) and using
the rewards and then the value of the reached state:

\deqn{
  G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})
}

While n-step methods conceptually look ahead, the implementation has to wait
for the values to become available. This means that updates are \eqn{n}
steps delayed, i.e., the update for step \eqn{t} is performed at \eqn{t+n}.

Methods:
\itemize{
\item \code{sarsa}: \strong{n-step Sarsa} (Sutton and Barto 2018).
The estimated return is used as the update target for Sarsa.

\code{n_step = 1} is regular 1-step Sarsa. Using \code{n_step = Inf} is equivalent to
Monte Carlo Control, however, \code{\link[=solve_MDP_MC]{solve_MDP_MC()}} is more memory efficient.

Sarsa learns on-policy (i.e., the behavioral policy which is often
an exploring \eqn{\epsilon}-greedy policy). For the off-policy case,
when the optimal policy is learned, the update are corrected using the
importance sampling ratio.
}
}

\subsection{Eligibility Traces}{

Eligibility traces also look ahead but the update is performed in a backward looking manner
by using a memory parameter called the trace that remember what states or Q-values lead to the
current event. We focus here on tabular Sarsa(\eqn{\lambda})

From the forward-view perspective, the \eqn{\lambda}-reward can be seen as
an average of infinite n-step backups:
\deqn{G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} R_t^{(n)},}
where \eqn{\lambda} is the eligibility trace decay factor. The implementation
uses the backward view with the TD-error
\deqn{\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_{t}, A_{t})}
and the update
\deqn{Q(s,a) = Q(s,a) + \alpha \delta_t z_t(s,a) \qquad \forall s, a.}

The trace is updated at each time step as
\deqn{z_t(s,a) = \gamma z_{t-1}(s,a) \qquad \forall s, a}
and \eqn{z_t(S_t,A_t) = z_t(S_t,A_t) + 1}. This means, the trace is set to one
for the current state and action and then decays with the parameter \eqn{\lambda}.

Methods:
\itemize{
\item \code{sarsa}: \strong{Sarsa(lambda)} (Sutton and Barto 2018).
\item \code{q}: \strong{Watkins's Q(lambda)} (Sutton and Barto 2018). An off-policy
learning algorithm similar to Q-learning which looks only ahead as far as the next
exploratory action (i.e., the eligibility vector \eqn{z_t(\cdot,\cdot)}
is set to zero whenever an exploratory
non-greedy exploratory action is taken).
}
}

\subsection{Schedules}{
\itemize{
\item epsilon schedule: \code{t} is increased by each processed episode.
\item alpha schedule: \code{t} is set to the number of times the a Q-value for state
\code{s} was updated.
}
}
}
\examples{
data(Maze)

# Example 1: Learn a Policy using Q-Learning
maze_learned <- solve_MDP(Maze, method = "TD:q_learning",
    n = 200, horizon = 100)
maze_learned

policy(maze_learned)
gw_plot(maze_learned)

# Example 2: Learn a Policy using 1-step Sarsa
maze_learned <- solve_MDP(Maze, method = "TD:sarsa",
    n = 200, horizon = 100)
maze_learned

policy(maze_learned)
gw_plot(maze_learned)

# Example 3: Perform one episode for 3-step Sarsa

# run one episode in verbose mode.
maze_learned <- solve_MDP(Maze, method = "TD:sarsa",
    n_step = 5, n = 1, horizon = 100, verbose = 2)
    
# verbose output:
#  * tau ... time step updated (laggs n_step)
#  * -> ... update of the Q-value
#  * N ... number of updates for this state
#  * G ... reward estimate for the n_steps
#  * alpha ... learning rate (schedule may depend on N)
#  * rho ... importance sampling ratio (1 for on-policy learning)

# run more episode
maze_learned <- solve_MDP(Maze, method = "TD:sarsa",
    n_step = 5, n = 100, horizon = 100)
maze_learned
policy(maze_learned)
gw_plot(maze_learned)

# Example: Tabular Sarsa(lambda)
maze_learned <- solve_MDP(Maze, method = "TD:sarsa",
    lambda = .1, n = 100, horizon = 100)
maze_learned
policy(maze_learned)
gw_plot(maze_learned)
}
\references{
Rummery, G., and Mahesan Niranjan. 1994. "On-Line Q-Learning Using Connectionist Systems." Techreport CUED/F-INFENG/TR 166. Cambridge University Engineering Department.

Sutton, R. 1988. "Learning to Predict by the Method of Temporal Differences." Machine Learning 3: 9-44. \url{https://link.springer.com/article/10.1007/BF00115009}.

Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Watkins, Christopher J. C. H., and Peter Dayan. 1992. "Q-Learning." Machine Learning 8 (3): 279-92. \doi{10.1007/BF00992698}.
}
\seealso{
Other solver: 
\code{\link{add_linear_approx_Q_function}()},
\code{\link{schedule}},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_MC}()},
\code{\link{solve_MDP_PG}()},
\code{\link{solve_MDP_SAMP}()}
}
\concept{solver}
