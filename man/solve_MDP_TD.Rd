% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_TD.R
\name{solve_MDP_TD}
\alias{solve_MDP_TD}
\alias{solve_MDP_TDN}
\title{Solve MDPs using Temporal Differencing}
\usage{
solve_MDP_TD(
  model,
  method = "q_learning",
  horizon = NULL,
  discount = NULL,
  alpha = function(t, n) min(10/n, 1),
  epsilon = 0.2,
  n,
  Q = 0,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

solve_MDP_TDN(
  model,
  method = "sarsa_on_policy",
  horizon = NULL,
  discount = NULL,
  n_step,
  alpha = function(t, n) min(10/n, 1),
  epsilon = 0.2,
  n,
  Q = 0,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods:
\itemize{
\item for TD: \code{"sarsa"}, \code{"q_learning"}, or \code{"expected_sarsa"}
\item for TDN: \code{"sarsa_on_policy"}, or \code{"sarsa_off_policy"}
}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{alpha}{step size as a function of the time step \code{t} and the number of times
the respective Q-value was updated \code{n} or a scalar. For expected Sarsa, alpha is
often set to 1.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies.}

\item{n}{number of episodes used for learning.}

\item{Q}{an initial state-action value matrix. By default an all 0 matrix is
used.}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}

\item{n_step}{integer; steps for bootstrapping}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Solve MDPs using 1-step and n-step tabular temporal difference control
methods like q-learning and Sarsa.
}
\details{
Implemented are several tabular temporal difference control methods
described in Sutton and Barto (2018).
Note that the MDP transition and reward models are used
for these reinforcement learning methods only to sample from
the environment.

The implementation uses an \eqn{\epsilon}-greedy behavior policy,
where the parameter \code{epsilon} controls the degree of exploration.
The algorithms use a step size parameter \eqn{\alpha} (learning rate).
The learning rate \code{alpha} can be specified as a
function with the signature \verb{function(t, n)}, where \code{t} is the number of episodes
processed and \code{n} is the number of updates for the entry in the Q-table.

The general 1-step update is
\deqn{
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [G_t - Q(S_t,A_t)],
}
where \eqn{G_t} is the target estimate for the given q-value. The different methods below
estimate the target value differently.

If the model has absorbing states to terminate episodes, then no maximal episode length
(\code{horizon}) needs to
be specified. To make sure that the algorithm does finish in a reasonable amount of time,
episodes are stopped after 1000 actions (with a warning). For models without absorbing states,
the episode length has to be specified via \code{horizon}.
\itemize{
\item \strong{Q-Learning} (Watkins and Dayan 1992) is an off-policy temporal difference method that uses
an \eqn{\epsilon}-greedy behavior policy and learns a greedy target
policy. The target value is estimated as the one-step bootstrapping using the
target greedy policy:
\deqn{G_t = R_{t+1} + \gamma \max_a Q(S_{t+1}, a)}
\item \strong{Sarsa} (Rummery and Niranjan 1994) is an on-policy method that follows and learns
the same policy. Here a an \eqn{\epsilon}-greedy policy is used.
The final \eqn{\epsilon}-greedy policy is converted into a greedy policy.
\eqn{\epsilon} can be lowered over time (see parameter \code{continue})
to learn a greedy policy. The target is estimated
as the one-step bootstrapping following the behavior policy:
\deqn{G_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})}
\item \strong{Expected Sarsa} (Sutton and Barto 2018). We implement an on-policy
Sarsa with an \eqn{\epsilon}-greedy policy which uses the
the expected value under the current policy for the update.
It moves deterministically in the same direction as Sarsa would
move in expectation.
\deqn{G_t = R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a)}

Because it uses the expectation, we can
set the step size \eqn{\alpha} to large values and 1 is common.
The off-policy use of expected Sarsa simplifies to
the Q-learning algorithm.
\item \strong{On and off-policy n-step Sarsa} (Sutton and Barto 2018).
Estimate the return using the last \eqn{n} time steps:
\deqn{
  G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})
  }

\eqn{n = 1} is regular 1-step Sarsa. Uaing \eqn{n = \inf} is equivalent to
Monte Carlo Control, howver, \code{solve_MDP_MC()} is more memory efficient.

This estimate is used as the target for Sarsa. For the off-policy case,
the update uses the importance sampling ratio. Note that updates are delayed
\eqn{n} steps in this backward looking algorithm.
}
}
\examples{
data(Maze)

# Example 1: Learn a Policy using Q-Learning
maze_learned <- solve_MDP(Maze, method = "TD:q_learning",
    epsilon = 0.2, n = 500, horizon = 100, verbose = TRUE)
maze_learned

policy(maze_learned)
plot_value_function(maze_learned)
gw_plot(maze_learned)

# Keep on learning, but with a reduced epsilon
maze_learned <- solve_MDP(maze_learned, method = "TD:q_learning",
    epsilon = 0.01, n = 500, horizon = 100, continue = TRUE, verbose = TRUE)

policy(maze_learned)
plot_value_function(maze_learned)
gw_plot(maze_learned)

# Example 2: n-step Sarsa
maze_learned <- solve_MDP(Maze, method = "TDN:sarsa_on_policy",
    n_step = 3, n = 10, horizon = 100, verbose = TRUE)
maze_learned

gw_plot(maze_learned)
}
\references{
Rummery, G., and Mahesan Niranjan. 1994. "On-Line Q-Learning Using Connectionist Systems." Techreport CUED/F-INFENG/TR 166. Cambridge University Engineering Department.

Sutton, R. 1988. "Learning to Predict by the Method of Temporal Differences." Machine Learning 3: 9-44. \url{https://link.springer.com/article/10.1007/BF00115009}.

Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Watkins, Christopher J. C. H., and Peter Dayan. 1992. "Q-Learning." Machine Learning 8 (3): 279-92. \doi{10.1007/BF00992698}.
}
\seealso{
Other solver: 
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_SAMP}()}
}
\concept{solver}
