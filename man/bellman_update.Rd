% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bellman_operator.R
\name{bellman_update}
\alias{bellman_update}
\alias{bellman_operator}
\title{Bellman Update and Bellman operator}
\usage{
bellman_update(model, V)

bellman_operator(model, pi, V)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{V}{a vector representing the value function. A single 0 can be
used as a shorthand for a value function with all 0s.}

\item{pi}{a policy as a data.frame with at least columns for states and action. If \code{NULL},
then the policy in model is used.}
}
\value{
a list with the updated state value vector U and the taken actions
pi.
}
\description{
Update the value function with a Bellman update.
}
\details{
The Bellman update updates a value function given the model
by applying the Bellman equation as an update rule for each state:

\deqn{v_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}(s)} \sum_{s'} p(s' | s,a) [r(s,a, s') + \gamma v_k(s')]}

The Bellman update moves the estimated value function \eqn{V} closer to the
optimal value function \eqn{v_*}.

The Bellman operator \eqn{B_\pi} updates a value function given the model,
and a policy \eqn{\pi}:

\deqn{(B_\pi v)(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s'} p(s' | s,a) [r(s,a,s') + \gamma v(s')]}

The Bellman error is \eqn{\delta = B_\pi v - v}.
The Bellman operator reduces the Bellman error and moves the value function
closer to the fixed point of the true value function:

\deqn{v_\pi = B_\pi v_\pi.}
}
\examples{
data(Maze)
Maze

# single Bellman update from a all 0 value function
bellman_update(Maze, V = 0)

# perform simple value iteration for 10 iterations
V <- 0
for (i in seq(10))
  V <- bellman_update(Maze, V)$V
  
V
}
\references{
Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other policy: 
\code{\link{Q_values}()},
\code{\link{action}()},
\code{\link{greedy_action}()},
\code{\link{policy}()},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{reward}()},
\code{\link{value_function}()},
\code{\link{visit_probability}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{policy}
