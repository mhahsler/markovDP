% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bellman_update.R
\name{bellman_update}
\alias{bellman_update}
\title{Bellman Update}
\usage{
bellman_update(model, U, return_Q = FALSE)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{U}{a vector with value function representing the state utilities
(expected sum of discounted rewards from that point on). A single 0 can be
used as a shorthand for a value function with all 0s.}

\item{return_Q}{logical; return also the Q matrix.}
}
\value{
a list with the updated state value vector U and the taken actions
pi.
}
\description{
Update the value function with a Bellman update.
}
\details{
The Bellman updates a value function given the model defining
\eqn{T}, \eqn{\gamma} and \eqn{R}
by applying the Bellman equation as an update rule for each state:

\deqn{U_{k+1}(s) \leftarrow \max_{a \in A(s)} \sum_{s'} T(s' | s,a) [R(s,a) + \gamma U_k(s')]}
}
\examples{
data(Maze)
Maze

# single Bellman update from a all 0 value function
bellman_update(Maze, U = 0)

# perform simple value iteration for 10 iterations
U <- list(U = 0)
for (i in seq(10))
  U <- bellman_update(Maze, U$U)
  
U
}
\references{
Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{actions}},
\code{\link{add_policy}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{q_values}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_and_absorbing}},
\code{\link{value_function}()}

Other policy: 
\code{\link{action}()},
\code{\link{add_policy}()},
\code{\link{policy}()},
\code{\link{policy_evaluation}()},
\code{\link{q_values}()},
\code{\link{reward}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{policy}
