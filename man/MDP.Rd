% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MDP.R
\name{MDP}
\alias{MDP}
\alias{S}
\alias{A}
\alias{is_solved_MDP}
\alias{is_converged_MDP}
\alias{P_}
\alias{R_}
\title{Define an MDP Problem}
\usage{
MDP(
  states,
  actions,
  transition_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  start = "uniform",
  info = NULL,
  name = NA
)

S(model)

A(model)

is_solved_MDP(model, allow_MDPE = FALSE, stop = FALSE)

is_converged_MDP(model, stop = FALSE)

P_(action = NA, start.state = NA, end.state = NA, probability)

R_(action = NA, start.state = NA, end.state = NA, value)
}
\arguments{
\item{states}{a character vector specifying the names of the states.}

\item{actions}{a character vector specifying the names of the available
actions.}

\item{transition_prob}{Specifies the transition probabilities between
states.}

\item{reward}{Specifies the rewards dependent on action and states.}

\item{discount}{numeric; discount rate between 0 and 1.}

\item{horizon}{numeric; Number of epochs. \code{Inf} specifies an infinite
horizon.}

\item{start}{Specifies in which state the MDP starts.}

\item{info}{A list with additional information.}

\item{name}{a string to identify the MDP problem.}

\item{model}{an \code{MDP} object.}

\item{allow_MDPE}{logical; also accept \link{MDPE} objects.}

\item{stop}{logical; stop with an error.}

\item{action}{action as a action label or integer. The value \code{NA} matches any action.}

\item{start.state, end.state}{state as a state label or an integer. The value \code{NA} matches any state.}

\item{probability, value}{Values used in the helper functions \code{P_()} and \code{R_()}.}
}
\value{
The function returns an object of class MDP which is list with
the model specification. \code{\link[=solve_MDP]{solve_MDP()}} reads the object and adds a list element called
\code{'solution'}.
}
\description{
Defines all the elements of a discrete-time finite state-space MDP problem.
}
\details{
Markov decision processes (MDPs) are discrete-time stochastic control
process. We implement here MDPs with a finite state space.
\code{MDP()} defines all the element of an MDP problem including the discount rate, the
set of states, the set of actions,the transition
probabilities, and the rewards.

In the following we use the following notation. The MDP is a 5-duple:

\eqn{(S,A,P,R, \gamma)}.

\eqn{S} is the set of states; \eqn{A}
is the set of actions; \eqn{P} are the conditional transition probabilities
between states; \eqn{R} is the reward function; and
\eqn{\gamma} is the discount factor. We will use lower case letters to
represent a member of a set, e.g., \eqn{s} is a specific state. To refer to
the size of a set we will use cardinality, e.g., the number of actions is
\eqn{|A|}.
\subsection{Names used for mathematical symbols in code}{
\itemize{
\item \eqn{S, s, s'}: \verb{'states', start.state', 'end.state'}
\item \eqn{A, a}: \verb{'actions', 'action'}
}

State names and actions can be specified as strings or index numbers
(e.g., \code{start.state} can be specified as the index of the state in \code{states}).
For the specification as data.frames below, \code{NA} can be used to mean
any  \code{start.state}, \code{end.state} or \code{action}.
}

\subsection{Specification of transition model: \eqn{P(s' | s, a)}}{

Transition probability to transition to state \eqn{s'} from given state \eqn{s}
and action \eqn{a}. The transition probabilities can be
specified in the following ways:
\itemize{
\item A data.frame with columns exactly like the arguments of \code{P_()}.
You can use \code{rbind()} with helper function \code{P_()} to create this data
frame. Probabilities can be specified multiple times and the definition that
appears last in the data.frame will take affect.
\item A named list of matrices, one for each action. Each matrix is square with
rows representing start states \eqn{s} and columns representing end states \eqn{s'}.
Instead of a matrix, also the strings \code{'identity'} or \code{'uniform'} can be specified.
\item A function with the following arguments:
\itemize{
\item A function with the argument list \code{model}, \code{action}, \code{start.state}, \code{end.state}
which returns a single transition probability.
\item A function with the argument list \code{model}, \code{action}, \code{start.state}
which returns a transition probability vector for all end states. This vector can
be dense, a \link[Matrix:sparseVector]{Matrix::sparseVector} or a named vector only containing
the non-zero probabilities named by the corresponding end state.
}

The arguments \code{action}, \code{start.state}, and \code{end.state} will be always
called with the state names as a character vectors of length 1.
}
}

\subsection{Specification of the reward function: \eqn{R(a, s, s')}}{

The reward function can be specified in the following
ways:
\itemize{
\item A data frame with columns named exactly like the arguments of \code{R_()}.
You can use \code{rbind()}
with helper function \code{R_()} to create this data frame. Rewards can be specified
multiple times and the definition that
appears last in the data.frame will take affect.
\item A named list of matrices, one for each action. Each matrix is square with
rows representing start states \eqn{s} and columns representing end states \eqn{s'}.
\item A function following the same rules as for transition probabilities.
}

To avoid overflow problems with rewards, reward values should stay well within the
range of
\verb{[-1e10, +1e10]}. \code{-Inf} can be used as the reward for unavailable actions and
will be translated into a large negative reward for solvers that only support
finite reward values.
}

\subsection{Specification of the Start State}{

The start state of the agent can be a single state or a distribution over the states.
The start state definition is used as the default when the reward is calculated by \code{\link[=reward]{reward()}}
and for sampling with \code{\link[=sample_MDP]{sample_MDP()}}.

Options to specify the start state are:
\itemize{
\item A string specifying the name of a single starting state.
\item An integer in the range \eqn{1} to \eqn{n} to specify the index of a single starting state.
\item The string \code{"uniform"} where the start state is chosen using a uniform distribution over all states.
\item A probability distribution over the states. That is, a vector
of \eqn{|S|} probabilities, that add up to \eqn{1}.
}

The default state state is a uniform
distribution over all states.
}

\subsection{Accessing Elements of the MDP}{

The convenience functions \code{S()} and \code{A()} return the set of states and actions.

See \link{accessors} for accessing transition probabilities, rewards, and the
start state distribution.
}
}
\examples{
# simple MDP example
#
# states:    s1 s2 s3 s4
# transitions: forward moves -> and backward moves <-
# start: s1
# reward: s1, s2, s4 = 0 and s3 = 1

car <- MDP(
  states = c("s1", "s2", "s3", "s4"),
  actions = c("forward", "back", "stop"),
  transition <- list(
    forward = rbind(c(0, 1, 0, 0), 
                    c(0, 0, 1, 0), 
                    c(0, 0, 0, 1), 
                    c(0, 0, 0, 1)),
    back =    rbind(c(1, 0, 0, 0), 
                    c(1, 0, 0, 0), 
                    c(0, 1, 0, 0), 
                    c(0, 0, 1, 0)),
    stop = "identity"
  ),
  reward = rbind(
    R_(value = 0),
    R_(end.state = "s3", value = 1)
  ),
  discount = 0.9,
  start = "s1",
  name = "Simple Car MDP"
)

car

# internal representation
str(car)

# accessing elements
S(car)
A(car)
start_vector(car, sparse = "states")
transition_matrix(car)
transition_matrix(car, sparse = TRUE)
reward_matrix(car)
reward_matrix(car, sparse = TRUE)

sol <- solve_MDP(car)
sol

policy(sol)
}
\seealso{
Other MDP: 
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other MDP_examples: 
\code{\link{Cliff_walking}},
\code{\link{DynaMaze}},
\code{\link{Maze}},
\code{\link{Windy_gridworld}}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{MDP_examples}
