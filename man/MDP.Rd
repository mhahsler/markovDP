% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MDP.R
\name{MDP}
\alias{MDP}
\alias{is_solved_MDP}
\alias{T_}
\alias{R_}
\alias{epoch_to_episode}
\alias{normalize_MDP}
\title{Define an MDP Problem}
\usage{
MDP(
  states,
  actions,
  transition_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  start = "uniform",
  info = NULL,
  name = NA
)

is_solved_MDP(x, stop = FALSE)

T_(action = NA, start.state = NA, end.state = NA, probability)

R_(action = NA, start.state = NA, end.state = NA, value)

epoch_to_episode(x, epoch)

normalize_MDP(
  model,
  sparse = TRUE,
  trans_keyword = TRUE,
  trans_function = TRUE,
  keep_reward_df = FALSE,
  cache_absorbing_unreachable = TRUE,
  progress = TRUE
)
}
\arguments{
\item{states}{a character vector specifying the names of the states.}

\item{actions}{a character vector specifying the names of the available
actions.}

\item{transition_prob}{Specifies the transition probabilities between
states.}

\item{reward}{Specifies the rewards dependent on action and states.}

\item{discount}{numeric; discount rate between 0 and 1.}

\item{horizon}{numeric; Number of epochs. \code{Inf} specifies an infinite
horizon.}

\item{start}{Specifies in which state the MDP starts.}

\item{info}{A list with additional information.}

\item{name}{a string to identify the MDP problem.}

\item{stop}{logical; stop with an error.}

\item{action}{action as a action label or integer. The value \code{NA} matches any action.}

\item{start.state, end.state}{state as a state label or an integer. The value \code{NA} matches any state.}

\item{probability, value}{Values used in the helper functions \code{T_()} and \code{R_()}.}

\item{epoch}{integer; an epoch that should be converted to the
corresponding episode in a time-dependent MDP.}

\item{model, x}{a \code{MDP} object.}

\item{sparse}{logical; use sparse representation. matrices when the density is below 50\% and keeps data.frame representation
for the reward field.}

\item{trans_keyword}{logical; convert distribution keywords (uniform and identity)
in \code{transition_prob} matrices?}

\item{trans_function}{logical; convert functions into matrices?}

\item{keep_reward_df}{logical; if reward is a data.frame, then keep it.}

\item{cache_absorbing_unreachable}{logical; should absorbing and unreachable states be precalculated?}

\item{progress}{logical; show a progress bar with estimated time for completion.}
}
\value{
The function returns an object of class MDP which is list with
the model specification. \code{\link[=solve_MDP]{solve_MDP()}} reads the object and adds a list element called
\code{'solution'}.
}
\description{
Defines all the elements of a discrete-time finite state-space MDP problem.
}
\details{
Markov decision processes (MDPs) are discrete-time stochastic control
process. We implement here MDPs with a finite state space.
\code{MDP()} defines all the element of an MDP problem including the discount rate, the
set of states, the set of actions,the transition
probabilities, and the rewards.

In the following we use the following notation. The MDP is a 5-duple:

\eqn{(S,A,T,R, \gamma)}.

\eqn{S} is the set of states; \eqn{A}
is the set of actions; \eqn{T} are the conditional transition probabilities
between states; \eqn{R} is the reward function; and
\eqn{\gamma} is the discount factor. We will use lower case letters to
represent a member of a set, e.g., \eqn{s} is a specific state. To refer to
the size of a set we will use cardinality, e.g., the number of actions is
\eqn{|A|}.
\subsection{Names used for mathematical symbols in code}{
\itemize{
\item \eqn{S, s, s'}: \verb{'states', start.state', 'end.state'}
\item \eqn{A, a}: \verb{'actions', 'action'}
}

State names and actions can be specified as strings or index numbers
(e.g., \code{start.state} can be specified as the index of the state in \code{states}).
For the specification as data.frames below, \code{NA} can be used to mean
any  \code{start.state}, \code{end.state} or \code{action}.
}

\subsection{Specification of transition model: \eqn{T(s' | s, a)}}{

Transition probability to transition to state \eqn{s'} from given state \eqn{s}
and action \eqn{a}. The transition probabilities can be
specified in the following ways:
\itemize{
\item A data.frame with columns exactly like the arguments of \code{T_()}.
You can use \code{rbind()} with helper function \code{T_()} to create this data
frame. Probabilities can be specified multiple times and the definition that
appears last in the data.frame will take affect.
\item A named list of matrices, one for each action. Each matrix is square with
rows representing start states \eqn{s} and columns representing end states \eqn{s'}.
Instead of a matrix, also the strings \code{'identity'} or \code{'uniform'} can be specified.
\item One of the following functions:
\itemize{
\item A function with the argument list \code{model}, \code{action}, \code{start.state}, \code{end.state}
which returns a single transition probability.
\item A function with the argument list \code{model}, \code{action}, \code{start.state}
which returns a transition probability vector for all end states. This vector can
be dense, a \link[Matrix:sparseVector]{Matrix::sparseVector} or a named vector only containing
the non-zero probabilities named by the corresponding end state.
}

The arguments \code{action}, \code{start.state}, and \code{end.state} will be called with the
state name as a string.
}
}

\subsection{Specification of the reward function: \eqn{R(a, s, s')}}{

The reward function can be specified in the following
ways:
\itemize{
\item A data frame with columns named exactly like the arguments of \code{R_()}.
You can use \code{rbind()}
with helper function \code{R_()} to create this data frame. Rewards can be specified
multiple times and the definition that
appears last in the data.frame will take affect.
\item A list of state x state matrices.
The list elements are for \code{'action'}. The matrix rows are \code{start.state} and the
columns are \code{end.state}.
\item A function following the same rules as for transition probabilities.
}

To avoid overflow problems with rewards, reward values should stay well within the
range of
\verb{[-1e10, +1e10]}. \code{-Inf} can be used as the reward for unavailable actions and
will be translated into a large negative reward for solvers that only support
finite reward values.
}

\subsection{Specification of the Start State}{

The start state of the agent can be a single state or a distribution over the states.
The start state definition is used as the default when the reward is calculated by \code{\link[=reward]{reward()}}
and for sampling with \code{\link[=sample_MDP]{sample_MDP()}}.

Options to specify the start state are:
\itemize{
\item A string specifying the name of a single starting state.
\item An integer in the range \eqn{1} to \eqn{n} to specify the index of a single starting state.
\item The string \code{"uniform"} where the start state is chosen using a uniform distribution over all states.
\item A probability distribution over the states. That is, a vector
of \eqn{|S|} probabilities, that add up to \eqn{1}.
}

The default state state is a uniform
distribution over all states.
}

\subsection{Normalizing MDP Descriptions}{

Different components can be specified in various ways. It is often
necessary to convert each component into a specific form (e.g., a
dense matrix) to save time during access.
Convert the Complete MDP Description into a consistent form
\code{normalize_MDP()} converts all components of the MDP description
into a consistent form and
returns a new MDP definition where \code{transition_prob},
\code{reward}, and \code{start} are normalized. This includes the internal
representation (dense, sparse, as a data.frame) and
also, \code{states}, and \code{actions} are ordered as given in the problem
definition to make safe access using numerical indices possible. Normalized
MDP descriptions can be
used in custom code that expects consistently a certain format.
}
}
\examples{
# simple MDP example
#
# states:    s1 s2 s3 s4
# transitions: forward moves -> and backward moves <-
# start: s1
# reward: s1, s2, s4 = 0 and s3 = 1

car <- MDP(
  states = c("s1", "s2", "s3", "s4"),
  actions = c("forward", "back", "stop"),
  transition <- list(
    forward = rbind(c(0, 1, 0, 0), 
                    c(0, 0, 1, 0), 
                    c(0, 0, 0, 1), 
                    c(0, 0, 0, 1)),
    back =    rbind(c(1, 0, 0, 0), 
                    c(1, 0, 0, 0), 
                    c(0, 1, 0, 0), 
                    c(0, 0, 1, 0)),
    stop = "identity"
  ),
  reward = rbind(
    R_(value = 0),
    R_(end.state = "s3", value = 1)
  ),
  discount = 0.9,
  start = "s1",
  name = "Simple Car MDP"
)

car

# internal representation
str(car)

# accessing elements
transition_matrix(car, sparse = FALSE)
transition_matrix(car, sparse = TRUE)
reward_matrix(car, sparse = FALSE)
reward_matrix(car, sparse = TRUE)

sol <- solve_MDP(car)
policy(sol)

# normalize MDP: make everything dense (transition_prob, reward and start)
car_dense <- normalize_MDP(car, sparse = FALSE)
str(car_dense)
}
\seealso{
Other MDP: 
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{actions}},
\code{\link{add_policy}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{q_values}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_and_absorbing}},
\code{\link{value_function}()}

Other MDP_examples: 
\code{\link{Cliff_walking}},
\code{\link{DynaMaze}},
\code{\link{Maze}},
\code{\link{Windy_gridworld}}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{MDP_examples}
