% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MDP.R
\name{MDP}
\alias{MDP}
\alias{is_solved_MDP}
\alias{is_timedependent}
\alias{epoch_to_episode}
\alias{T_}
\alias{R_}
\title{Define an MDP Problem}
\usage{
MDP(
  states,
  actions,
  transition_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  start = "uniform",
  info = NULL,
  name = NA
)

is_solved_MDP(x, stop = FALSE)

is_timedependent(x)

epoch_to_episode(x, epoch)

T_(action = NA, start.state = NA, end.state = NA, probability)

R_(action = NA, start.state = NA, end.state = NA, observation = NA, value)
}
\arguments{
\item{states}{a character vector specifying the names of the states.}

\item{actions}{a character vector specifying the names of the available
actions.}

\item{transition_prob}{Specifies the transition probabilities between
states.}

\item{reward}{Specifies the rewards dependent on action and states.}

\item{discount}{numeric; discount rate between 0 and 1.}

\item{horizon}{numeric; Number of epochs. \code{Inf} specifies an infinite
horizon.}

\item{start}{Specifies in which state the MDP starts.}

\item{info}{A list with additional information.}

\item{name}{a string to identify the MDP problem.}

\item{x}{a \code{MDP} object.}

\item{stop}{logical; stop with an error.}

\item{epoch}{integer; an epoch that should be converted to the
corresponding episode in a time-dependent MDP.}

\item{action}{action as a action label or integer. The value \code{NA} matches any action.}

\item{start.state, end.state}{state as a state label or an integer. The value \code{NA} matches any state.}

\item{probability, value}{Values used in the helper functions \code{T_()} and \code{R_()}.}

\item{observation}{unused for MDPs. Must be \code{NA}.}
}
\value{
The function returns an object of class MDP which is list with
the model specification. \code{\link[=solve_MDP]{solve_MDP()}} reads the object and adds a list element called
\code{'solution'}.
}
\description{
Defines all the elements of a discrete-time finite state-space MDP problem.
}
\details{
Markov decision processes (MDPs) are discrete-time stochastic control
process. We implement here MDPs with a finite state space.
\code{MDP()} defines all the element of a MDP problem including the discount rate, the
set of states, the set of actions,the transition
probabilities, the observation probabilities, and the rewards.

In the following we use the following notation. The MDP is a 5-duple:

\eqn{(S,A,T,R, \gamma)}.

\eqn{S} is the set of states; \eqn{A}
is the set of actions; \eqn{T} are the conditional transition probabilities
between states; \eqn{R} is the reward function; \eqn{\Omega} is the set of
observations; and
\eqn{\gamma} is the discount factor. We will use lower case letters to
represent a member of a set, e.g., \eqn{s} is a specific state. To refer to
the size of a set we will use cardinality, e.g., the number of actions is
\eqn{|A|}.

\strong{Names used for mathematical symbols in code}
\itemize{
\item \eqn{S, s, s'}: \verb{'states', start.state', 'end.state'}
\item \eqn{A, a}: \verb{'actions', 'action'}
}

State names and actions can be specified as strings or index numbers
(e.g., \code{start.state} can be specified as the index of the state in \code{states}).
For the specification as data.frames below, \code{NA} can be used to mean
any  \code{start.state}, \code{end.state} or \code{action}.

\strong{Specification of transition probabilities: \eqn{T(s' | s, a)}}

Transition probability to transition to state \eqn{s'} from given state \eqn{s}
and action \eqn{a}. The transition probabilities can be
specified in the following ways:
\itemize{
\item A data.frame with columns exactly like the arguments of \code{T_()}.
You can use \code{rbind()} with helper function \code{T_()} to create this data
frame. Probabilities can be specified multiple times and the definition that
appears last in the data.frame will take affect.
\item A named list of matrices, one for each action. Each matrix is square with
rows representing start states \eqn{s} and columns representing end states \eqn{s'}.
Instead of a matrix, also the strings \code{'identity'} or \code{'uniform'} can be specified.
\item A function with the same arguments are \code{T_()}, but no default values
that returns the transition probability.
}

\strong{Specification of the reward function: \eqn{R(a, s, s')}}

The reward function can be specified in the following
ways:
\itemize{
\item A data frame with columns named exactly like the arguments of \code{R_()}.
You can use \code{rbind()}
with helper function \code{R_()} to create this data frame. Rewards can be specified
multiple times and the definition that
appears last in the data.frame will take affect.
\item A list of lists. The list levels are \code{'action'} and \code{'start.state'}. The list elements
are matrices with
rows representing end states \eqn{s'} and columns representing an observation \eqn{o}.
\item A function with the same arguments are \code{R_()}, but no default values
that returns the reward.
}

To avoid overflow problems with rewards, reward values should stay well within the
range of
\verb{[-1e10, +1e10]}. \code{-Inf} can be used as the reward for unavailable actions and
will be translated into a large negative reward for solvers that only support
finite reward values.

Note: The code also includes in \code{R_()} an argument called \code{observation}.
Observations are only used POMDPs implemented in package
\code{pomdp} abs must always be \code{NA} for MDPs.

\strong{Start State}

The start state of the agent can be a single state or a distribution over the states.
The start state definition is used as the default when the reward is calculated by \code{\link[=reward]{reward()}}
and for simulations with \code{\link[=simulate_MDP]{simulate_MDP()}}.

Options to specify the start state are:
\itemize{
\item A string specifying the name of a single starting state.
\item An integer in the range \eqn{1} to \eqn{n} to specify the index of a single starting state.
\item The string \code{"uniform"} where the start state is chosen using a uniform distribution over all states.
\item A probability distribution over the states. That is, a vector
of \eqn{|S|} probabilities, that add up to \eqn{1}.
}

The default state state is a uniform
distribution over all states.

\strong{Time-dependent MDPs}

Time dependence of transition probabilities and
reward structure can be modeled by considering a set of \strong{episodes}
representing \strong{epoch} with the same settings. The length of each episode is
specified as a vector for \code{horizon}, where the length is the number of
episodes and each value is the length of the episode in epochs. Transition
probabilities, observation probabilities and/or reward structure can contain
a list with the values for each episode. The helper function \code{epoch_to_episode()} converts
an epoch to the episode it belongs to.
}
\examples{
# Michael's Sleepy Tiger Problem is like the POMDP Tiger problem, but
# has completely observable states because the tiger is sleeping in front
# of the door. This makes the problem an MDP.

STiger <- MDP(
  name = "Michael's Sleepy Tiger Problem",
  discount = .9,
  states = c("tiger-left", "tiger-right"),
  actions = c("open-left", "open-right", "do-nothing"),
  start = "uniform",

  # opening a door resets the problem
  transition_prob = list(
    "open-left" =  "uniform",
    "open-right" = "uniform",
    "do-nothing" = "identity"
  ),

  # the reward helper R_() expects: action, start.state, end.state, value
  reward = rbind(
    R_("open-left", "tiger-left", v = -100),
    R_("open-left", "tiger-right", v = 10),
    R_("open-right", "tiger-left", v = 10),
    R_("open-right", "tiger-right", v = -100),
    R_("do-nothing", v = 0)
  )
)

STiger

sol <- solve_MDP(STiger)
sol

policy(sol)
plot_value_function(sol)
}
\seealso{
Other MDP: 
\code{\link{MDP_policy_functions}},
\code{\link{accessors}},
\code{\link{actions}()},
\code{\link{add_policy}()},
\code{\link{gridworld}},
\code{\link{reachable_and_absorbing}},
\code{\link{regret}()},
\code{\link{simulate_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{value_function}()}

Other MDP_examples: 
\code{\link{Cliff_walking}},
\code{\link{DynaMaze}},
\code{\link{Maze}},
\code{\link{Windy_gridworld}}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{MDP_examples}
