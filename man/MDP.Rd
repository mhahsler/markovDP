% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MDP.R
\name{MDP}
\alias{MDP}
\alias{is_solved_MDP}
\alias{epoch_to_episode}
\alias{T_}
\alias{R_}
\title{Define an MDP Problem}
\usage{
MDP(
  states,
  actions,
  transition_prob,
  reward,
  discount = 0.9,
  horizon = Inf,
  start = "uniform",
  info = NULL,
  name = NA,
  normalize = TRUE
)

is_solved_MDP(x, stop = FALSE)

epoch_to_episode(x, epoch)

T_(action = NA, start.state = NA, end.state = NA, probability)

R_(action = NA, start.state = NA, end.state = NA, observation = NA, value)
}
\arguments{
\item{states}{a character vector specifying the names of the states.}

\item{actions}{a character vector specifying the names of the available
actions.}

\item{transition_prob}{Specifies the transition probabilities between
states.}

\item{reward}{Specifies the rewards dependent on action and states.}

\item{discount}{numeric; discount rate between 0 and 1.}

\item{horizon}{numeric; Number of epochs. \code{Inf} specifies an infinite
horizon.}

\item{start}{Specifies in which state the MDP starts.}

\item{info}{A list with additional information.}

\item{name}{a string to identify the MDP problem.}

\item{normalize}{logical; normalize representation (see \code{\link[=normalize_MDP]{normalize_MDP()}}).}

\item{x}{a \code{MDP} object.}

\item{stop}{logical; stop with an error.}

\item{epoch}{integer; an epoch that should be converted to the
corresponding episode in a time-dependent MDP.}

\item{action}{action as a action label or integer. The value \code{NA} matches any action.}

\item{start.state, end.state}{state as a state label or an integer. The value \code{NA} matches any state.}

\item{probability, value}{Values used in the helper functions \code{T_()} and \code{R_()}.}

\item{observation}{unused for MDPs. Must be \code{NA}.}
}
\value{
The function returns an object of class MDP which is list with
the model specification. \code{\link[=solve_MDP]{solve_MDP()}} reads the object and adds a list element called
\code{'solution'}.
}
\description{
Defines all the elements of a discrete-time finite state-space MDP problem.
}
\details{
Markov decision processes (MDPs) are discrete-time stochastic control
process. We implement here MDPs with a finite state space.
\code{MDP()} defines all the element of a MDP problem including the discount rate, the
set of states, the set of actions,the transition
probabilities, the observation probabilities, and the rewards.

In the following we use the following notation. The MDP is a 5-duple:

\eqn{(S,A,T,R, \gamma)}.

\eqn{S} is the set of states; \eqn{A}
is the set of actions; \eqn{T} are the conditional transition probabilities
between states; \eqn{R} is the reward function; \eqn{\Omega} is the set of
observations; and
\eqn{\gamma} is the discount factor. We will use lower case letters to
represent a member of a set, e.g., \eqn{s} is a specific state. To refer to
the size of a set we will use cardinality, e.g., the number of actions is
\eqn{|A|}.

\strong{Names used for mathematical symbols in code}
\itemize{
\item \eqn{S, s, s'}: \verb{'states', start.state', 'end.state'}
\item \eqn{A, a}: \verb{'actions', 'action'}
}

State names and actions can be specified as strings or index numbers
(e.g., \code{start.state} can be specified as the index of the state in \code{states}).
For the specification as data.frames below, \code{NA} can be used to mean
any  \code{start.state}, \code{end.state} or \code{action}.

\strong{Specification of transition probabilities: \eqn{T(s' | s, a)}}

Transition probability to transition to state \eqn{s'} from given state \eqn{s}
and action \eqn{a}. The transition probabilities can be
specified in the following ways:
\itemize{
\item A data.frame with columns exactly like the arguments of \code{T_()}.
You can use \code{rbind()} with helper function \code{T_()} to create this data
frame. Probabilities can be specified multiple times and the definition that
appears last in the data.frame will take affect.
\item A named list of matrices, one for each action. Each matrix is square with
rows representing start states \eqn{s} and columns representing end states \eqn{s'}.
Instead of a matrix, also the strings \code{'identity'} or \code{'uniform'} can be specified.
\item A function with the same arguments are \code{T_()}, but no default values
that returns the transition probability.
}

\strong{Specification of the reward function: \eqn{R(a, s, s')}}

The reward function can be specified in the following
ways:
\itemize{
\item A data frame with columns named exactly like the arguments of \code{R_()}.
You can use \code{rbind()}
with helper function \code{R_()} to create this data frame. Rewards can be specified
multiple times and the definition that
appears last in the data.frame will take affect.
\item A list of state x state matrices.
The list elements are for \code{'action'}. The matrix rows are \code{start.state} and the
columns are \code{end.state}.
\item A function with the same arguments are \code{R_()}, but no default values
that returns the reward.
}

To avoid overflow problems with rewards, reward values should stay well within the
range of
\verb{[-1e10, +1e10]}. \code{-Inf} can be used as the reward for unavailable actions and
will be translated into a large negative reward for solvers that only support
finite reward values.

Note: The code also includes in \code{R_()} an argument called \code{observation}.
Observations are only used POMDPs implemented in package
\code{pomdp} abs must always be \code{NA} for MDPs.

\strong{Start State}

The start state of the agent can be a single state or a distribution over the states.
The start state definition is used as the default when the reward is calculated by \code{\link[=reward]{reward()}}
and for simulations with \code{\link[=simulate_MDP]{simulate_MDP()}}.

Options to specify the start state are:
\itemize{
\item A string specifying the name of a single starting state.
\item An integer in the range \eqn{1} to \eqn{n} to specify the index of a single starting state.
\item The string \code{"uniform"} where the start state is chosen using a uniform distribution over all states.
\item A probability distribution over the states. That is, a vector
of \eqn{|S|} probabilities, that add up to \eqn{1}.
}

The default state state is a uniform
distribution over all states.
}
\examples{
# simple MDP example
#
# states:    s1 s2 s3 s4
# transitions: forward moves -> and backward moves <-
# start: s1
# reward: s1, s2, s4 = 0 and s3 = 1

car <- MDP(
  states = c("s1", "s2", "s3", "s4"),
  actions = c("forward", "back", "stop"),
  transition <- list(
    forward = rbind(c(0, 1, 0, 0), c(0, 0, 1, 0), c(0, 0, 0, 1), c(0, 0, 0, 1)),
    back = rbind(c(1, 0, 0, 0), c(1, 0, 0, 0), c(0, 1, 0, 0), c(0, 0, 1, 0)),
    stop = "identity"
  ),
  reward = rbind(
    R_(value = 0),
    R_(end.state = "s3", value = 1)
  ),
  discount = 0.9,
  start = "s1",
  name = "Simple Car MDP"
)

car

transition_matrix(car)
reward_matrix(car, sparse = TRUE)
reward_matrix(car)

sol <- solve_MDP(car)
policy(sol)
}
\seealso{
Other MDP: 
\code{\link{accessors}},
\code{\link{actions}()},
\code{\link{add_policy}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{q_values}()},
\code{\link{reachable_and_absorbing}},
\code{\link{regret}()},
\code{\link{simulate_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{value_function}()}

Other MDP_examples: 
\code{\link{Cliff_walking}},
\code{\link{DynaMaze}},
\code{\link{Maze}},
\code{\link{Windy_gridworld}}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{MDP_examples}
