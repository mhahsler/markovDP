% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sample_MDPTF.R
\name{sample_MDP.MDPTF}
\alias{sample_MDP.MDPTF}
\title{Sample Trajectories from an MDPTF}
\usage{
\method{sample_MDP}{MDPTF}(
  model,
  n,
  start = NULL,
  horizon = NULL,
  epsilon = NULL,
  exploring_starts = FALSE,
  trajectories = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{model}{an MDPTF model.}

\item{n}{number of trajectories.}

\item{start}{start state.}

\item{horizon}{epochs end once an absorbing state is reached or after
the maximal number of epochs specified via \code{horizon}. If \code{NULL} then the
horizon for the model is used.}

\item{epsilon}{the probability of random actions for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.}

\item{exploring_starts}{logical; randomly sample a start/action combination to
start the episode from.}

\item{trajectories}{logical; return the complete trajectories.}

\item{progress}{show a progress bar?}

\item{verbose}{report used parameters}

\item{...}{further arguments are ignored.}
}
\value{
A list with elements:
\itemize{
\item \code{avg_reward}: The average discounted reward.
\item \code{reward}: Reward for each trajectory.
\item \code{trajectories}: A data.frame with the trajectories. Each row
contains the \code{episode} id, the \code{time} step, the state \code{s},
the chosen action \code{a},
the reward \code{r}, and the next state \code{s_prime}. Trajectories are
only returned for \code{trajectories = TRUE}.
}
}
\description{
Sample trajectories through using a MDPTF.
}
\examples{
# enable parallel simulation (useful for sampling with large n)
# doParallel::registerDoParallel()

# Create a simple maze with the layout:
# XXXXXX
# XSX  X  
# X    X
# X    X
# X  XGX
# XXXXXX

model <- gw_maze_MDPTF(
           dim = s(4, 4),
           start = s(1, 1),
           goal = s(4, 4),
           walls = rbind(s(1, 2), s(4, 3)),
           discount = 0.95,
           name = "Simple Maze"
       )
model
gw_plot(model)

# sample a random walk (epsilon = 1) from the unsolved MDP
set.seed(1234)
sim <- sample_MDP(model, horizon = 500, n = 1, 
                   verbose = TRUE, trajectories = TRUE)
sim

# sample from a solved MDPTF by following the policy
set.seed(1234)
sol <- solve_MDP_APPROX(model, horizon = 500, n = 100,
                   transformation = transformation_fourier_basis, order = 2)
gw_plot(sol)

sim <- sample_MDP(sol, horizon = 500, n = 1, 
                   verbose = TRUE, trajectories = TRUE)
sim
}
\seealso{
Other MDPTF: 
\code{\link{MDPTF}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{reachable_states}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX}()},
\code{\link{solve_MDP_PG}()},
\code{\link{start}()}
}
\author{
Michael Hahsler
}
\concept{MDPTF}
