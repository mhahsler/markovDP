% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/action.R
\name{action}
\alias{action}
\title{Choose an Action Given a Policy}
\usage{
action(model, state, epsilon = 0, epoch = 1, ...)
}
\arguments{
\item{model}{a solved \link{MDP}.}

\item{state}{the state.}

\item{epsilon}{make the policy epsilon soft.}

\item{epoch}{what epoch of the policy should be used. Use 1 for converged policies.}

\item{...}{further parameters are passed on.}
}
\value{
The name of the optimal action as a factor.
}
\description{
Returns an action given a deterministic policy.
The policy can be made epsilon-soft.
}
\examples{
data("Maze")
Maze

sol <- solve_MDP(Maze)
policy(sol)

action(sol, state = "s(1,3)")

## choose from an epsilon-soft policy
table(replicate(100, action(sol, state = "s(1,3)", epsilon = 0.1)))
}
\seealso{
Other policy: 
\code{\link{Q_values}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{policy}()},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{reward}()},
\code{\link{value_function}()},
\code{\link{visit_probability}()}
}
\author{
Michael Hahsler
}
\concept{policy}
