% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/action.R
\name{action}
\alias{action}
\alias{action.MDP}
\title{Choose an Action Given a Policy}
\usage{
action(model, ...)

\method{action}{MDP}(model, state, epsilon = 0, epoch = 1, ...)
}
\arguments{
\item{model}{a solved \link{MDP}.}

\item{...}{further parameters are passed on.}

\item{state}{the state.}

\item{epsilon}{make the policy epsilon soft.}

\item{epoch}{what epoch of the policy should be used. Use 1 for converged policies.}
}
\value{
The name of the optimal action as a factor.
}
\description{
Returns an action given a deterministic policy. The policy can be made epsilon-soft.
}
\examples{
data("Maze")
Maze

sol <- solve_MDP(Maze)
policy(sol)

action(sol, state = "s(1,3)")

## choose from an epsilon-soft policy
table(replicate(100, action(sol, state = "s(1,3)", epsilon = 0.1)))
}
\seealso{
Other policy: 
\code{\link{Q_values}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{policy}()},
\code{\link{policy_evaluation}()},
\code{\link{reward}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{policy}
