% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy.R
\name{policy}
\alias{policy}
\alias{add_policy}
\alias{random_policy}
\alias{manual_policy}
\alias{induced_transition_matrix}
\alias{induced_reward_matrix}
\title{Extract, Create Add a Policy to a Model}
\usage{
policy(model, epoch = NULL, drop = TRUE)

add_policy(model, policy)

random_policy(
  model,
  prob = NULL,
  estimate_V = FALSE,
  only_available_actions = FALSE,
  ...
)

manual_policy(model, actions, V = NULL, estimate_V = FALSE)

induced_transition_matrix(model, policy = NULL, epoch = 1L, sparse = FALSE)

induced_reward_matrix(model, policy = NULL, epoch = 1L)
}
\arguments{
\item{model}{A solved \link{MDP} object.}

\item{epoch}{return the policy of the given epoch. \code{NULL} returns a list
with elements for each epoch.}

\item{drop}{logical; drop the list for converged, epoch-independent policies.}

\item{policy}{a policy data.frame.}

\item{prob}{probability vector for random actions for \code{random_policy()}.
a logical indicating if action probabilities should be returned for
\code{greedy_action()}.}

\item{estimate_V}{logical; estimate the value function
using \code{\link[=policy_evaluation]{policy_evaluation()}}?}

\item{only_available_actions}{logical; only sample from available actions?
(see \code{\link[=available_actions]{available_actions()}} for details)}

\item{...}{is passed on to \code{\link[=available_actions]{available_actions()}}.}

\item{actions}{a vector with the action (either the action label or the
numeric id) for each state.}

\item{V}{a vector representing the value function for the policy. If \code{TRUE}, then
the it is estimated using \code{policy_evaluation()}.}

\item{sparse}{logical; should a sparse transition matrix be returned?}
}
\value{
\itemize{
\item \code{policy()}, \code{random_policy()} and \code{manual_policy()} return a data.frame
containing the policy. If \code{drop = FALSE} then the policy is returned
as a list with the policy for each epoch.
\item \code{add_policy()} returns an MDP object.
\item \code{induced_transition_matrix} returns a single transition matrix.
}

The model description with the added policy.
}
\description{
Extracts the policy from a solved model or create a policy. All
policies are deterministic.
}
\details{
\code{policy()} extracts the (deterministic) policy from a solved MDP in the form of a
a data.frame with columns for:
\itemize{
\item \code{state}: The state.
\item \code{V}: The state values if the policy is followed.
\item \code{action}: The prescribed action.
}

For unconverged, finite-horizon problems, the solution is a policy for
each epoch. This is returned as a list of data.frames.

\code{add_policy()} adds a policy to an existing MDP object.

\code{random_policy()} and \code{manual_policy()} construct new policies.

\code{induced_transition_matrix()} returns the single transition matrix which follows
the actions specified in a policy.
}
\examples{
data("Maze")

sol <- solve_MDP(Maze)
sol

## policy with value function and optimal action.
policy(sol)
plot_value_function(sol)
gw_plot(sol)

induced_transition_matrix(sol)

## create a random policy
pi_random <- random_policy(Maze, estimate_V = TRUE)
pi_random

gw_plot(add_policy(Maze, pi_random))

## create a manual policy (go up and in some squares to the right)
acts <- rep("up", times = length(Maze$states))
names(acts) <- Maze$states
acts[c("s(1,1)", "s(1,2)", "s(1,3)")] <- "right"
acts

pi_manual <- manual_policy(Maze, acts, estimate_V = TRUE)
pi_manual

gw_plot(add_policy(Maze, pi_manual))

# Transition matrix induced by the policy
induced_transition_matrix(Maze, pi_manual, sparse = TRUE)

## Finite horizon (we use incremental pruning because grid does not converge)
sol <- solve_MDP(model = Maze, horizon = 3)
sol

policy(sol)
gw_plot(sol, epoch = 1)
gw_plot(sol, epoch = 2)
gw_plot(sol, epoch = 3)
}
\seealso{
Other policy: 
\code{\link{Q_values}()},
\code{\link{action}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{reward}()},
\code{\link{value_function}()},
\code{\link{visit_probability}()}
}
\author{
Michael Hahsler
}
\concept{policy}
\keyword{graphs}
