% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy.R
\name{policy}
\alias{policy}
\title{Extract the Policy}
\usage{
policy(x, epoch = NULL, drop = TRUE)
}
\arguments{
\item{x}{A solved \link{MDP} object.}

\item{epoch}{return the policy of the given epoch. \code{NULL} returns a list
with elements for each epoch.}

\item{drop}{logical; drop the list for converged, epoch-independent policies.}
}
\value{
A list with the policy for each epoch. Converged policies
have only one element. If \code{drop = TRUE} then the policy is returned
without a list.
}
\description{
Extracts the policy from a solved model.
}
\details{
A list (one entry per epoch) with the optimal policy.
For converged, infinite-horizon problems solutions, a list with only the
converged solution is produced.

For an MDP, the policy is a data.frame with columns for:
\itemize{
\item \code{state}: The state.
\item \code{U}: The state's value (discounted expected utility U) if the policy
is followed
\item \code{action}: The prescribed action.
}
}
\examples{
data("Maze")

sol <- solve_MDP(Maze)
sol

# policy with value function and optimal action.
policy(sol)
plot_value_function(sol)

# Finite horizon (we use incremental pruning because grid does not converge)
sol <- solve_MDP(model = Maze, horizon = 3)
sol

policy(sol)
}
\seealso{
Other policy: 
\code{\link{MDP_policy_functions}},
\code{\link{action}()},
\code{\link{reward}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{policy}
\keyword{graphs}
