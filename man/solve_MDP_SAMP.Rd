% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_SAMP.R
\name{solve_MDP_SAMP}
\alias{solve_MDP_SAMP}
\title{Solve MDPs using Random-Sampling}
\usage{
solve_MDP_SAMP(
  model,
  method = "q_planning",
  horizon = NULL,
  discount = NULL,
  alpha = schedule_exp(0.2, 0.1),
  n = 1000,
  Q = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods: \code{'q_planning'}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{alpha}{step size (learning rate). A scalar value between 0 and 1 or a
\link{schedule}.}

\item{n}{number of episodes used for learning.}

\item{Q}{an initial state-action value matrix. By default an all 0 matrix is
used.}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}
}
\value{
\code{solve_MDP()} returns an object of class MDP or MDPTF which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements that depend on the used method. Common
elements are:
\itemize{
\item \code{method} with the name of the used method
\item parameters used.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
}
}
\description{
Solve MDPs via random-sampling. Implemented is Sutton and Barto's one-step
random-sampling one-step tabular Q-planning.
}
\details{
Random-sample one-step tabular Q-planning is a simple, not very effective,
planning method shown as an illustration in Chapter 8 of
Sutton and Barto (2018). It randomly selects a
state/action pair and samples the following state \eqn{s'} to
perform a single a one-step update:

\deqn{Q(s,a) \leftarrow Q(s, a) + \alpha * (R + \gamma \max_a'(Q(s', a')) - Q(s, a))}
\subsection{Schedules}{
\itemize{
\item alpha schedule: \code{t} is set to the number of times the a Q-value for state
\code{s} was updated.
}
}
}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.
}
\seealso{
Other solver: 
\code{\link{schedule}},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_TD}()}
}
\concept{solver}
