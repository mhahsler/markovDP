% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sample_MDP.R
\name{sample_MDP}
\alias{sample_MDP}
\alias{sample_MDP.MDP}
\title{Sample Trajectories from an MDP}
\usage{
sample_MDP(model, n, ...)

\method{sample_MDP}{MDP}(
  model,
  n,
  start = NULL,
  horizon = NULL,
  epsilon = NULL,
  exploring_starts = FALSE,
  delta_horizon = 0.001,
  trajectories = FALSE,
  engine = NULL,
  progress = TRUE,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{model}{an MDP model.}

\item{n}{number of trajectories.}

\item{...}{further arguments are ignored.}

\item{start}{probability distribution over the states for choosing the
starting states for the trajectories. Defaults to "uniform".}

\item{horizon}{epochs end once an absorbing state is reached or after
the maximal number of epochs specified via \code{horizon}. If \code{NULL} then the
horizon for the model is used.}

\item{epsilon}{the probability of random actions  for using an epsilon-greedy policy.
Default for solved models is 0 and for unsolved model 1.}

\item{exploring_starts}{logical; randomly sample a start/action combination to
start the episode from.}

\item{delta_horizon}{precision used to determine the horizon for infinite-horizon problems.}

\item{trajectories}{logical; return the complete trajectories.}

\item{engine}{\code{'cpp'} or \code{'r'} to perform simulation using a faster C++
or a native R implementation \code{NULL} uses the C++ implementation unless the transition model or
the reward are specified as R functions (which are slow in C++).}

\item{progress}{show a progress bar?}

\item{verbose}{report used parameters}
}
\value{
A list with elements:
\itemize{
\item \code{avg_reward}: The average discounted reward.
\item \code{reward}: Reward for each trajectory.
\item \code{action_cnt}: Action counts.
\item \code{state_cnt}: State counts.
\item \code{trajectories}: A data.frame with the trajectories. Each row
contains the \code{episode} id, the \code{time} step, the state \code{s},
the chosen action \code{a},
the reward \code{r}, and the next state \code{s_prime}. Trajectories are
only returned for \code{trajectories = TRUE}.
}
}
\description{
Sample trajectories through an MDP. The start state for each
trajectory is chosen using the start definition in the model. Actions are chosen
randomly of using an epsilon-greedy policy.
}
\details{
The default is a
faster C++ implementation (\code{engine = 'cpp'}).
A native R implementation is available (\code{engine = 'r'}).

Both implementations support parallel execution using the package
\pkg{foreach}. To enable parallel execution, a parallel backend like
\pkg{doparallel} needs to be available needs to be registered (see
\code{\link[doParallel:registerDoParallel]{doParallel::registerDoParallel()}}).
Note that small samples are slower using parallelization. Therefore, C++ simulations
with n * horizon less than 100,000 are always executed using a single worker.
}
\examples{
# enable parallel simulation
# doParallel::registerDoParallel()

data(Maze)

# solve the MDP for 5 epochs and no discounting
sol <- solve_MDP(Maze, discount = 1)
sol

# V in the policy is and estimate of the state values when following the optimal policy.
policy(sol)
gw_matrix(sol, what = "action")

## Example 1: simulate 100 trajectories following the policy,
#             only the final belief state is returned
sim <- sample_MDP(sol, n = 100, horizon = 10, verbose = TRUE)
sim

# Note that all simulations for this model start at s_1 and that the simulated avg. reward
# is therefore an estimate to the value function for the start state s_1.
policy(sol)[1, ]

# Calculate proportion of actions taken in the simulation
round_stochastic(sim$action_cnt / sum(sim$action_cnt), 2)

# reward distribution
hist(sim$reward)

## Example 2: simulate starting following a uniform distribution over all
#             states and return all trajectories
sim <- sample_MDP(sol,
  n = 100, start = "uniform", horizon = 10,
  trajectories = TRUE
)
head(sim$trajectories)

# how often was each state visited?
table(sim$trajectories$s)
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{reachable_states}()},
\code{\link{regret}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX_LAMBDA}()},
\code{\link{start}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
