% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP.R, R/solve_MDP_DP.R,
%   R/solve_MDP_LP.R, R/solve_MDP_MC.R, R/solve_MDP_TD.R, R/solve_MDP_sampling.R
\name{solve_MDP}
\alias{solve_MDP}
\alias{solve_MDP_DP}
\alias{solve_MDP_LP}
\alias{solve_MDP_MC}
\alias{solve_MDP_TD}
\alias{solve_MDP_sampling}
\title{Solve an MDP Problem}
\usage{
solve_MDP(model, method = "value_iteration", ...)

solve_MDP_DP(
  model,
  method = "value_iteration",
  horizon = NULL,
  discount = NULL,
  iter_max = 1000,
  error = 0.01,
  k_backups = 10,
  U = NULL,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)

solve_MDP_LP(
  model,
  method = "lp",
  horizon = NULL,
  discount = NULL,
  verbose = FALSE,
  ...
)

solve_MDP_MC(
  model,
  method = "MC_exploring_starts",
  horizon = NULL,
  discount = NULL,
  n = 100,
  ...,
  Q = NULL,
  epsilon = 0.1,
  first_visit = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

solve_MDP_TD(
  model,
  method = "q_learning",
  horizon = NULL,
  discount = NULL,
  alpha = 0.5,
  epsilon = 0.2,
  n = 1000,
  Q = NULL,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

solve_MDP_sampling(
  model,
  method = "q_planning",
  horizon = NULL,
  discount = NULL,
  alpha = 0.5,
  n = 1000,
  Q = NULL,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods:
\code{'value_iteration'},
\code{'policy_iteration'},
\code{'lp'},
\code{'q_learning'},
\code{'sarsa'},
\code{'expected_sarsa'},
\code{'MC_exploring_starts'},
\code{'MC_on_policy'},
\verb{'MC_off_policy', }'q_planning'`.}

\item{...}{further parameters are passed on to the solver function.}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{iter_max}{maximum number of iterations allowed to converge. If the
maximum is reached then the non-converged solution is returned with a
warning.}

\item{error}{value iteration: maximum error allowed in the utility of any state
(i.e., the maximum policy loss) used as the termination criterion.}

\item{k_backups}{policy iteration: number of look ahead steps used for approximate policy evaluation
used by the policy iteration method.}

\item{U}{a vector with initial utilities used for each state. If
\code{NULL}, then the default of a vector of all 0s is used.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time to calculate the matrices and it saves memory.}

\item{continue}{logical; Continue with an unconverged solution specified in \code{model}.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical, if set to \code{TRUE}, the function provides the
output of the solver in the R console.}

\item{n}{number of episodes used for learning.}

\item{Q}{a action value matrix.}

\item{epsilon}{used for \eqn{\epsilon}-greedy policies.}

\item{first_visit}{if \code{TRUE} then only the first visit of a state/action pair
in an episode is used to update Q, otherwise, every-visit update is used.}

\item{alpha}{step size in \verb{(0, 1]}.}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Implementation of value iteration, modified policy iteration and other
methods based on reinforcement learning techniques to solve finite
state space MDPs.
}
\details{
Several solvers are available. Note that some solvers are only implemented
for finite-horizon problems.
\subsection{Dynamic Programming}{

Implemented are the following dynamic programming methods (following
Russell and Norvig, 2010):
\itemize{
\item \strong{Modified Policy Iteration} (Howard 1960; Puterman and Shin 1978)
starts with a random policy and iteratively performs
a sequence of
\enumerate{
\item approximate policy evaluation (estimate the value function for the
current policy using \code{k_backups} and function \code{\link[=policy_evaluation]{policy_evaluation()}}, and
\item policy improvement (calculate a greedy policy given the value function).
The algorithm stops when it converges to a stable policy (i.e., no changes
between two iterations).
}
\item \strong{Value Iteration} (Bellman 1957) starts with
an arbitrary value function (by default all 0s) and iteratively
updates the value function for each state using the Bellman equation.
The iterations
are terminated either after \code{iter_max} iterations or when the solution converges.
Approximate convergence is achieved
for discounted problems (with \eqn{\gamma < 1})
when the maximal value function change for any state \eqn{\delta} is
\eqn{\delta \le error (1-\gamma) / \gamma}. It can be shown that this means
that no state value is more than
\eqn{error} from the value in the optimal value function. For undiscounted
problems, we use \eqn{\delta \le error}.

The greedy policy
is calculated from the final value function. Value iteration can be seen as
policy iteration with truncated policy evaluation.
\item \strong{Prioritized Sweeping} (Moore and Atkeson, 1993) approximate the optimal value
function by iteratively adjusting always only the state value of the state
with the largest Bellman error. That is, if a state is updated, all states leading
to this states will have their priority increased by the update difference.
This update order leads to faster convergence compared
to sweeping the whole state state in regular value iteration.

The additional parameter \code{init_H} can be used to define how to initialize
the priority \eqn{H(s)}. The default is \code{init_H = "reward"} which uses the
absolute value of the largest reward of going to the state.
Since the value function \eqn{U} is initialized with all zeros, this represents
the error and leads the algorithm to start
sweeping backwards from the high reward states.
\code{init_H = "random"} initializes the priorities randomly with a value larger than
\code{error} + a small value to make sure each state is tries at least once.

This implementation stops updating when the largest priority values
over all states is less than the specified \code{error}.
}

Note that policies converge earlier than value functions.
}

\subsection{Linear Programming}{

The following linear programming formulation (Manne 1960) is implemented. For the
optimal value function, the Bellman equation holds:

\deqn{
 V^*(s) = \max_{a \in A}\sum_{s' \in S} T(s, a, s') [ R(s, a, s') + \gamma V^*(s')]\; \forall a\in A, s \in S
}

We can find the optimal value function by solving the following linear program:
\deqn{\text{min} \sum_{s\in S} V(s)}
subject to
\deqn{V(s) \ge \sum_{s' \in S} T(s, a, s') [R(s, a, s') + \gamma V(s')],\; \forall a\in A, s \in S
}

Note:
\itemize{
\item The discounting factor has to be strictly less than 1.
\item Additional parameters to to \code{solve_MDP} are passed on to \code{\link[lpSolve:lp]{lpSolve::lp()}}.
\item We use the solver in
\code{\link[lpSolve:lp]{lpSolve::lp()}} which requires all decision variables (state values) to be non-negative.
To ensure this, for negative rewards, all rewards as shifted so the
smallest reward is
0. This transformation does not change the optimal policy.
}
}

\subsection{Monte Carlo Control}{

Monte Carlo control simulates a whole episode using the current behavior
policy and then updates the target policy before simulating the next episode.
Implemented are the following temporal difference control methods
described in Sutton and Barto (2020).
\itemize{
\item \strong{Monte Carlo Control with exploring Starts} uses the same greedy policy for
behavior and target (on-policy). To make sure all states/action pairs are
explored, it uses exploring starts meaning that new episodes are started at a randomly
chosen state using a randomly chooses action.
\item \strong{On-policy Monte Carlo Control} uses for behavior and as the target policy
an epsilon-greedy policy.
\item \strong{Off-policy Monte Carlo Control} uses for behavior an arbitrary policy
(we use an epsilon-greedy policy) and learns a greedy policy using
importance sampling.
}
}

\subsection{Temporal Difference Control}{

Implemented are several temporal difference control methods
described in Sutton and Barto (2020).
Note that the MDP transition and reward models are used
for these reinforcement learning methods only to sample from
the environment.
The algorithms use a step size parameter \eqn{\alpha} (learning rate) for the
updates and the exploration parameter \eqn{\epsilon} for
the \eqn{\epsilon}-greedy behavior policy.

If the model has absorbing states to terminate episodes, then no maximal episode length
(\code{horizon}) needs to
be specified. To make sure that the algorithm does finish in a reasonable amount of time,
episodes are stopped after 1000 actions (with a warning). For models without absorbing states,
the episode length has to be specified via \code{horizon}.
\itemize{
\item \strong{Q-Learning} (Watkins and Dayan 1992) is an off-policy temporal difference method that uses
an \eqn{\epsilon}-greedy behavior policy and learns a greedy target
policy.
\item \strong{Sarsa} (Rummery and Niranjan 1994) is an on-policy method that follows and learns
an \eqn{\epsilon}-greedy policy. The final \eqn{\epsilon}-greedy policy
is converted into a greedy policy.
\item \strong{Expected Sarsa} (R. S. Sutton and Barto 2018). We implement an on-policy version that uses
the expected value under the current policy for the update.
It moves deterministically in the same direction as Sarsa would
move in expectation. Because it uses the expectation, we can
set the step size \eqn{\alpha} to large values and 1 is common.
}
}

\subsection{Planning by Sampling}{

A simple, not very effective, planning method proposed by Sutton and Barto (2020) in Chapter 8.
\itemize{
\item \strong{Random-sample one-step tabular Q-planning} randomly selects a
state/action pair and samples the resulting reward and next state from
the model. This
information is used to update a single Q-table value.
}
}
}
\examples{
data(Maze)
Maze

# use value iteration
maze_solved <- solve_MDP(Maze, method = "value_iteration")
maze_solved
policy(maze_solved)

# plot the value function U
plot_value_function(maze_solved)

# Gridworld solutions can be visualized
gridworld_plot(maze_solved)

# Use linear programming
maze_solved <- solve_MDP(Maze, method = "lp")
maze_solved
policy(maze_solved)

# use prioritized sweeping (which is known to be fast for mazes)
maze_solved <- solve_MDP(Maze, method = "prioritized_sweeping")
policy(maze_solved)

# finite horizon
maze_solved <- solve_MDP(Maze, method = "value_iteration", horizon = 3)
policy(maze_solved)
gridworld_plot(maze_solved, epoch = 1)
gridworld_plot(maze_solved, epoch = 2)
gridworld_plot(maze_solved, epoch = 3)

# create a random policy where action n is very likely and approximate
#  the value function. We change the discount factor to .9 for this.
Maze_discounted <- Maze
Maze_discounted$discount <- .9
pi <- random_policy(Maze_discounted,
                    prob = c(n = .7, e = .1, s = .1, w = 0.1)
)
pi

# compare the utility function for the random policy with the function for the optimal
#  policy found by the solver.
maze_solved <- solve_MDP(Maze)

policy_evaluation(Maze, pi, k_backup = 100)
policy_evaluation(Maze, policy(maze_solved), k_backup = 100)

# Note that the solver already calculates the utility function and returns it with the policy
policy(maze_solved)

# Learn a Policy using Q-Learning
maze_learned <- solve_MDP(Maze, method = "q_learning", n = 100)
maze_learned

maze_learned$solution
policy(maze_learned)
plot_value_function(maze_learned)
gridworld_plot(maze_learned)
}
\references{
Bellman, Richard. 1957. "A Markovian Decision Process." Indiana University Mathematics Journal 6: 679-84. \url{https://www.jstor.org/stable/24900506}.

Howard, R. A. 1960. Dynamic Programming and Markov Processes. Cambridge, MA: MIT Press.

Manne, Alan. 1960. "On the Job-Shop Scheduling Problem." Operations Research 8 (2): 219-23. \doi{10.1287/opre.8.2.219}.

Moore, Andrew, and C. G. Atkeson. 1993. "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time." Machine Learning 13 (1): 103â€“30. \doi{10.1007/BF00993104}.

Puterman, Martin L., and Moon Chirl Shin. 1978. "Modified Policy Iteration Algorithms for Discounted Markov Decision Problems." Management Science 24: 1127-37. \doi{10.1287/mnsc.24.11.1127}.

Rummery, G., and Mahesan Niranjan. 1994. "On-Line Q-Learning Using Connectionist Systems." Techreport CUED/F-INFENG/TR 166. Cambridge University Engineering Department.

Russell, Stuart J., and Peter Norvig. 2020. Artificial Intelligence: A Modern Approach (4th Edition). Pearson. \url{http://aima.cs.berkeley.edu/}.

Sutton, R. 1988. "Learning to Predict by the Method of Temporal Differences." Machine Learning 3: 9-44. \url{https://link.springer.com/article/10.1007/BF00115009}.

Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Watkins, Christopher J. C. H., and Peter Dayan. 1992. "Q-Learning." Machine Learning 8 (3): 279-92. \doi{10.1007/BF00992698}.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{actions}},
\code{\link{add_policy}()},
\code{\link{bellman_update}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{q_values}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_and_absorbing}},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{solver}
