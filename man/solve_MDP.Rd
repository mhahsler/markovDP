% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP.R
\name{solve_MDP}
\alias{solve_MDP}
\alias{solve_MDP.MDP}
\alias{solve_MDP.MDPTF}
\title{Solve an MDP Problem}
\usage{
solve_MDP(model, ...)

\method{solve_MDP}{MDP}(
  model,
  method = "DP:VI",
  horizon = NULL,
  discount = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  verbose = FALSE,
  progress = !verbose
)

\method{solve_MDP}{MDPTF}(
  model,
  method = "APPROX:semi_gradient_sarsa",
  horizon = NULL,
  discount = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  verbose = FALSE,
  progress = !verbose
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{...}{further parameters are passed on to the solver function.}

\item{method}{string; Composed of the algorithm family abbreviation and the algorithm
separated by \code{:}. The algorithm families can be found in the See Also
section under "Other solvers". The family abbreviation follows \code{solve_MDP_} in the function name.}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}

\item{progress}{logical; show a progress bar with estimated time for completion.}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Implementation of value iteration, modified policy iteration and other
methods based on reinforcement learning techniques to solve finite
state space MDPs.
}
\details{
Several solvers are available. Note that some solvers are only implemented
for finite-horizon problems.

Most solvers can be interrupted using Esc/CTRL-C and will return the
current solution. Solving can be continued by calling \code{solve_MDP} with the
partial solution as the model and the parameter \code{continue = TRUE}. This method
can also be used to reduce parameters like \code{alpha} or \code{epsilon} (see Q-learning
in the Examples section).

A list of available solvers can be found in the See Also section under
"Other solvers".

While \link{MDP} model contain an explicit specification of the state space,
the transition probabilities and the reward structure, \link{MDPTF} only contains a
transition function. This means that only a small subset of solvers can be
used for MDPTFs. This currently includes only includes
the solvers in \code{\link[=solve_MDP_APPROX]{solve_MDP_APPROX()}}.
}
\examples{
data(Maze)
Maze

# default is value iteration (VI)
maze_solved <- solve_MDP(Maze)
maze_solved
policy(maze_solved)

# plot the value function U
plot_value_function(maze_solved)

# Gridworld solutions can be visualized
gw_plot(maze_solved)

}
\references{
Russell, Stuart J., and Peter Norvig. 2020. Artificial Intelligence: A Modern Approach (4th Edition). Pearson. \url{http://aima.cs.berkeley.edu/}.

Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.
}
\seealso{
Other solver: 
\code{\link{solve_MDP_APPROX}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()}

Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{reachable_states}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{start}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other MDPTF: 
\code{\link{MDPTF}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{reachable_states}()},
\code{\link{sample_MDP.MDPTF}()},
\code{\link{solve_MDP_APPROX}()},
\code{\link{start}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{MDPTF}
\concept{solver}
