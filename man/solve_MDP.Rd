% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP.R, R/solve_MDP_DP.R,
%   R/solve_MDP_LP.R, R/solve_MDP_MC.R, R/solve_MDP_TD.R, R/solve_MDP_sampling.R
\name{solve_MDP}
\alias{solve_MDP}
\alias{solve_MDP_DP}
\alias{solve_MDP_LP}
\alias{solve_MDP_MC}
\alias{solve_MDP_TD}
\alias{solve_MDP_sampling}
\title{Solve an MDP Problem}
\usage{
solve_MDP(model, method = "value_iteration", ...)

solve_MDP_DP(
  model,
  method = "value_iteration",
  horizon = NULL,
  discount = NULL,
  n = 1000L,
  error = 0.001,
  k_backups = 10L,
  V = NULL,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)

solve_MDP_LP(
  model,
  method = "lp",
  horizon = NULL,
  discount = NULL,
  inf = 1000,
  verbose = FALSE,
  ...
)

solve_MDP_MC(
  model,
  method = "MC_exploring_starts",
  horizon = NULL,
  discount = NULL,
  n = 100,
  ...,
  Q = NULL,
  epsilon = 0.2,
  first_visit = TRUE,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

solve_MDP_TD(
  model,
  method = "q_learning",
  horizon = NULL,
  discount = NULL,
  alpha = function(t, n) 1/n,
  alpha_expected_sarsa = 1,
  epsilon = 0.2,
  n = 1000,
  Q = 0,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

solve_MDP_sampling(
  model,
  method = "q_planning",
  horizon = NULL,
  discount = NULL,
  alpha = function(t, n) 1/n,
  n = 1000,
  Q = NULL,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods:
\code{'value_iteration'},
\code{'policy_iteration'},
\code{'lp'},
\code{'q_learning'},
\code{'sarsa'},
\code{'expected_sarsa'},
\code{'MC_exploring_starts'},
\code{'MC_on_policy'},
\verb{'MC_off_policy', }'q_planning'`.}

\item{...}{further parameters are passed on to the solver function.}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{n}{number of episodes used for learning.}

\item{error}{value iteration: maximum Bellman error allowed for the
convergence criterion.}

\item{k_backups}{policy iteration: maximum number of Bellman backups used in
the iterative policy evaluation step. Policy evaluation typically converges earlier
with a maximum Bellman error less than \code{error}.}

\item{V}{a vector with initial state values. If
\code{NULL}, then the default of a vector of all 0s (\code{\link[=V_zero]{V_zero()}}) is used.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; Continue with an unconverged solution specified in \code{model}.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical, if set to \code{TRUE}, the function provides the
output of the solver in the R console.}

\item{inf}{value used for infinity when calling \code{lpSolve::lp()}. This
should me much larger/smaller than the largest/smallest
reward in the model.}

\item{Q}{a state-action value matrix.}

\item{epsilon}{used for \eqn{\epsilon}-greedy policies.}

\item{first_visit}{if \code{TRUE} then only the first visit of a state/action pair
in an episode is used to update Q, otherwise, every-visit update is used.}

\item{alpha}{step size as a function of the time step \code{t} and the number of times
the respective Q-value was updated \code{n} or a scalar.}

\item{alpha_expected_sarsa}{step size for expected Sarsa defaults to 1. Can be
a function like for \code{alpha}.}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Implementation of value iteration, modified policy iteration and other
methods based on reinforcement learning techniques to solve finite
state space MDPs.
}
\details{
Several solvers are available. Note that some solvers are only implemented
for finite-horizon problems.
\subsection{Dynamic Programming}{

Implemented are the following dynamic programming methods (following
Russell and Norvig, 2010):
\itemize{
\item \strong{Modified Policy Iteration} (Howard 1960; Puterman and Shin 1978)
starts with a random policy and iteratively performs
a sequence of
\enumerate{
\item approximate policy evaluation (estimate the value function for the
current policy using \code{k_backups} and function \code{\link[=policy_evaluation]{policy_evaluation()}}, and
\item policy improvement (calculate a greedy policy given the value function).
The algorithm stops when it converges to a stable policy (i.e., no changes
between two iterations).
}
\item \strong{Value Iteration} (Bellman 1957) starts with
an arbitrary value function (by default all 0s) and iteratively
updates the value function for each state using the Bellman equation.
The iterations
are terminated either after \code{n} iterations or when the solution converges.
Approximate convergence is achieved
for discounted problems (with \eqn{\gamma < 1})
when the maximal value function change for any state \eqn{\delta} is
\eqn{\delta \le error (1-\gamma) / \gamma}. It can be shown that this means
that no state value is more than
\eqn{error} from the value in the optimal value function. For undiscounted
problems, we use \eqn{\delta \le error}.

A greedy policy
is calculated from the final value function. Value iteration can be seen as
policy iteration with truncated policy evaluation.
\item \strong{Prioritized Sweeping} (Moore and Atkeson, 1993; Andre et al., 1997; Li and Littman, 2008)
approximate the optimal value
function by iteratively adjusting one state at a time. The state to be updated is chosen
depending on its priority which reflects how much a state value may change
given the most recently updated other states that can be directly reached via an action.
This update order often lead to faster convergence compared
to sweeping the whole state state in regular value iteration.

We implement the two priority update strategies described as \strong{PS} and
\strong{GenPS} by Li and Littman.
\itemize{
\item \strong{PS} (Moore and Atkeson, 1993) updates the priority of a state \eqn{H(s)}
using:
\deqn{
       \forall{s \in \mathcal{S}}: H_{t+1}(s)  \leftarrow \begin{cases}
         \max(H_{t}(s), \Delta_t \max_{a \in \mathcal{A}}(p(s_t|s,a)) \text{ for } s \ne s_{t+1} \\
         \Delta_t \max_{a \in A}(p(s_t|s,a) \text{ for } s = s_{t+1}
         \end{cases}
     }

where \eqn{\Delta_t = |V_{t+1}(s_t) - V_t(s_t)| = |E(s_t; V_{t+1})|}, i.e.,
the Bellman error for the updated state.
\item \strong{GenPS} (Andre et al., 1997) updates all state priorities using their
current Bellman error:

\deqn{\forall{s \in \mathcal{S}}: H_{t+1}(s) \leftarrow |E(s; V_{t+1})|}

where \eqn{E(s; V_{t+1}) = \max_{a \in A} \left[R(s,a) + \gamma \sum_{s \in S} p(s'|s,a) V(s')\right] - V(s)}
is a state's Bellman error.
}

The update method can be chosen using the additional parameter \code{H_update}
as the character string \code{"PS_random"}, \code{"PS_error"} or \code{"GenPS"}.
The default is \code{H_update = "GenPS"}. For PS, random means that the
priority vector is initialized with random values (larger than 0),
and error means they are initialized with the Bellman error as in
GenPS. However, this requires one complete sweep over all states.

This implementation stops updating when the largest priority values
over all states is less than the specified \code{error}.

Since the algorithm does not sweep through the whole state space for each
iteration, \code{n} is converted into an equivalent number of state updates
\eqn{n = n |S|}.
}

Note that policies converge earlier than value functions.
}

\subsection{Linear Programming}{

A linear programming formulation was developed by Manne (1960) and further
described by Puterman (1996). For the
optimal value function, the Bellman equation holds:

\deqn{
 v^*(s) = \max_{a \in \mathcal{A}}\sum_{s' \in  \mathcal{S}} p(s, a, s') [ r(s, a, s') + \gamma v^*(s')]\; \forall a\in \mathcal{A}, s \in \mathcal{S}
}

The maximization problem can reformulate as a minimization with
a linear constraint for each state action pair.
The optimal value function can be found by solving the
following linear program:
\deqn{\text{min} \sum_{s\in S} v(s)}
subject to
\deqn{v(s) \ge r(s, a, s') + \gamma \sum_{s' \in \mathcal{S}} p(s, a, s')v(s'),\; \forall a\in \mathcal{A}, s \in \mathcal{S}
}

This optimization problem finds the optimal value function and the optimal policy.

Note:
\itemize{
\item The discounting factor has to be strictly less than 1.
\item The used solver does not support infinity and a sufficiently large
value needs to be used instead (see parameter \code{inf}).
\item Additional parameters to to \code{solve_MDP} are passed on to \code{\link[lpSolve:lp]{lpSolve::lp()}}.
}
}

\subsection{Monte Carlo Control}{

Monte Carlo control simulates a whole episode using the current behavior
policy and then updates the target policy before simulating the next episode.
Implemented are the following temporal difference control methods
described in Sutton and Barto (2020).
\itemize{
\item \strong{Monte Carlo Control with exploring Starts} uses the same greedy policy for
behavior and target (on-policy). To make sure all states/action pairs are
explored, it uses exploring starts meaning that new episodes are started at a randomly
chosen state using a randomly chooses action.
\item \strong{On-policy Monte Carlo Control} uses for behavior and as the target policy
an epsilon-greedy policy.
\item \strong{Off-policy Monte Carlo Control} uses for behavior an arbitrary policy
(we use an epsilon-greedy policy) and learns a greedy policy using
importance sampling.
}
}

\subsection{Temporal Difference Control}{

Implemented are several temporal difference control methods
described in Sutton and Barto (2020).
Note that the MDP transition and reward models are used
for these reinforcement learning methods only to sample from
the environment.
The algorithms use a step size parameter \eqn{\alpha} (learning rate) for the
updates and the exploration parameter \eqn{\epsilon} for
the \eqn{\epsilon}-greedy behavior policy.

If the model has absorbing states to terminate episodes, then no maximal episode length
(\code{horizon}) needs to
be specified. To make sure that the algorithm does finish in a reasonable amount of time,
episodes are stopped after 1000 actions (with a warning). For models without absorbing states,
the episode length has to be specified via \code{horizon}.
\itemize{
\item \strong{Q-Learning} (Watkins and Dayan 1992) is an off-policy temporal difference method that uses
an \eqn{\epsilon}-greedy behavior policy and learns a greedy target
policy.
\item \strong{Sarsa} (Rummery and Niranjan 1994) is an on-policy method that follows and learns
an \eqn{\epsilon}-greedy policy. The final \eqn{\epsilon}-greedy policy
is converted into a greedy policy.
\item \strong{Expected Sarsa} (R. S. Sutton and Barto 2018). We implement an on-policy version that uses
the expected value under the current policy for the update.
It moves deterministically in the same direction as Sarsa would
move in expectation. Because it uses the expectation, we can
set the step size \eqn{\alpha} to large values and 1 is common.
}
}

\subsection{Planning by Sampling}{

A simple, not very effective, planning method proposed by Sutton and Barto (2020) in Chapter 8.
\itemize{
\item \strong{Random-sample one-step tabular Q-planning} randomly selects a
state/action pair and samples the resulting reward and next state from
the model. This
information is used to update a single Q-table value.
}
}
}
\examples{
data(Maze)
Maze

# use value iteration
maze_solved <- solve_MDP(Maze, method = "value_iteration")
maze_solved
policy(maze_solved)

# plot the value function U
plot_value_function(maze_solved)

# Gridworld solutions can be visualized
gw_plot(maze_solved)

# Use linear programming
maze_solved <- solve_MDP(Maze, method = "lp")
maze_solved
policy(maze_solved)

# use prioritized sweeping (which is known to be fast for mazes)
maze_solved <- solve_MDP(Maze, method = "prioritized_sweeping")
policy(maze_solved)

# finite horizon
maze_solved <- solve_MDP(Maze, method = "value_iteration", horizon = 3)
policy(maze_solved)
gw_plot(maze_solved, epoch = 1)
gw_plot(maze_solved, epoch = 2)
gw_plot(maze_solved, epoch = 3)

# create a random policy where action n is very likely and approximate
#  the value function. We change the discount factor to .9 for this.
Maze_discounted <- Maze
Maze_discounted$discount <- .9
pi <- random_policy(Maze_discounted,
                    prob = c(n = .7, e = .1, s = .1, w = 0.1)
)
pi

# compare the utility function for the random policy with the function for the optimal
#  policy found by the solver.
maze_solved <- solve_MDP(Maze)

policy_evaluation(Maze, pi, k_backup = 100)
policy_evaluation(Maze, policy(maze_solved), k_backup = 100)

# Note that the solver already calculates the utility function and returns it with the policy
policy(maze_solved)

# Learn a Policy using Q-Learning
maze_learned <- solve_MDP(Maze, method = "q_learning", n = 100, horizon = 100)
maze_learned

maze_learned$solution
policy(maze_learned)
plot_value_function(maze_learned)
gw_plot(maze_learned)
}
\references{
Andre, D., Friedman, N., and Parr, R. 1997. "Generalized prioritized sweeping." In Advances in Neural Information Processing Systems 10, pp. 1001-1007. \href{https://proceedings.neurips.cc/paper_files/paper/1997/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf}{NeurIPS Proceedings}

Bellman, Richard. 1957. "A Markovian Decision Process." Indiana University Mathematics Journal 6: 679-84. \url{https://www.jstor.org/stable/24900506}.

Howard, R. A. 1960. "Dynamic Programming and Markov Processes." Cambridge, MA: MIT Press.

Li, Lihong, and Michael Littman. 2008. "Prioritized Sweeping Converges to the Optimal Value Function." DCS-TR-631. Rutgers University. \doi{10.7282/T3TX3JSX}

Manne, Alan. 1960. "On the Job-Shop Scheduling Problem." Operations Research 8 (2): 219-23. \doi{10.1287/opre.8.2.219}.

Moore, Andrew, and C. G. Atkeson. 1993. "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time." Machine Learning 13 (1): 103–30. \doi{10.1007/BF00993104}.

Puterman, Martin L. 1996. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons.

Puterman, Martin L., and Moon Chirl Shin. 1978. "Modified Policy Iteration Algorithms for Discounted Markov Decision Problems." Management Science 24: 1127-37. \doi{10.1287/mnsc.24.11.1127}.

Rummery, G., and Mahesan Niranjan. 1994. "On-Line Q-Learning Using Connectionist Systems." Techreport CUED/F-INFENG/TR 166. Cambridge University Engineering Department.

Russell, Stuart J., and Peter Norvig. 2020. Artificial Intelligence: A Modern Approach (4th Edition). Pearson. \url{http://aima.cs.berkeley.edu/}.

Sutton, R. 1988. "Learning to Predict by the Method of Temporal Differences." Machine Learning 3: 9-44. \url{https://link.springer.com/article/10.1007/BF00115009}.

Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Watkins, Christopher J. C. H., and Peter Dayan. 1992. "Q-Learning." Machine Learning 8 (3): 279-92. \doi{10.1007/BF00992698}.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{solver}
