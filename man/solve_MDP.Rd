% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP.R, R/solve_MDP_DP.R,
%   R/solve_MDP_LP.R, R/solve_MDP_MC.R, R/solve_MDP_SGD.R, R/solve_MDP_TD.R,
%   R/solve_MDP_sampling.R
\name{solve_MDP}
\alias{solve_MDP}
\alias{solve_MDP_DP}
\alias{solve_MDP_LP}
\alias{solve_MDP_MC}
\alias{solve_MDP_SGD}
\alias{solve_MDP_TD}
\alias{solve_MDP_TD_n_step}
\alias{solve_MDP_sampling}
\title{Solve an MDP Problem}
\usage{
solve_MDP(model, method = "value_iteration", ...)

solve_MDP_DP(
  model,
  method = "value_iteration",
  horizon = NULL,
  discount = NULL,
  n = 1000L,
  error = 0.001,
  k_backups = 10L,
  V = NULL,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)

solve_MDP_LP(
  model,
  method = "lp",
  horizon = NULL,
  discount = NULL,
  inf = 1000,
  verbose = FALSE,
  lpSolve_args = list(),
  ...
)

solve_MDP_MC(
  model,
  method = "MC_exploring_starts",
  horizon = NULL,
  discount = NULL,
  n = 100,
  Q = NULL,
  epsilon = NULL,
  alpha = NULL,
  first_visit = TRUE,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)

solve_MDP_SGD(
  model,
  method = "semi_gradient_sarsa",
  horizon = NULL,
  discount = NULL,
  q_func = NULL,
  q_gradient = NULL,
  alpha = 0.2,
  epsilon = 0.2,
  n = 1000,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)

solve_MDP_TD(
  model,
  method = "q_learning",
  horizon = NULL,
  discount = NULL,
  alpha = function(t, n) min(10/n, 1),
  epsilon = 0.2,
  n = 1000,
  Q = 0,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)

solve_MDP_TD_n_step(
  model,
  method = "n_step_sarsa_on_policy",
  horizon = NULL,
  discount = NULL,
  alpha = function(t, n) min(10/n, 1),
  n_step = 1,
  epsilon = 0.2,
  n = 1000,
  Q = 0,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)

solve_MDP_sampling(
  model,
  method = "q_planning",
  horizon = NULL,
  discount = NULL,
  alpha = function(t, n) min(10/n, 1),
  n = 1000,
  Q = NULL,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods:
\code{'value_iteration'},
\code{'policy_iteration'},
\code{'lp'},
\code{'q_learning'},
\code{'sarsa'},
\code{'expected_sarsa'},
\code{'n_step_SARSA_on_policy'},
\code{'n_step_SARSA_off_policy'},
\code{'MC_exploring_starts'},
\code{'MC_on_policy'},
\verb{'MC_off_policy', }'q_planning'`.}

\item{...}{further parameters are passed on to the solver function.}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{n}{number of episodes used for learning.}

\item{error}{value iteration: maximum Bellman error allowed for the
convergence criterion.}

\item{k_backups}{policy iteration: maximum number of Bellman backups used in
the iterative policy evaluation step. Policy evaluation typically converges earlier
with a maximum Bellman error less than \code{error}.}

\item{V}{a vector with initial state values. If
\code{NULL}, then the default of a vector of all 0s (\code{\link[=V_zero]{V_zero()}}) is used.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; Continue with an unconverged solution specified in \code{model}.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}

\item{inf}{value used for infinity when calling \code{lpSolve::lp()}. This
should me much larger/smaller than the largest/smallest
reward in the model.}

\item{lpSolve_args}{a list with additional arguments passed on to \code{lpSolve::lp()}.}

\item{Q}{a state-action value matrix.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies.}

\item{alpha}{step size as a function of the time step \code{t} and the number of times
the respective Q-value was updated \code{n} or a scalar. For expected Sarsa, alpha is
often set to 1.}

\item{first_visit}{if \code{TRUE} then only the first visit of a state/action pair
in an episode is used to update Q, otherwise, every-visit update is used.}

\item{n_step}{integer; steps for bootstrapping}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Implementation of value iteration, modified policy iteration and other
methods based on reinforcement learning techniques to solve finite
state space MDPs.
}
\details{
Several solvers are available. Note that some solvers are only implemented
for finite-horizon problems.

Most solvers can be interrupted using Esc/CTRL-C and will return the
current solution. Solving can be continued by calling \code{solve_MDP} with the
partial solution as the model and the parameter \code{continue = TRUE}. This method
can also be used to reduce parameters like \code{alpha} or \code{epsilon} (see Q-learning
in the Examples section).

Next, we describe the different types of available solvers.

\subsection{Dynamic Programming}{

Implemented are the following dynamic programming methods (following
Russell and Norvig, 2010):
\itemize{
\item \strong{(Modified) Policy Iteration} (Howard 1960; Puterman and Shin 1978)
starts with a random policy and iteratively performs
a sequence of
\enumerate{
\item (Approximate) policy evaluation to estimate the value function for the
current policy. Iterative policy evaluation can be approximated by
stopping early after \code{k_backups} iterations
(see \code{\link[=policy_evaluation]{policy_evaluation()}}. In this case the algorithm is called
\emph{modified} policy iteration.
\item Policy improvement is performed by updating the policy to be greedy
(see \code{\link[=greedy_policy]{greedy_policy()}})
with respect to the new value function.
The algorithm stops when it converges to a stable policy (i.e., no changes
between two iterations). Note that the policy typically stabilizes before
the value function converges.
}
\item \strong{Value Iteration} (Bellman 1957) starts with
an arbitrary value function (by default all 0s) and iteratively
updates the value function for each state using the Bellman update
equation (see \code{\link[=bellman_update]{bellman_update()}}).

\deqn{v(s) \leftarrow \max_{a \in \mathcal{A}(s)} \sum_{s'} p(s' | s,a) [r(s,a, s') + \gamma v(s')]}

The iteration
is terminated when the solution converges or the maximum of \code{n} iterations
has been reached.
Approximate convergence is achieved
for discounted problems (with \eqn{\gamma < 1})
when the maximal value function change for any state \eqn{\delta} is
\deqn{\delta \le \frac{error (1-\gamma)}{\gamma}.}
It can be shown that this means
that no state value is more than
\eqn{error} from the value in the optimal value function. For undiscounted
problems, we use \eqn{\delta \le error}.

A greedy policy
is extracted from the final value function. Value iteration can be seen as
policy iteration with policy evaluation truncated to one step.
\item \strong{Prioritized Sweeping} (Moore and Atkeson, 1993; Andre et al., 1997; Li and Littman, 2008)
approximate the optimal value
function by iteratively adjusting one state at a time. While value and policy iteration
sweep in every iteration through all states, prioritized sweeping
updates states in the order given by their priority.
The priority reflects how much a state value may change
given the most recently updated other states that can be directly reached via an action.
This update order often lead to faster convergence compared
to sweeping the whole state space in regular value iteration.

We implement the two priority update strategies described as \strong{PS} and
\strong{GenPS} by Li and Littman.
\itemize{
\item \strong{PS} (Moore and Atkeson, 1993) updates the priority of a state \eqn{H(s)}
using:
\deqn{
       \forall{s \in \mathcal{S}}: H_{t+1}(s)  \leftarrow \begin{cases}
         \max(H_{t}(s), \Delta_t \max_{a \in \mathcal{A}}(p(s_t|s,a)) \text{ for } s \ne s_{t+1} \\
         \Delta_t \max_{a \in A}(p(s_t|s,a) \text{ for } s = s_{t+1}
         \end{cases}
     }

where \eqn{\Delta_t = |V_{t+1}(s_t) - V_t(s_t)| = |E(s_t; V_{t+1})|}, i.e.,
the Bellman error for the updated state.
\item \strong{GenPS} (Andre et al., 1997) updates all state priorities using their
current Bellman error:

\deqn{\forall{s \in \mathcal{S}}: H_{t+1}(s) \leftarrow |E(s; V_{t+1})|}

where \eqn{E(s; V_{t+1}) = \max_{a \in A} \left[R(s,a) + \gamma \sum_{s \in S} p(s'|s,a) V(s')\right] - V(s)}
is a state's Bellman error.
}

The update method can be chosen using the additional parameter \code{H_update}
as the character string \code{"PS_random"}, \code{"PS_error"} or \code{"GenPS"}.
The default is \code{H_update = "GenPS"}. For PS, random means that the
priority vector is initialized with random values (larger than 0),
and error means they are initialized with the Bellman error as in
GenPS. However, this requires one complete sweep over all states.

This implementation stops updating when the largest priority values
over all states is less than the specified \code{error}.

Since the algorithm does not sweep through the whole state space for each
iteration, \code{n} is converted into an equivalent number of state updates
\eqn{n = n\ |S|}.
}
}

\subsection{Linear Programming}{

A linear programming formulation was developed by Manne (1960) and further
described by Puterman (1996). For the
optimal value function, the Bellman equation holds:

\deqn{
 v^*(s) = \max_{a \in \mathcal{A}}\sum_{s' \in  \mathcal{S}} p(s, a, s') [ r(s, a, s') + \gamma v^*(s')]\; \forall a\in \mathcal{A}, s \in \mathcal{S}
}

The maximization problem can reformulate as a minimization with
a linear constraint for each state action pair.
The optimal value function can be found by solving the
following linear program:
\deqn{\text{min} \sum_{s\in S} v(s)}
subject to
\deqn{v(s) \ge \sum_{s' \in \mathcal{S}} p(s, a, s')[r(s, a, s') + \gamma v(s')],\; \forall a\in \mathcal{A}, s \in \mathcal{S}
}

Note:
\itemize{
\item The discounting factor has to be strictly less than 1.
\item The used solver does not support infinity and a sufficiently large
value needs to be used instead (see parameter \code{inf}).
\item Additional parameters to to \code{solve_MDP} are passed on to \code{\link[lpSolve:lp]{lpSolve::lp()}}.
}
}

\subsection{Monte Carlo Control}{

The idea is to estimate the action value function for a policy as the
average of sampled returns.

\deqn{q_\pi(s,a) = \mathbb{E}_\pi[R_i|S_0=s,A_0=a] \approx \frac{1}{n} \sum_{i=1}^n R_i}

Monte Carlo control simulates a whole episode using the current behavior
policy and uses the sampled reward to update the Q values. For on-policy
methods, the behavior policy is updated to be greedy (i.e., optimal) with
respect to the new Q values. Then the next episode is simulated till
the predefined number of episodes is completed.

Implemented are the following temporal difference control methods
described in Sutton and Barto (2020).
\itemize{
\item \strong{Monte Carlo Control with exploring Starts} learns the optimal greedy policy.
It uses the same greedy policy for
behavior and target (on-policy learning).
After each episode, the policy is updated to be greedy with respect to the
current Q values.
To make sure all states/action pairs are
explored, it uses exploring starts meaning that new episodes are started at a randomly
chosen state using a randomly chooses action.
\item \strong{On-policy Monte Carlo Control} learns an epsilon-greedy policy
which it uses for behavior and as the target policy
(on-policy learning). An epsilon-greedy policy is used to provide
exploration. For calculating running averages, an update with \eqn{\alpha = 1/n}
is used by default. A different update factor can be set using the parameter \code{alpha}
as either a fixed value or a function with the signature \verb{function(t, n)}
which returns the factor in the range \eqn{[0,1]}.
\item \strong{Off-policy Monte Carlo Control} uses for behavior an arbitrary soft policy
(a soft policy has in each state a probability greater than 0 for all
possible actions).
We use an epsilon-greedy policy and the method learns a greedy policy using
importance sampling. Note: This method can only learn from the tail of the
sampled runs where greedy actions are chosen. This means that it is very
inefficient in learning the beginning portion of long episodes. This problem
is especially problematic when larger values for \eqn{\epsilon} are used.
}
}

\subsection{Semi-gradient TD(0)}{
}

\subsection{Temporal Difference Control}{

Implemented are several temporal difference control methods
described in Sutton and Barto (2020).
Note that the MDP transition and reward models are used
for these reinforcement learning methods only to sample from
the environment.

The implementation uses an \eqn{\epsilon}-greedy behavior policy,
where the parameter \code{epsilon} controls the degree of exploration.
The algorithms use a step size parameter \eqn{\alpha} (learning rate).
The learning rate \code{alpha} can be specified as a
function with the signature \verb{function(t, n)}, where \code{t} is the number of episodes
processed and \code{n} is the number of updates for the entry in the Q-table.

The general update is
\deqn{
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [G_t - Q(S_t,A_t)],
}
where \eqn{G_t} is the target estimate for the given q-value. The different methods below
estimate the target value differently.

If the model has absorbing states to terminate episodes, then no maximal episode length
(\code{horizon}) needs to
be specified. To make sure that the algorithm does finish in a reasonable amount of time,
episodes are stopped after 1000 actions (with a warning). For models without absorbing states,
the episode length has to be specified via \code{horizon}.
\itemize{
\item \strong{Q-Learning} (Watkins and Dayan 1992) is an off-policy temporal difference method that uses
an \eqn{\epsilon}-greedy behavior policy and learns a greedy target
policy. The target value is estimated as the one-step bootstrapping using the
target greedy policy:
\deqn{G_t = R_{t+1} + \gamma \max_a Q(S_{t+1}, a)}
\item \strong{Sarsa} (Rummery and Niranjan 1994) is an on-policy method that follows and learns
the same policy. Here a an \eqn{\epsilon}-greedy policy is used.
The final \eqn{\epsilon}-greedy policy is converted into a greedy policy.
\eqn{\epsilon} can be lowered over time (see parameter \code{continue})
to learn a greedy policy. The target is estimated
as the one-step bootstrapping following the behavior policy:
\deqn{G_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})}
\item \strong{Expected Sarsa} (R. S. Sutton and Barto 2018). We implement an on-policy
Sarsa with an \eqn{\epsilon}-greedy policy which uses the
the expected value under the current policy for the update.
It moves deterministically in the same direction as Sarsa would
move in expectation.
\deqn{G_t = R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a)}

Because it uses the expectation, we can
set the step size \eqn{\alpha} to large values and 1 is common.
The off-policy use of expected Sarsa simplifies to
the Q-learning algorithm.
\item \strong{On and off-policy n-step Sarsa} (R. S. Sutton and Barto 2018).
Estimate the return using the last \eqn{n} timesteps:
\deqn{
  G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})
  }

This estimate is used as the target for Sarsa. For the off-policy case,
the update uses the importance sampling ratio. Note that updates are delayed
\eqn{n} steps in this backward looking algorithm.
}
}

\subsection{Planning by Sampling}{

A simple, not very effective, planning method proposed by Sutton and Barto (2020) in Chapter 8.
\itemize{
\item \strong{Random-sample one-step tabular Q-planning} randomly selects a
state/action pair and samples the resulting reward and next state from
the model. This
information is used to update a single Q-table value.
}
}
}
\examples{
data(Maze)
Maze

# use value iteration
maze_solved <- solve_MDP(Maze, method = "value_iteration")
maze_solved
policy(maze_solved)

# plot the value function U
plot_value_function(maze_solved)

# Gridworld solutions can be visualized
gw_plot(maze_solved)

# Use linear programming
maze_solved <- solve_MDP(Maze, method = "lp")
maze_solved
policy(maze_solved)

# use prioritized sweeping (which is known to be fast for mazes)
maze_solved <- solve_MDP(Maze, method = "prioritized_sweeping")
policy(maze_solved)

# finite horizon
maze_solved <- solve_MDP(Maze, method = "value_iteration", horizon = 3)
policy(maze_solved)
gw_plot(maze_solved, epoch = 1)
gw_plot(maze_solved, epoch = 2)
gw_plot(maze_solved, epoch = 3)

# create a random policy where action n is very likely and approximate
#  the value function. We change the discount factor to .9 for this.
Maze_discounted <- Maze
Maze_discounted$discount <- .9
pi <- random_policy(Maze_discounted,
                    prob = c(n = .7, e = .1, s = .1, w = 0.1)
)
pi

# compare the utility function for the random policy with the function for the optimal
#  policy found by the solver.
maze_solved <- solve_MDP(Maze)

policy_evaluation(Maze, pi, k_backup = 100)
policy_evaluation(Maze, policy(maze_solved), k_backup = 100)

# Note that the solver already calculates the utility function and returns it with the policy
policy(maze_solved)

# Learn a Policy using Q-Learning
maze_learned <- solve_MDP(Maze, method = "q_learning", 
    epsilon = 0.2, n = 500, horizon = 100, verbose = TRUE)
maze_learned

policy(maze_learned)
plot_value_function(maze_learned)
gw_plot(maze_learned)

# Keep on learning, but with a reduced epsilon
maze_learned <- solve_MDP(maze_learned, method = "q_learning",
    epsilon = 0.01, n = 500, horizon = 100, continue = TRUE, verbose = TRUE)

policy(maze_learned)
plot_value_function(maze_learned)
gw_plot(maze_learned)

}
\references{
Andre, D., Friedman, N., and Parr, R. 1997. "Generalized prioritized sweeping." In Advances in Neural Information Processing Systems 10, pp. 1001-1007. \href{https://proceedings.neurips.cc/paper_files/paper/1997/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf}{NeurIPS Proceedings}

Bellman, Richard. 1957. "A Markovian Decision Process." Indiana University Mathematics Journal 6: 679-84. \url{https://www.jstor.org/stable/24900506}.

Howard, R. A. 1960. "Dynamic Programming and Markov Processes." Cambridge, MA: MIT Press.

Li, Lihong, and Michael Littman. 2008. "Prioritized Sweeping Converges to the Optimal Value Function." DCS-TR-631. Rutgers University. \doi{10.7282/T3TX3JSX}

Manne, Alan. 1960. "On the Job-Shop Scheduling Problem." Operations Research 8 (2): 219-23. \doi{10.1287/opre.8.2.219}.

Moore, Andrew, and C. G. Atkeson. 1993. "Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time." Machine Learning 13 (1): 103â€“30. \doi{10.1007/BF00993104}.

Puterman, Martin L. 1996. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons.

Puterman, Martin L., and Moon Chirl Shin. 1978. "Modified Policy Iteration Algorithms for Discounted Markov Decision Problems." Management Science 24: 1127-37. \doi{10.1287/mnsc.24.11.1127}.

Rummery, G., and Mahesan Niranjan. 1994. "On-Line Q-Learning Using Connectionist Systems." Techreport CUED/F-INFENG/TR 166. Cambridge University Engineering Department.

Russell, Stuart J., and Peter Norvig. 2020. Artificial Intelligence: A Modern Approach (4th Edition). Pearson. \url{http://aima.cs.berkeley.edu/}.

Sutton, R. 1988. "Learning to Predict by the Method of Temporal Differences." Machine Learning 3: 9-44. \url{https://link.springer.com/article/10.1007/BF00115009}.

Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Watkins, Christopher J. C. H., and Peter Dayan. 1992. "Q-Learning." Machine Learning 8 (3): 279-92. \doi{10.1007/BF00992698}.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{solver}
