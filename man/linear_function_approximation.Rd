% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/linear_function_approximation.R
\name{linear_function_approximation}
\alias{linear_function_approximation}
\alias{q_approx_linear}
\alias{v_approx_linear}
\alias{pi_approx_linear}
\alias{approx_value}
\title{Linear Function Approximation}
\usage{
q_approx_linear(model, transformation = transformation_linear_basis, ...)

v_approx_linear(model, transformation = transformation_linear_basis, ...)

pi_approx_linear(model, transformation = transformation_linear_basis, ...)

approx_value(f, state, action = NULL, w = NULL, model = NULL)
}
\arguments{
\item{model}{a \link{MDP} model with defined state features.}

\item{transformation}{a transformation function. See \link{transformation}.}

\item{...}{further parameters are passed on to the \link{transformation} function.}

\item{f}{a linear approximation object.}

\item{state}{a state or state features.}

\item{action}{an action. If \code{NULL}, then the value for all available actions
will be calculated.}

\item{w}{a weight vector to be used instead of \code{f$w}.}
}
\description{
Approximate a Q-function a value function or a policy using linear
function approximation. These approximation functions are
used internally by: \link{solve_MDP_APPROX}
}
\details{
\subsection{Linear Approximation}{

The state-action value function is approximated by
\deqn{\hat{q}(s,a) = \boldsymbol{w}^\top\phi(s,a),}

where \eqn{\boldsymbol{w} \in \mathbb{R}^n} is a weight vector
and \eqn{\phi: S \times A \rightarrow \mathbb{R}^n}  is a
feature function that
maps each state-action pair to a feature vector.
Linear approximation has a single optimum and can be optimized using
a simple update rule following the gradient of the state-action function
\deqn{\nabla \hat{q}(s,a,\boldsymbol{w}) = \phi(s,a).}

Value function approximation works in the same way but \eqn{\phi(s)} is used.
}

\subsection{State-action Feature Vector Construction}{

For a small number of actions, we can
follow the construction described by Geramifard et al (2013)
which uses a state feature function \eqn{\phi: S \rightarrow \mathbb{R}^{m}}
to construct the complete state-action feature vector.
Here, we also add an intercept term.
The state-action feature vector has length \eqn{1 + |A| \times m}.
It has the intercept and then one component for each action. All these components
are set to zero and only the active action component is set to \eqn{\phi(s)},
where \eqn{s} is the current state.
For example, for the state feature
vector \eqn{\phi(s) = (3,4)} and action \eqn{a=2} out of three possible
actions \eqn{A = \{1, 2, 3\}}, the complete
state-action feature vector is \eqn{\phi(s,a) = (0,0,0,1,3,4,0,0,0)}.
Each action component has three entries and the 1 represent the intercept
for the state feature vector. The zeros represent the components for the
two not chosen actions.

The construction of the state-action values is implemented in \code{add_linear_approx_Q_function()}.

The state feature function \eqn{\phi()} starts with raw state feature vector
\eqn{\mathbf{x} = (x_1,x_2, ..., x_m)} that
are either user-specified or constructed by parsing the state labels of
form \verb{s(feature list)}.  Then an optional nonlinear transformation
can be performed (see \link{transformation}).
}

\subsection{Internal Representation}{

All approximations are lists with the elements:
\itemize{
\item \code{x(s, a)} ... function to construct from state features, action-state features.
\item \code{f(s, a, w)} ... approx. function
\item \code{gradient(s, a, w)} ... gradient of f at w
\item \code{w} ... the weight vector (initially all 0s)
\item \code{transformation} ... a transformation kernel function that is applied to state features in x.
}
}

\subsection{Prediction}{

\code{approx_value()} calculates approximate value given the weights in
the model or specified weights.
}
}
\examples{
data(Maze)

f_q <- q_approx_linear(Maze)
f_q
approx_value(f_q, 1, 1, model = Maze)

f_v <- v_approx_linear(Maze)
f_v
approx_value(f_v, 1, model = Maze)

# TODO: Implement
# f_pi <- pi_approx_linear(Maze)
# f_pi

}
\references{
Alborz Geramifard, Thomas J. Walsh, Stefanie Tellex, Girish Chowdhary, Nicholas Roy, and Jonathan P. How. 2013. A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning. Foundations and Trends in Machine Learning 6(4), December 2013, pp. 375-451. \doi{10.1561/2200000042}

Konidaris, G., Osentoski, S., & Thomas, P. 2011. Value Function Approximation in Reinforcement Learning Using the Fourier Basis. Proceedings of the AAAI Conference on Artificial Intelligence, 25(1), 380-385. \doi{10.1609/aaai.v25i1.7903}
}
