% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_PG.R
\name{solve_MDP_PG}
\alias{solve_MDP_PG}
\title{Solve MDPs with Policy Gradient Methods}
\usage{
solve_MDP_PG(
  model,
  method = "actor-critic",
  horizon = NULL,
  discount = NULL,
  alpha_actor = schedule_exp(0.2, 0.1),
  alpha_critic = schedule_exp(0.2, 0.1),
  epsilon = schedule_exp(1, 0.1),
  lambda = 0,
  n,
  w = NULL,
  theta = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods: \code{'sarsa'}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies. A scalar value between 0 and 1 or a
\link{schedule}.}

\item{lambda}{the trace-decay parameter for the an accumulating trace. If \code{lambda = 0}
then 1-step Sarsa is used.}

\item{n}{number of episodes used for learning.}

\item{w}{an initial weight vector. By default a vector with 0s is used.}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}
}
\value{
\code{solve_MDP()} returns an object of class MDP or MDPTF which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements that depend on the used method. Common
elements are:
\itemize{
\item \code{method} with the name of the used method
\item parameters used.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
}
}
\description{
Solve the MDP control problem using
a parameterized policy and policy gradient methods.
The implemented method one-step actor-critic control and
actor-critic control with eligibility traces.
}
\examples{
E EXAMPLE
}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.
}
\seealso{
Other solver: 
\code{\link{add_linear_approx_Q_function}()},
\code{\link{schedule}},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()}

Other MDPTF: 
\code{\link{MDPTF}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{add_linear_approx_Q_function}()},
\code{\link{reachable_states}()},
\code{\link{sample_MDP.MDPTF}()},
\code{\link{solve_MDP}()},
\code{\link{start}()}
}
\concept{MDPTF}
\concept{solver}
