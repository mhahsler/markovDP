% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_APPROX.R
\name{solve_MDP_APPROX}
\alias{solve_MDP_APPROX}
\alias{add_linear_approx_Q_function}
\alias{add_linear_approx_Q_function.MDP}
\alias{add_linear_approx_Q_function.MDPTF}
\alias{transformation_fourier}
\alias{approx_Q_value}
\alias{approx_greedy_action}
\alias{approx_greedy_policy}
\title{Episodic Semi-gradient Sarsa with Linear Function Approximation}
\usage{
solve_MDP_APPROX(
  model,
  method = "semi_gradient_sarsa",
  horizon = NULL,
  discount = NULL,
  alpha = 0.01,
  epsilon = 0.2,
  n = 1000,
  w = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

add_linear_approx_Q_function(model, ...)

\method{add_linear_approx_Q_function}{MDP}(
  model,
  state_features = NULL,
  transformation = NULL,
  ...
)

\method{add_linear_approx_Q_function}{MDPTF}(model, transformation = NULL, ...)

transformation_fourier(min, max, order, cs = expand.grid(0:order, 0:order))

approx_Q_value(model, state = NULL, action = NULL, w = NULL)

approx_greedy_action(model, state, w = NULL, epsilon = 0)

approx_greedy_policy(model, w = NULL)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods: \code{'semi_gradient_sarsa'}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{alpha}{step size.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies.}

\item{n}{number of episodes used for learning.}

\item{w}{a weight vector}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}

\item{state_features}{a matrix with state features. Each row is the feature
vector for a state.}

\item{transformation}{a transformation function that is applied to the feature vector
before it is used in the linear approximation.}

\item{min, max}{vectors with the minimum and maximum values for each feature.
This is used to scale the feature to the \eqn{[0,1]} interval for the
Fourier basis.}

\item{order}{order for the Fourier basis.}

\item{cs}{an optional matrix or a data frame to specify cs values for the
Fourier features selectively (overrides \code{order}).}

\item{state}{a state (index or name)}

\item{action}{an action (index or name)}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
MDP control using state-value approximation. Semi-gradient Sarsa for
episodic problems.
}
\details{
\subsection{Linear Approximation}{

The state-action value function is approximated by
\deqn{\hat{q}(s,a) = \boldsymbol{w}^\top\phi(s,a),}

where \eqn{\boldsymbol{w} \in \mathbb{R}^n} is a weight vector
and \eqn{\phi: S \times A \rightarrow \mathbb{R}^n}  is a
feature function that
maps each state-action pair to a feature vector.
The gradient of the state-action function is
\deqn{\nabla \hat{q}(s,a,\boldsymbol{w}) = \phi(s,a).}
}

\subsection{State-action Feature Vector Construction}{

For a small number of actions, we can
follow the construction described by Geramifard et al (2013)
which uses a state feature function \eqn{\phi: S \rightarrow \mathbb{R}^{m}}
to construct the complete state-action feature vector.
Here, we also add an intercept term.
The state-action feature vector has length \eqn{1 + |A| \times m}.
It has the intercept and then one component for each action. All these components
are set to zero and only the active action component is set to \eqn{\phi(s)},
where \eqn{s} is the current state.
For example, for the state feature
vector \eqn{\phi(s) = (3,4)} and action \eqn{a=2} out of three possible
actions \eqn{A = \{1, 2, 3\}}, the complete
state-action feature vector is \eqn{\phi(s,a) = (1,0,0,3,4,0,0)}.
The leading 1 is for the intercept and the zeros represent the two not
chosen actions.

This construction is implemented in \code{add_linear_approx_Q_function()}.
}

\subsection{Helper Functions}{

The following helper functions for using approximation are available:
\itemize{
\item \code{approx_Q_value()} calculates approximate Q values given the weights in
the model or specified weights.
\item \code{approx_greedy_action()} uses approximate Q values given the weights in
the model or specified weights to find the the greedy action for a
state.
\item \code{approx_greedy_policy()} calculates the greedy-policy
for the approximate Q values given the weights in
the model or specified weights.
}
}

\subsection{Episodic Semi-gradient Sarsa}{

The implementation follows the algorithm given in Sutton and Barto (2018).
}
}
\examples{
# Example 1: A maze without walls. The step cost is 1. The start is top-left and
# the goal (+100 reward) is bottom-right.
# This is the ideal problem for a linear approximation of the Q-function
# using the x/y location as state features.

m <- gw_maze_MDP(c(5, 5), start = "s(1,1)", goal = "s(5,5)")

# construct state features as the x/y coordinates in the gridworld
state_features <- gw_s2rc(S(m))
state_features

m <- add_linear_approx_Q_function(m, state_features)

# constructed state-action features (X) and approximate Q function
# and gradient
m$approx_Q_function

sol <- solve_MDP_APPROX(m, horizon = 1000, n = 10,
                     alpha = 0.01, epsilon = .5)
gw_plot(sol)
sol <- solve_MDP_APPROX(sol, horizon = 1000, n = 100,
          alpha = 0.01, epsilon = 0.05, verbose = FALSE, continue = TRUE)
gw_plot(sol)

### TESTS
sol <- solve_MDP_APPROX(m, horizon = 1000, n = 1,
                     alpha = 0.01, epsilon = .8, verbose = 2)
Q_values(sol)
gw_plot(sol)

gw_animate(m, method = "APPROX:semi", horizon = 1000, n = 10,
                     alpha = 0.01, epsilon = .8)

###

gw_matrix(sol, what = "value")

# learned weights and state values
sol$solution$w

# extracting approximate Q-values
approx_greedy_action(sol, "s(4,5)")
approx_Q_value(sol, "s(4,5)", "down")
approx_Q_value(sol)

# extracting a greedy policy using the approximate Q-values
approx_greedy_policy(sol)

# Example 2: Stuart Russell's 3x4 Maze using approximation
# The wall and the -1 absorbing state make linear approximation
# using just the position more difficult.
data(Maze)
gw_plot(Maze)

Maze_approx <- add_linear_approx_Q_function(Maze)
sol <- solve_MDP_APPROX(Maze_approx, horizon = 100, n = 100,
                     alpha = 0.01, epsilon = 0.3)
gw_plot(sol)

# Example 3: Use order-2 Fourier basis for features
order <- 2
cs <- expand.grid(0:order, 0:order)

# convert state features to Fourier basis features
x <- state2features(S(Maze))
x

x <- sweep(x, MARGIN = 2,
           STATS = apply(x, MARGIN = 2, max), FUN = "/")
x <- apply(cs, MARGIN = 1,
           FUN = function(c) cos(pi * x \%*\% c))
rownames(x) <- S(Maze)
x

Maze_approx <- add_linear_approx_Q_function(Maze, state_features = x)

sol <- solve_MDP_APPROX(Maze_approx, horizon = 100, n = 100, alpha = 0.1, epsilon = .5)
sol <- solve_MDP_APPROX(sol, horizon = 100, n = 1000, alpha = 0.1, epsilon = .1, continue = TRUE)
gw_plot(sol)

# order-2 fourier features can be specified easier
Maze_approx <- add_linear_approx_Q_function(Maze, 
      transformation = transformation_fourier(min = c(0,0), max = c(3,4), order = 2))
sol <- solve_MDP_APPROX(Maze_approx, horizon = 1000, n = 200, alpha = 0.1, epsilon = .1)
gw_plot(sol)

}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Alborz Geramifard, Thomas J. Walsh, Stefanie Tellex, Girish Chowdhary, Nicholas Roy, and Jonathan P. How. 2013. A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning. Foundations and Trends in Machine Learning 6(4), December 2013, pp. 375-451. \doi{10.1561/2200000042}
}
\seealso{
Other solver: 
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()}

Other MDPTF: 
\code{\link{MDPTF}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{sample_MDP.MDPTF}()},
\code{\link{solve_MDP}()},
\code{\link{start}()}
}
\concept{MDPTF}
\concept{solver}
