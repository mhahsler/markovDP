% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/linear_function_approximation.R,
%   R/solve_MDP_APPROX.R
\name{add_linear_approx_Q_function}
\alias{add_linear_approx_Q_function}
\alias{approx_Q_value}
\alias{approx_greedy_action}
\alias{approx_greedy_policy}
\alias{approx_V_plot}
\alias{solve_MDP_APPROX}
\title{Solve MDPs with Temporal Differencing with Linear Function Approximation}
\usage{
add_linear_approx_Q_function(
  model,
  state_features = NULL,
  transformation = transformation_linear_basis,
  ...
)

approx_Q_value(model, state = NULL, action = NULL, w = NULL)

approx_greedy_action(model, state, w = NULL, epsilon = 0)

approx_greedy_policy(model, w = NULL)

approx_V_plot(
  model,
  min = NULL,
  max = NULL,
  w = NULL,
  res = 25,
  col = hcl.colors(res, "YlOrRd", rev = TRUE),
  image = TRUE,
  contour = TRUE,
  main = NULL,
  ...
)

solve_MDP_APPROX(
  model,
  method = "sarsa",
  horizon = NULL,
  discount = NULL,
  alpha = schedule_exp(0.2, 0.1),
  epsilon = schedule_exp(1, 0.1),
  lambda = 0,
  n,
  w = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{state_features}{a matrix with state features. Each row is the feature
vector for a state.}

\item{transformation}{a transformation function that is applied to the feature vector
before it is used in the linear approximation. See \link{transformation}.}

\item{...}{further parameters are passed on to the solver function.}

\item{state}{a state (index or name)}

\item{action}{an action (index or name)}

\item{w}{an initial weight vector. By default a vector with 0s is used.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies. A scalar value between 0 and 1 or a
\link{schedule}.}

\item{res}{resolution as the number of values sampled from each feature.}

\item{col}{colors for the passed on to \code{\link[=image]{image()}}.}

\item{image, contour}{logical; include the false color image or the
contours in the plot?}

\item{main}{title for the plot.}

\item{method}{string; one of the following solution methods: \code{'sarsa'}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{alpha}{step size (learning rate). A scalar value between 0 and 1 or a
\link{schedule}.}

\item{lambda}{the trace-decay parameter for the an accumulating trace. If \code{lambda = 0}
then 1-step Sarsa is used.}

\item{n}{number of episodes used for learning.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}
}
\value{
\code{solve_MDP()} returns an object of class MDP or MDPTF which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements that depend on the used method. Common
elements are:
\itemize{
\item \code{method} with the name of the used method
\item parameters used.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
}
}
\description{
Solve the MDP control problem using state-value approximation
by semi-gradient Sarsa (temporal differencing) for
episodic problems.
}
\details{
\subsection{Linear Approximation}{

The state-action value function is approximated by
\deqn{\hat{q}(s,a) = \boldsymbol{w}^\top\phi(s,a),}

where \eqn{\boldsymbol{w} \in \mathbb{R}^n} is a weight vector
and \eqn{\phi: S \times A \rightarrow \mathbb{R}^n}  is a
feature function that
maps each state-action pair to a feature vector.
Linear approximation has a single optimum and can be optimized using
a simple update rule following the gradient of the state-action function
\deqn{\nabla \hat{q}(s,a,\boldsymbol{w}) = \phi(s,a).}
}

\subsection{State-action Feature Vector Construction}{

For a small number of actions, we can
follow the construction described by Geramifard et al (2013)
which uses a state feature function \eqn{\phi: S \rightarrow \mathbb{R}^{m}}
to construct the complete state-action feature vector.
Here, we also add an intercept term.
The state-action feature vector has length \eqn{1 + |A| \times m}.
It has the intercept and then one component for each action. All these components
are set to zero and only the active action component is set to \eqn{\phi(s)},
where \eqn{s} is the current state.
For example, for the state feature
vector \eqn{\phi(s) = (3,4)} and action \eqn{a=2} out of three possible
actions \eqn{A = \{1, 2, 3\}}, the complete
state-action feature vector is \eqn{\phi(s,a) = (0,0,0,1,3,4,0,0,0)}.
Each action component has three entries and the 1 represent the intercept
for the state feature vector. The zeros represent the components for the
two not chosen actions.

The construction of the state-action values is implemented in \code{add_linear_approx_Q_function()}.

The state feature function \eqn{\phi()} starts with raw state feature vector
\eqn{\mathbf{x} = (x_1,x_2, ..., x_m)} that
are either user-specified or constructed by parsing the state labels of
form \verb{s(feature list)}.  Then an optional nonlinear transformation
can be performed (see \link{transformation}).
}

\subsection{Helper Functions}{

The following helper functions for using approximation are available:
\itemize{
\item \code{approx_Q_value()} calculates approximate Q values given the weights in
the model or specified weights.
\item \code{approx_greedy_action()} uses approximate Q values given the weights in
the model or specified weights to find the the greedy action for a
state.
\item \code{approx_greedy_policy()} calculates the greedy-policy
for the approximate Q values given the weights in
the model or specified weights.
}
}

\subsection{Episodic Semi-gradient Sarsa}{

The implementation follows the temporal difference algorithm
episodic Semi-gradient Sarsa algorithm given in Sutton and Barto (2018).
}

\subsection{Schedules}{
\itemize{
\item epsilon schedule: \code{t} is increased by each processed episode.
\item alpha schedule: \code{t} is increased by each processed episode.
}
}
}
\examples{
# Example 1: A maze without walls. The step cost is 1. The start is top-left and
# the goal (+100 reward) is bottom-right.
# This is the ideal problem for a linear approximation of the Q-function
# using the x/y location as state features.

m <- gw_maze_MDP(c(5, 5), start = "s(1,1)", goal = "s(5,5)")

# construct state features as the x/y coordinates in the gridworld
state_features <- gw_s2rc(S(m))
state_features

m <- add_linear_approx_Q_function(m, state_features)

# constructed state-action features (X) and approximate Q function
# and gradient
m$approx_Q_function

set.seed(1000)
sol <- solve_MDP_APPROX(m, horizon = 1000, n = 100)

gw_plot(sol)
gw_matrix(sol, what = "value")

# learned weights and state values
sol$solution$w

# the approximate value function can be visualized for states 
# with two features.
approx_V_plot(sol)

# extracting approximate Q-values
approx_greedy_action(sol, "s(4,5)")
approx_Q_value(sol, "s(4,5)", "down")
approx_Q_value(sol)

# extracting a greedy policy using the approximate Q-values
approx_greedy_policy(sol)


# Example 2: Stuart Russell's 3x4 Maze using linear basis approximation
# The wall and the -1 absorbing state make linear approximation
# using just the position directly more difficult.

data(Maze)
gw_plot(Maze)

# if no state features are specified, then they are constructed
# by parsing the state label of the form s(feature list).
Maze_approx <- add_linear_approx_Q_function(Maze)

set.seed(1000)
sol <- solve_MDP_APPROX(Maze_approx, horizon = 100, n = 100,
                        alpha = schedule_exp(0.3, 0.01),
                        epsilon = schedule_exp(1, 0.1))
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol, res = 20)


# Example 3: Stuart Russell's 3x4 Maze using
#            order-1 Fourier basis for approximation

Maze_approx <- add_linear_approx_Q_function(Maze,
      transformation = transformation_fourier_basis, order = 1)

set.seed(1000)
sol <- solve_MDP_APPROX(Maze_approx, horizon = 100, n = 100,
                    alpha = schedule_exp(0.3, .01),
                    epsilon = schedule_exp(1, .1))
                    
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol, res = 20)

# Example 4: Stuart Russell's 3x4 Maze using
#            order-2 Fourier basis for approximation
#            and eligibility traces: Sarsa(lambda)

Maze_approx <- add_linear_approx_Q_function(Maze,
      transformation = transformation_fourier_basis, order = 1)

set.seed(1000)
sol <- solve_MDP_APPROX(Maze_approx, horizon = 100, n = 100,
                    alpha = schedule_exp(0.3, .01),
                    epsilon = schedule_exp(1, .1),
                    lambda = 0.1)
                    
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol, res = 20)

 
}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.

Alborz Geramifard, Thomas J. Walsh, Stefanie Tellex, Girish Chowdhary, Nicholas Roy, and Jonathan P. How. 2013. A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning. Foundations and Trends in Machine Learning 6(4), December 2013, pp. 375-451. \doi{10.1561/2200000042}

Konidaris, G., Osentoski, S., & Thomas, P. 2011. Value Function Approximation in Reinforcement Learning Using the Fourier Basis. Proceedings of the AAAI Conference on Artificial Intelligence, 25(1), 380-385. \doi{10.1609/aaai.v25i1.7903}
}
\seealso{
Other solver: 
\code{\link{schedule}},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_PG}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()}

Other MDPTF: 
\code{\link{MDPTF}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{reachable_states}()},
\code{\link{sample_MDP.MDPTF}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_PG}()},
\code{\link{start}()}
}
\concept{MDPTF}
\concept{solver}
