% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_APPROX.R
\name{solve_MDP_APPROX}
\alias{solve_MDP_APPROX}
\alias{approx_Q_value}
\alias{approx_greedy_action}
\alias{approx_greedy_policy}
\alias{approx_V_plot}
\title{Solve MDPs with Temporal Differencing with Function Approximation}
\usage{
solve_MDP_APPROX(
  model,
  method = "sarsa",
  horizon = NULL,
  discount = NULL,
  alpha = schedule_exp(0.2, 0.1),
  epsilon = schedule_exp(1, 0.1),
  lambda = 0,
  n,
  state_features = NULL,
  transformation = transformation_linear_basis,
  w = NULL,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)

approx_Q_value(model, state = NULL, action = NULL, w = NULL)

approx_greedy_action(model, state, w = NULL, epsilon = 0, as = "factor")

approx_greedy_policy(model, w = NULL)

approx_V_plot(
  model,
  min = NULL,
  max = NULL,
  w = NULL,
  res = 25,
  col = hcl.colors(res, "YlOrRd", rev = TRUE),
  image = TRUE,
  contour = TRUE,
  main = NULL,
  ...
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods: \code{'sarsa'}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{alpha}{step size (learning rate). A scalar value between 0 and 1 or a
\link{schedule}.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies. A scalar value between 0 and 1 or a
\link{schedule}.}

\item{lambda}{the trace-decay parameter for the an accumulating trace. If \code{lambda = 0}
then 1-step Sarsa is used.}

\item{n}{number of episodes used for learning.}

\item{state_features}{a matrix with one row per state with state features
to be used. If \code{NULL} then \code{\link[=get_state_features]{get_state_features()}} will be used to get
the state features stored in the model, or to construct
state features from state labels.}

\item{transformation}{a transformation function. See \link{transformation}.}

\item{w}{a weight vector}

\item{...}{further parameters are passed on to the \link{transformation} function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}

\item{state}{a state (index or name)}

\item{action}{an action (index or name)}

\item{as}{character; specifies the desired output format (see \code{\link[=normalize_action]{normalize_action()}})}

\item{min, max}{numeric vectors with minimum/maximum values for each feature
in the state feature representation.}

\item{res}{resolution as the number of values sampled from each feature.}

\item{col}{colors for the passed on to \code{\link[=image]{image()}}.}

\item{image, contour}{logical; include the false color image or the
contours in the plot?}

\item{main}{title for the plot.}
}
\value{
\code{solve_MDP()} returns an object of class MDP or MDPTF which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements that depend on the used method. Common
elements are:
\itemize{
\item \code{method} with the name of the used method
\item parameters used.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
}
}
\description{
Solve the MDP control problem using state-value approximation
by semi-gradient Sarsa (temporal differencing) for
episodic problems.
}
\details{
\subsection{Episodic Semi-gradient Sarsa}{

The implementation follows the temporal difference algorithm
episodic Semi-gradient Sarsa algorithm given in Sutton and Barto (2018).
}

\subsection{Schedules}{
\itemize{
\item epsilon schedule: \code{t} is increased by each processed episode.
\item alpha schedule: \code{t} is increased by each processed episode.
}
}
}
\examples{
# Example 1: A maze without walls. The step cost is 1. The start is top-left and
# the goal (+100 reward) is bottom-right.
# This is the ideal problem for a linear approximation of the Q-function
# using the x/y location as state features.

m <- gw_maze_MDP(c(5, 5), start = "s(1,1)", goal = "s(5,5)")

# gridworlds have state labels om the format "s(row, col)" which can be
# automatically converted into state features used for approximation.
S(m)
get_state_features(m)

# solve using linear state features (no transformation) 
set.seed(1000)
sol <- solve_MDP_APPROX(m, horizon = 1000, n = 100) 

# approximation
sol$solution$q_approx_linear

gw_plot(sol)
gw_matrix(sol, what = "value")

# the approximate value function can be visualized for states 
# with two features.
approx_V_plot(sol)

# extracting approximate Q-values
approx_greedy_action(sol, "s(4,5)")
approx_Q_value(sol, "s(4,5)", "down")
approx_Q_value(sol)

# extracting a greedy policy using the approximate Q-values
approx_greedy_policy(sol)


# Example 2: Stuart Russell's 3x4 Maze using linear basis approximation
# The wall and the -1 absorbing state make linear approximation
# using just the position directly more difficult.

data(Maze)
gw_plot(Maze)

# if no state features are specified, then they are constructed
# by parsing the state label of the form s(feature list).
set.seed(1000)
sol <- solve_MDP_APPROX(Maze, horizon = 100, n = 100,
                        alpha = schedule_exp(0.3, 0.01),
                        epsilon = schedule_exp(1, 0.1))
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol, res = 20)


# Example 3: Stuart Russell's 3x4 Maze using
#            order-1 Fourier basis for approximation and
#            1-step Sarsa

set.seed(1000)
sol <- solve_MDP_APPROX(Maze, horizon = 100, n = 100,
                    alpha = schedule_exp(0.3, .01),
                    epsilon = schedule_exp(1, .1),
                    transformation = transformation_fourier_basis, 
                    order = 1
                    )
                    
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol, res = 20)

# Example 4: Stuart Russell's 3x4 Maze using
#    order-1 Fourier basis for approximation
#    and eligibility traces: Sarsa(lambda)

set.seed(1000)

## TODO: The following example does not converge to 1!
data(Maze)
sol <- solve_MDP_APPROX(Maze, horizon = 100, n = 100,
                    alpha = schedule_exp(0.3, .01),
                    epsilon = schedule_exp(1, .1),
                    lambda = 0.1,
                    transformation = transformation_fourier_basis, 
                    order = 1
                    )
                    
gw_plot(sol)
gw_matrix(sol, what = "value")
approx_V_plot(sol, res = 20)
 
}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.
}
\seealso{
Other solver: 
\code{\link{schedule}},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_DP}()},
\code{\link{solve_MDP_LP}()},
\code{\link{solve_MDP_MC}()},
\code{\link{solve_MDP_PG}()},
\code{\link{solve_MDP_SAMP}()},
\code{\link{solve_MDP_TD}()}

Other MDPTF: 
\code{\link{MDPTF}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{reachable_states}()},
\code{\link{sample_MDP.MDPTF}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_PG}()},
\code{\link{start}()}
}
\concept{MDPTF}
\concept{solver}
