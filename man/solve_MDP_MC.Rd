% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solve_MDP_MC.R
\name{solve_MDP_MC}
\alias{solve_MDP_MC}
\title{Solve MDPs using Monte Carlo Control}
\usage{
solve_MDP_MC(
  model,
  method = "exploring_starts",
  horizon = NULL,
  discount = NULL,
  n = 100,
  Q = NULL,
  epsilon = NULL,
  alpha = NULL,
  first_visit = TRUE,
  ...,
  matrix = TRUE,
  continue = FALSE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{method}{string; one of the following solution methods:
\itemize{
\item \code{'exploring_starts'} - on-policy MC control with exploring starts.
\item \code{'on_policy'} - on-policy MC control with an \eqn{\epsilon}-greedy policy.
\item \code{'off_policy"'} - off-policy MC control using an \eqn{\epsilon}-greedy behavior policy.
}}

\item{horizon}{an integer with the number of epochs for problems with a
finite planning horizon. If set to \code{Inf}, the algorithm continues
running iterations till it converges to the infinite horizon solution. If
\code{NULL}, then the horizon specified in \code{model} will be used.}

\item{discount}{discount factor in range \eqn{(0, 1]}. If \code{NULL}, then the
discount factor specified in \code{model} will be used.}

\item{n}{number of episodes used for learning.}

\item{Q}{an initial state-action value matrix. By default an all 0 matrix is
used.}

\item{epsilon}{used for the \eqn{\epsilon}-greedy behavior policies.}

\item{alpha}{step size as a function of the time step \code{t} and the number of times
the respective Q-value was updated \code{n} or a scalar. For expected Sarsa, alpha is
often set to 1.}

\item{first_visit}{if \code{TRUE} then only the first visit of a state/action pair
in an episode is used to update Q, otherwise, every-visit update is used.}

\item{...}{further parameters are passed on to the solver function.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or do not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time and memory requirements needed to calculate the matrices.}

\item{continue}{logical; show a progress bar with estimated time for completion.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical or a numeric verbose level; if set to \code{TRUE} or \code{1}, the
function displays the used algorithm parameters and progress information.
Levels \verb{>1} provide more detailed solver output in the R console.}
}
\value{
\code{solve_MDP()} returns an object of class MDP which is a list with the
model specifications (\code{model}), the solution (\code{solution}).
The solution is a list with the elements:
\itemize{
\item \code{policy} a list representing the policy graph. The list only has one
element for converged solutions.
\item \code{converged} did the algorithm converge (\code{NA}) for finite-horizon problems.
\item \code{delta} final \eqn{\delta} (value iteration and infinite-horizon only)
\item \code{iterations} number of iterations to convergence (infinite-horizon only)
}
}
\description{
Solve MDPs using Monte Carlo control.
}
\details{
The idea is to estimate the action value function for a policy as the
average of sampled returns.

\deqn{q_\pi(s,a) = \mathbb{E}_\pi[R_i|S_0=s,A_0=a] \approx \frac{1}{n} \sum_{i=1}^n R_i}

Monte Carlo control simulates a whole episode using the current behavior
policy and uses the sampled reward to update the Q values. For on-policy
methods, the behavior policy is updated to be greedy (i.e., optimal) with
respect to the new Q values. Then the next episode is simulated till
the predefined number of episodes is completed.

Implemented are the following temporal difference control methods
described in Sutton and Barto (2018).
\itemize{
\item \strong{Monte Carlo Control with exploring Starts} learns the optimal greedy policy.
It uses the same greedy policy for
behavior and target (on-policy learning).
After each episode, the policy is updated to be greedy with respect to the
current Q values.
To make sure all states/action pairs are
explored, it uses exploring starts meaning that new episodes are started at a randomly
chosen state using a randomly chooses action.
\item \strong{On-policy Monte Carlo Control} learns an epsilon-greedy policy
which it uses for behavior and as the target policy
(on-policy learning). An epsilon-greedy policy is used to provide
exploration. For calculating running averages, an update with \eqn{\alpha = 1/n}
is used by default. A different update factor can be set using the parameter \code{alpha}
as either a fixed value or a function with the signature \verb{function(t, n)}
which returns the factor in the range \eqn{[0,1]}.
\item \strong{Off-policy Monte Carlo Control} uses for behavior an arbitrary soft policy
(a soft policy has in each state a probability greater than 0 for all
possible actions).
We use an epsilon-greedy policy and the method learns a greedy policy using
importance sampling. Note: This method can only learn from the tail of the
sampled runs where greedy actions are chosen. This means that it is very
inefficient in learning the beginning portion of long episodes. This problem
is especially problematic when larger values for \eqn{\epsilon} are used.
}
}
\references{
Sutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. The MIT Press. \url{http://incompleteideas.net/book/the-book-2nd.html}.
}
