% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_evaluation.R, R/policy_evaluation_LP.R
\name{policy_evaluation}
\alias{policy_evaluation}
\alias{policy_evaluation_LP}
\title{Policy Evaluation}
\usage{
policy_evaluation(
  model,
  pi = NULL,
  V = NULL,
  k_backups = 1000L,
  theta = 0.001,
  progress = TRUE,
  verbose = FALSE
)

policy_evaluation_LP(model, pi = NULL, inf = 1000, verbose = FALSE, ...)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{pi}{a policy as a data.frame with at least columns for states and action. If \code{NULL},
then the policy in model is used.}

\item{V}{a vector with estimated state values representing a value function.
If \code{model} is a solved model, then the state
values are taken from the solution.}

\item{k_backups}{number of look ahead steps used for approximate policy evaluation
used by the policy iteration method. Set k_backups to \code{Inf} to only use
\eqn{\theta} as the stopping criterion.}

\item{theta}{stop when the largest state Bellman error (\eqn{\delta = V_{k+1} - V})
is less than \eqn{\theta}.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical; should progress and approximation errors be printed.}

\item{inf}{value used to replace infinity for \code{lpSolve::lp()}.}

\item{...}{further arguments are ignored}
}
\value{
a vector with (approximate)
state values (U).
}
\description{
Estimate the value function for a policy applied to a
model by repeatedly applying the Bellman operator.
}
\details{
The value function for a policy can be estimated (called policy evaluation)
by repeatedly applying the Bellman operator
\deqn{v \leftarrow B_\pi(v)}
till convergence.

In each iteration, all state values are updated. In this implementation updating is
stopped when the largest state Bellman error is below a threshold.

\deqn{||v_{k+1} - v_k||_\infty < \theta.}

Or if \code{k_backups} iterations have been completed.
}
\examples{
data(Maze)
Maze

# create several policies:
# 1. optimal policy using value iteration
maze_solved <- solve_MDP(Maze, method = "value_iteration")
pi_opt <- policy(maze_solved)
pi_opt

# 2. a manual policy (go up and in some squares to the right)
acts <- rep("up", times = length(Maze$states))
names(acts) <- Maze$states
acts[c("s(1,1)", "s(1,2)", "s(1,3)")] <- "right"
pi_manual <- manual_policy(Maze, acts)
pi_manual

# 3. a random policy
set.seed(1234)
pi_random <- random_policy(Maze)
pi_random

# 4. an improved policy based on one policy evaluation and
#   policy improvement step.
V <- policy_evaluation(Maze, pi_random)
Q <- Q_values(Maze, V)
pi_greedy <- greedy_policy(Q)
pi_greedy

#' compare the approx. value functions for the policies (we restrict
#'    the number of backups for the random policy since it may not converge)
rbind(
  random = policy_evaluation(Maze, pi_random, k_backups = 100),
  manual = policy_evaluation(Maze, pi_manual),
  greedy = policy_evaluation(Maze, pi_greedy),
  optimal = policy_evaluation(Maze, pi_opt)
)
}
\references{
Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other policy: 
\code{\link{Q_values}()},
\code{\link{action}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{policy}()},
\code{\link{regret}()},
\code{\link{reward}()},
\code{\link{value_function}()},
\code{\link{visit_probability}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{policy}
