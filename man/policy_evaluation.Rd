% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_evaluation.R
\name{policy_evaluation}
\alias{policy_evaluation}
\title{Policy Evaluation}
\usage{
policy_evaluation(
  model,
  pi = NULL,
  V = NULL,
  k_backups = 1000L,
  theta = 0.001,
  matrix = TRUE,
  progress = TRUE,
  verbose = FALSE
)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{pi}{a policy as a data.frame with at least columns for states and action. If \code{NULL},
then the policy in model is used.}

\item{V}{a vector with estimated state values representing a value function.
If \code{model} is a solved model, then the state
values are taken from the solution.}

\item{k_backups}{number of look ahead steps used for approximate policy evaluation
used by the policy iteration method. Set k_backups to \code{Inf} to only use
\eqn{\theta} as the stopping criterion.}

\item{theta}{stop when the largest state Bellman error (\eqn{\delta = V_{k+1} - V})
is less than \eqn{\theta}.}

\item{matrix}{logical; if \code{TRUE} then matrices for the transition model and
the reward function are taken from the model first. This can be slow if functions
need to be converted or not fit into memory if the models are large. If these
components are already matrices, then this is very fast. For \code{FALSE}, the
transition probabilities and the reward is extracted when needed. This is slower,
but removes the time to calculate the matrices and it saves memory.}

\item{progress}{logical; show a progress bar with estimated time for completion.}

\item{verbose}{logical; should progress and approximation errors be printed.}
}
\value{
a vector with (approximate)
state values (U).
}
\description{
Estimate the value function for a policy applied to a
model by repeatedly applying the Bellman operator.
}
\details{
The value function for a policy can be estimated (called policy evaluation)
by repeatedly applying the Bellman operator \eqn{B_\pi} till convergence.
In each iteration, all state values are updated. In this implementation updating is
stopped when the largest state Bellman error is below a threshold.

\deqn{||V_{k+1} - V_k||_\infty < \theta.}

Or if \code{k_backups} iterations have been completed.
}
\examples{
data(Maze)
Maze

# create several policies:
# 1. optimal policy using value iteration
maze_solved <- solve_MDP(Maze, method = "value_iteration")
pi_opt <- policy(maze_solved)
pi_opt

# 2. a manual policy (go up and in some squares to the right)
acts <- rep("up", times = length(Maze$states))
names(acts) <- Maze$states
acts[c("s(1,1)", "s(1,2)", "s(1,3)")] <- "right"
pi_manual <- manual_policy(Maze, acts)
pi_manual

# 3. a random policy
set.seed(1234)
pi_random <- random_policy(Maze)
pi_random

# 4. an improved policy based on one policy evaluation and
#   policy improvement step.
V <- policy_evaluation(Maze, pi_random)
Q <- Q_values(Maze, V)
pi_greedy <- greedy_policy(Q)
pi_greedy

#' compare the approx. value functions for the policies (we restrict
#'    the number of backups for the random policy since it may not converge)
rbind(
  random = policy_evaluation(Maze, pi_random, k_backups = 100),
  manual = policy_evaluation(Maze, pi_manual),
  greedy = policy_evaluation(Maze, pi_greedy),
  optimal = policy_evaluation(Maze, pi_opt)
)
}
\references{
Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{accessors}},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{gridworld}},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other policy: 
\code{\link{Q_values}()},
\code{\link{action}()},
\code{\link{bellman_update}()},
\code{\link{policy}()},
\code{\link{reward}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{policy}
