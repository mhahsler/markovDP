% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/accessors.R, R/accessors_reward.R,
%   R/accessors_transitions.R
\name{accessors}
\alias{accessors}
\alias{start_vector}
\alias{normalize_MDP}
\alias{reward_matrix}
\alias{reward_val}
\alias{transition_matrix}
\alias{transition_val}
\title{Access to Parts of the Model Description}
\usage{
start_vector(x, start = NULL)

normalize_MDP(
  x,
  sparse = TRUE,
  trans_start = FALSE,
  trans_function = TRUE,
  trans_keyword = FALSE
)

reward_matrix(
  x,
  action = NULL,
  start.state = NULL,
  end.state = NULL,
  observation = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = FALSE
)

reward_val(
  x,
  action,
  start.state,
  end.state = NULL,
  observation = NULL,
  episode = NULL,
  epoch = NULL
)

transition_matrix(
  x,
  action = NULL,
  start.state = NULL,
  end.state = NULL,
  episode = NULL,
  epoch = NULL,
  sparse = FALSE,
  trans_keyword = TRUE
)

transition_val(x, action, start.state, end.state, episode = NULL, epoch = NULL)
}
\arguments{
\item{x}{A \link{MDP} object.}

\item{start}{a start state description (see \link{MDP}). If \code{NULL} then the
start vector is created using the start stored in the model.}

\item{sparse}{logical; use sparse matrices when the density is below 50\% and keeps data.frame representation
for the reward field. \code{NULL} returns the
representation stored in the problem description which saves the time for conversion.}

\item{trans_start}{logical; expand the start to a probability vector?}

\item{trans_function}{logical; convert functions into matrices?}

\item{trans_keyword}{logical; convert distribution keywords (uniform and identity)
in \code{transition_prob} matrices?}

\item{action}{name or index of an action.}

\item{start.state, end.state}{name or index of the state.}

\item{observation}{unused for MDPs needs to be \code{NULL}.}

\item{episode, epoch}{Episode or epoch used for time-dependent MDPs. Epochs are internally converted
to the episode using the model horizon.}
}
\value{
A list or a list of lists of matrices.
}
\description{
Functions to provide uniform access to different parts of the MDP
problem description.
}
\details{
Several parts of the MDP description can be defined in different ways. In particular,
the fields \code{transition_prob}, \code{reward}, and \code{start} can be defined using matrices, data frames,
keywords, or functions. See \link{MDP} for details.
The functions provided here, provide unified access to the data in these fields
to make writing code easier.
\subsection{Transition Probabilities \eqn{T(s'|s,a)}}{

\code{transition_matrix()} accesses the transition model. The complete model
is a list with one element for each action. Each element contains a states x states matrix
with \eqn{s} (\code{start.state}) as rows and \eqn{s'} (\code{end.state}) as columns.
Matrices with a density below 50\% can be requested in sparse format
(as a \link[Matrix:dgCMatrix-class]{Matrix::dgCMatrix}).
}

\subsection{Reward \eqn{R(s,s',a)}}{

\code{reward_matrix()} accesses the reward model.
The preferred representation is a data.frame with the
columns \code{action}, \code{start.state}, \code{end.state},
and \code{value}. This is a sparse representation.
The dense representation is a list of lists of matrices.
The list levels are \eqn{a} (\code{action})  and \eqn{s} (\code{start.state}).
The matrices are column vectors with rows representing \eqn{s'} (\code{end.state}).
The reward structure cannot be efficiently stored using a standard sparse matrix
since there might be a fixed cost for each action
resulting in no entries with 0.
}

\subsection{Start state}{

\code{start_vector()} translates the start state description into a probability vector.
}

\subsection{Convert the Complete MDP Description into a consistent form}{

\code{normalize_MDP()} returns a new MDP definition where \code{transition_prob},
\code{reward}, and \code{start} are normalized.

Also, \code{states}, and \code{actions} are ordered as given in the problem
definition to make safe access using numerical indices possible. Normalized
MDP descriptions can be
used in custom code that expects consistently a certain format.
}
}
\examples{
data("Maze")
gridworld_matrix(Maze)

# List of |A| transition matrices. One per action in the from start.states x end.states
Maze$transition_prob
transition_matrix(Maze)
transition_matrix(Maze, action = "up", sparse = TRUE)
transition_matrix(Maze, action = "up", 
                        start.state = "s(3,1)", end.state = "s(2,1)")

# List of list of reward matrices. 1st level is action and second level is the
#  start state in the form of a column vector with elements for end states.
Maze$reward
reward_matrix(Maze)
reward_matrix(Maze, sparse = TRUE)
reward_matrix(Maze, action = "up", 
              start.state = "s(3,1)", end.state = "s(2,1)")

# Translate the initial start probability vector
Maze$start
start_vector(Maze)

# Normalize the whole model using dense representation
Maze_norm <- normalize_MDP(Maze, sparse = FALSE)
Maze_norm$transition_prob
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{MDP_policy_functions}},
\code{\link{actions}()},
\code{\link{add_policy}()},
\code{\link{gridworld}},
\code{\link{reachable_and_absorbing}},
\code{\link{regret}()},
\code{\link{simulate_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
