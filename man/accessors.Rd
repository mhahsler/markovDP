% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/accessors_transitions.R, R/accessors_reward.R,
%   R/accessors.R
\name{transition_matrix}
\alias{transition_matrix}
\alias{reward_matrix}
\alias{accessors}
\alias{start_vector}
\alias{normalize_MDP}
\title{Access to Parts of the Model Description}
\usage{
transition_matrix(
  model,
  action = NULL,
  start.state = NULL,
  end.state = NULL,
  ...,
  sparse = NULL,
  drop = TRUE,
  simplify = FALSE,
  trans_keyword = TRUE
)

reward_matrix(
  model,
  action = NULL,
  start.state = NULL,
  end.state = NULL,
  ...,
  sparse = NULL,
  drop = TRUE,
  simplify = FALSE
)

start_vector(model, start = NULL, sparse = NULL)

normalize_MDP(
  model,
  transition_prob = TRUE,
  reward = TRUE,
  start = FALSE,
  sparse = NULL,
  precompute_absorbing = TRUE,
  check_and_fix = FALSE,
  progress = TRUE
)
}
\arguments{
\item{model}{A \link{MDP} object.}

\item{action}{name or index of an action.}

\item{start.state, end.state}{name or index of the state.}

\item{...}{further arguments are passed on.}

\item{sparse}{logical; use sparse matrix representation? \code{NULL} decides the representation
based on the memory it would take to store the faster dense representation.}

\item{drop}{logical; drop matrices to vectors when one row/one column is selected.}

\item{simplify}{logical; try to simplify action lists into a vector or matrix?}

\item{trans_keyword}{logical; translate keywords like "uniform" into matrices.}

\item{start}{logical; convert the start probability distribution into a vector.}

\item{transition_prob}{logical; convert the transition probabilities into a list of matrices.}

\item{reward}{logical; convert the reward model into a list of matrices.}

\item{precompute_absorbing}{logical; should absorbing states be precalculated?}

\item{check_and_fix}{logical; checks the structure of the problem description.}

\item{progress}{logical; show a progress bar with estimated time for completion.}
}
\value{
A list or a list of lists of matrices.
}
\description{
Functions to provide uniform access to different parts of the MDP
problem description.
}
\details{
Several parts of the MDP description can be defined in different ways. In particular,
the fields \code{transition_prob}, \code{reward}, and \code{start} can be defined using matrices, data frames,
keywords, or functions. See \link{MDP} for details.
The functions provided here, provide unified access to the data in these fields
to make writing code easier.
\subsection{Transition Probabilities \eqn{p(s'|s,a)}}{

\code{transition_matrix()} accesses the transition model. The complete model
is a list with one element for each action. Each element contains a states x states matrix
with \eqn{s} (\code{start.state}) as rows and \eqn{s'} (\code{end.state}) as columns.
Matrices with a low density can be requested in sparse format
(as a \link[Matrix:dgRMatrix-class]{Matrix::dgRMatrix}). It is recommended to load package \code{MatrixExtra}
to work with sparse matrices.
}

\subsection{Reward \eqn{r(s,s',a)}}{

\code{reward_matrix()} accesses the reward model.
The preferred representation is a data.frame with the
columns \code{action}, \code{start.state}, \code{end.state},
and \code{value}. This is a sparse representation.

The dense representation is a list of lists of matrices.
The list levels are \eqn{a} (\code{action})  and \eqn{s} (\code{start.state}).
The matrices are column vectors with rows representing \eqn{s'} (\code{end.state}).

To represent the rewards as a sparse matrices, \strong{rewards that correspond to a transition
with probability zero are zeroed out if the transition model} is stored as a list
of matrices. This makes the reward matrices as sparse as the transition matrices.
The function \code{normalize_MDP()} with \code{sparse = TRUE} will perform this representation.
}

\subsection{Start state}{

\code{start_vector()} translates the start state description into a probability vector.
}

\subsection{Sparse Matrices and Normalizing MDPs}{

Different components can be specified in various ways. It is often
necessary to convert each component into a specific form (e.g., a
dense matrix) to save time during access.
Convert the Complete MDP Description into a consistent form
\code{normalize_MDP()} converts all components of the MDP description
into a consistent form and
returns a new MDP definition where \code{transition_prob},
\code{reward}, and \code{start} are normalized. This includes the internal
representation (dense, sparse, as a data.frame) and
also, \code{states}, and \code{actions} are ordered as given in the problem
definition to make safe access using numerical indices possible. Normalized
MDP descriptions can be
used in custom code that expects consistently a certain format.

The default behavior of \code{sparse = NULL} uses parse matrices for large models
where the dense transition model would need more
than \code{options("MDP_SPARSE_LIMIT")} (the default is about 100 MB which
can be changed using
\code{\link[=options]{options()}}). Smaller models use faster dense
matrices.
}
}
\examples{
data("Maze")
gw_matrix(Maze)

# here is the internal structure of the Maze object
str(Maze)

# List of |A| transition matrices. One per action in the from start.states x end.states
Maze$transition_prob
transition_matrix(Maze)
transition_matrix(Maze, action = "up", sparse = FALSE)
transition_matrix(Maze,
  action = "up",
  start.state = "s(3,1)", end.state = "s(2,1)"
)

# List of list of reward matrices. 1st level is action and second level is the
#  start state in the form of a column vector with elements for end states.
Maze$reward
reward_matrix(Maze)
reward_matrix(Maze, sparse = TRUE)
reward_matrix(Maze,
  action = "up",
  start.state = "s(3,1)", end.state = "s(2,1)"
)

# Translate the initial start probability vector
Maze$start
start_vector(Maze, sparse = FALSE)
start_vector(Maze, sparse = "states")
start_vector(Maze, sparse = "index")

# Normalize the whole model using sparse representation
Maze_norm <- normalize_MDP(Maze, sparse = TRUE)
str(Maze_norm)

# Note to make the reward matrix sparse, all rewards
# for transitions with probability of 0 are zeroed out.
reward_matrix(Maze_norm)
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{Q_values}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{reachable_states}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{solve_MDP_APPROX_LAMBDA}()},
\code{\link{start}()},
\code{\link{transition_graph}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
