% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Q_values.R
\name{Q_values}
\alias{Q_values}
\alias{Q_zero}
\alias{Q_random}
\title{Q-Values}
\usage{
Q_values(model, V = NULL)

Q_zero(model, value = 0)

Q_random(model, min = 1e-06, max = 1)
}
\arguments{
\item{model}{an MDP problem specification.}

\item{V}{the state values. If \code{model} is a solved model, then the state
values are taken from the solution.}

\item{value}{value to initialize the Q-value matrix. Default is 0.}

\item{min, max}{range of the random values}
}
\value{
\code{Q_values()} returns a state by action matrix specifying the Q-function,
i.e., the action value for executing each action in each state. The Q-values
are calculated from the value function (U) and the transition model.

\code{Q_zero()} and \code{Q_random} return a matrix with q-values.
}
\description{
Several useful functions to deal with Q-values (action values)
which map each state/action pair to a utility value.
}
\details{
Implemented functions are:
\itemize{
\item \code{Q_values()} approximates
Q values for a given model and value function using the Bellman
optimality equation:

\deqn{q_*(s,a) = \sum_{s'} p(s'|s,a) [r(s,a,s') + \gamma v_*(s')]}

Exact Q values are calculated if \eqn{v = v_*}, the optimal value function,
otherwise we get an approximation that might not
be consistent with \eqn{v} or the implied policy.
Q values can be used as the input for several other functions.
\item \code{Q_zero()} and \code{Q_random()} create initial Q value matrices for algorithms.
}
}
\examples{
data(Maze)
Maze

# create a random policy and calculate q-values
pi_random <- random_policy(Maze)
pi_random

V <- policy_evaluation(Maze, pi_random)
V

# calculate Q values
Q <- Q_values(Maze, V)
Q
}
\references{
Sutton, R. S., Barto, A. G. (2020). Reinforcement Learning: An Introduction.
Second edition. The MIT Press.
}
\seealso{
Other MDP: 
\code{\link{MDP}()},
\code{\link{absorbing_states}()},
\code{\link{act}()},
\code{\link{available_actions}()},
\code{\link{bellman_update}()},
\code{\link{convergence_horizon}()},
\code{\link{greedy_action}()},
\code{\link{gridworld}},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{sample_MDP}()},
\code{\link{solve_MDP}()},
\code{\link{transition_graph}()},
\code{\link{transition_matrix}()},
\code{\link{unreachable_states}()},
\code{\link{value_function}()}

Other policy: 
\code{\link{action}()},
\code{\link{bellman_update}()},
\code{\link{greedy_action}()},
\code{\link{policy}()},
\code{\link{policy_evaluation}()},
\code{\link{regret}()},
\code{\link{reward}()},
\code{\link{value_function}()},
\code{\link{visit_probability}()}
}
\author{
Michael Hahsler
}
\concept{MDP}
\concept{policy}
