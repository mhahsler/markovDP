---
title: "markovDP: Discrete-Time Markov Decision Processes (MDPs)"
author: "Michael Hahsler"
bibliography: references.bib
link-citations: yes
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{markovDP: Discrete-Time Markov Decision Processes (MDPs)}
  %\VignetteEngine{knitr::rmarkdown}
output:
  rmarkdown::html_vignette
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library("markovDP")
```

# Introduction

The package **markovDP** provides the infrastructure to work with discrete-time Markov
Decision Processes (MDPs) [@Bellman1957; @Howard1960] in R. The focus is on convenience in formulating MDPs,
the support of sparse representations (using sparse matrices, lists and
data.frames) and visualization of results. Some key components are implemented
in C++ to speed up computation. 
It also provides to the following popular solving procedures:

* __Dynamic Programming__
  - Value Iteration [@Bellman1957]
  - Modified Policy Iteration [@Howard1960; @Puterman1978]
  - Prioritized Sweeping [@Moore1993]

* __Linear Programming__
  - Primal Formulation [@Manne1960]

* __Monte Carlo Control__
  - Monte Carlo Control with Exploring Starts [@Sutton1998]
  - On-policy Monte Carlo Control [@Sutton1998]
  - Off-policy Monte Carlo Control [@Sutton1998]

* __Termporal Differencing__
  - Q-Learning [@Watkins1992]
  - Sarsa [@Rummery1994]
  - Expected Sarsa [@Sutton1998]
  
* __Sampling__
  - Random-sample one-step tabular Q-planning [@Sutton1998]

The implementations follow the description is [@Russell2020] and 
[@Sutton1998] closely.
The package is intended to solve small to medium sized 
problems, to teach how the different methods work and to develop and test 
new algorithms.

**markovDP** provides an alternative implementation to the existing R package
**MDPToolbox** [@CRAN_MDPtoolbox]. The main difference is that **markovDP**
has a stronger focus on visualizing models and results and is designed with 
compatibility with the package **pomdp** for Partially Observable Markov 
Processes [@CRAN_pomdp] in mind. We also implement some reinforcement learning 
algorithms (e.g., Q-learning) to solve MDP models. Here the MDP is defines the
simulated environment the agent interacts with.
Reinforcement learning algorithm implementations
can also be found in the R package **ReinforcementLearning** [@Proellochs2020] 
which focuses on model-free learning from pre-defined observations
or interaction with an environment.

In this document, we will give a very brief introduction to the concept
of MDPs, describe the features of the R package **markovDP**, and illustrate the
usage with a toy example.

# Markov Decision Processes

A Markov decision process (MDP) [@Bellman1957; @Howard1960]
is a discrete-time stochastic control
process. In each time step, an agent can perform an action which affect
the system (i.e., may cause the system state to change). The agent's
goal is to maximize its expected future rewards that depend on the
sequence of system states and the agent's actions in the future. The
goal is to find the optimal policy that guides the agent's actions.

The MDP framework is general enough to model a variety of real-world
sequential decision-making problems. Applications include robot
navigation problems, machine maintenance, and planning under uncertainty
in general.

A discrete-time POMDP can formally be described as a 7-tuple
$$\mathcal{P} = (S, A, T, R, \gamma),$$ where

-   $S = \{s_1, s_2, \dots, s_n\}$ is a set of fully observable states,

-   $A = \{a_1, a_2, \dots, a_m\}$ is a set of actions,

-   $T$ a set of conditional transition probabilities $T(s' \mid s,a)$
    for the state transition $s \rightarrow s'$ conditioned on the taken
    action.

-   $R: S \times A \rightarrow \mathbb{R}$ is the reward function,

-   $\gamma \in [0, 1]$ is the discount factor.

At each time period, the environment is in some known state $s \in S$.
The agent chooses an action $a \in A$, which causes the environment to
transition to state $s' \in S$ with probability $T(s' \mid s,a)$ and the
agent receives the reward $R(s,a)$. This process repeats for each time
period. The goal is for the agent to choose actions that maximizes the
expected sum of discounted future rewards, i.e., she chooses the actions
at each time $t$ that
$$\max \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right].$$

For a finite time horizon, only the expectation over the sum up to the
time horizon is used.

# Package Functionality

Solving a MDP problem with the **markovDP** package consists of two
steps:

1.  Define a MDP problem using the function `MDP()`, and
2.  solve the problem using `solve_MDP()`.

## Defining a MDP Problem

The `MDP()` function has the following arguments, each corresponds to
one of the elements of a MDP.

```{r}
str(args(MDP))
```

where

-   `states` defines the set of states $S$,

-   `actions` defines the set of actions $A$,

-   `transition_prob` defines the conditional transition probabilities
    $T(s' \mid s,a)$,

-   `reward` specifies the reward function $R$,

-   `discount` is the discount factor $\gamma$ in range $[0,1]$,

-   `horizon` is the problem horizon as the number of periods to
    consider.

-   `start` The state in which the agent starts. It can be specified as
    a probability distribution over the system states $S$.

While specifying the discount rate and the set of states, and actions is
straight-forward. Some arguments can be specified in different ways. The
initial state `start` can be specified as

-   A vector of $n$ probabilities that add up to 1, where $n$ is the
    number of states.

    ```{r, eval = FALSE}
    start <- c(0.5, 0.3, 0.2)
    ```

-   The string `"uniform"` for a uniform distribution over all states.

    ```{r, eval = FALSE}
    start <- "uniform"
    ```

-   A vector of integer indices specifying a subset as start states. The
    initial probability is uniform over these states. For example, start
    only in state 3 or start in state 1 and 3:

    ```{r, eval = FALSE}
    start <- 3
    start <- c(1, 3)
    ```

-   A vector of strings specifying a subset as equally likely start
    states.

    ```{r, eval = FALSE}
    start <- "state3"
    start <- c("state1", "state3")
    ```

-   A vector of strings starting with `"-"` specifying which states to
    exclude from the uniform initial probability distribution.

    ```{r, eval = FALSE}
    start <- c("-", "state2")
    ```

The transition probabilities (`transition_prob`) and reward function
(`reward`) can be specified in several ways:

-   As a `data.frame` created using `rbind()` and the helper functions
    `T_()` and `R_()`. This is the preferred and most sparse
    representation for most problems.
-   A named list of matrices representing the transition probabilities
    or rewards. Each list elements corresponds to an action.
-   A function with the same arguments `T_()` or `R_()` that returns the
    probability or reward.

More details can be found in the manual page for `MDP()`.

## Solving a MDP

POMDP problems are solved with the function `solve_MDP()` with the
following arguments.

```{r}
str(args(solve_MDP))
```

The `model` argument is a MDP problem created using the `MDP()`
function. The
`method` argument specifies what algorithm the solver should use.
Available methods including `"value_iteration"`, `"policy_iteration"`,
`"q_learning"`, `"sarsa"`, `"expected_sarsa"` and several more.

# Toy Example: Steward Russell's 4x3 Maze Gridworld MDP

We will demonstrate how to use the package with the 4x3 Maze Gridworld
described in Chapter 17 of the textbook "Artificial Intelligence: A
Modern Approach" (AIMA) [@Russell2020]. The simple maze has the
following layout:

```         
    1234           Transition model:
   ######             .8 (action direction)
  1#   +#              ^
  2# # -#              |
  3#S   #         .1 <-|-> .1
   ######
```

We represent the maze states as a gridworld matrix with 3 rows and 4
columns. The states are labeled s(row, col) representing the position in
the matrix. The \# (state s(2,2)) in the middle of the maze is an
obstruction and not reachable. Rewards are associated with transitions.
The default reward (penalty) is -0.04 for each action taken. 
The start state marked with S is
s(3,1). Transitioning to + (state s(1,4)) gives a reward of +1.0,
transitioning to - (state s\_(2,4)) has a reward of -1.0. Both these
states are absorbing (i.e., terminal) states.

Actions are movements (up, right, down, left). The actions are
unreliable with a .8 chance to move in the correct direction and a 0.1
chance to instead to move in a perpendicular direction. This means that 
the maze has a stochastic transition model.

## Specifying the Maze

```{r}
library("markovDP")
```

After loading the library, we create the states using a gridworld helper 
function.

```{r}
gw <- gridworld_init(dim = c(3, 4), unreachable_states = c("s(2,2)"))
gw$states
gw$actions
```

The helper function also creates a deterministic transition function,
but we need to create our own stochastic transition function is stochastic.

```{r}
T <- function(action, start.state, end.state) {
  action <- match.arg(action, choices = gw$actions)

  # absorbing states
  if (start.state %in% c("s(1,4)", "s(2,4)")) {
    if (start.state == end.state) {
      return(1)
    } else {
      return(0)
    }
  }

  # actions are stochastic so we cannot use gw$trans_prob
  if (action %in% c("up", "down")) {
    error_direction <- c("right", "left")
  } else {
    error_direction <- c("up", "down")
  }

  rc <- gridworld_s2rc(start.state)
  delta <- list(
    up = c(-1, 0),
    down = c(+1, 0),
    right = c(0, +1),
    left = c(0, -1)
  )
  P <- matrix(0, nrow = 3, ncol = 4)

  add_prob <- function(P, rc, a, value) {
    new_rc <- rc + delta[[a]]
    if (!(gridworld_rc2s(new_rc) %in% gw$states)) {
      new_rc <- rc
    }
    P[new_rc[1], new_rc[2]] <- P[new_rc[1], new_rc[2]] + value
    P
  }

  P <- add_prob(P, rc, action, .8)
  P <- add_prob(P, rc, error_direction[1], .1)
  P <- add_prob(P, rc, error_direction[2], .1)
  P[rbind(gridworld_s2rc(end.state))]
}
```

Next, we specify the reward. We use a table. Every time a reward
is calculated, the last matching row will be used. We set the default
reward to -0.04 which will be used if no other row matches the start or end 
state. Then we set the reward for the terminal states. Finally, we make sure 
that staying in the terminal state does not add to the reward.



```{r}
R <- rbind(
  R_(value = -0.04),
  R_(end.state = "s(1,4)", value = +1),
  R_(end.state = "s(2,4)", value = -1),
  R_(start.state = "s(1,4)", value = 0),
  R_(start.state = "s(2,4)", value = 0)
)
```

Now, we can create the complete MDP problem. 
```{r}
Maze <- MDP(
  name = "Stuart Russell's 3x4 Maze",
  discount = 1,
  horizon = Inf,
  states = gw$states,
  actions = gw$actions,
  start = "s(3,1)",
  transition_prob = T,
  reward = R,
  info = list(
    gridworld_dim = c(3, 4),
    gridworld_labels = list(
      "s(3,1)" = "Start",
      "s(2,4)" = "-1",
      "s(1,4)" = "Goal: +1"
    )
  )
)

Maze
```

## Solving the Maze

We can solve the problem with the default solver method.

```{r}
sol <- solve_MDP(Maze)
sol
```

The output is an object of class MDP which contains the solution as an
additional list component. It indicates that the algorithm has converged to 
a stable solution.
The found policy shows for each state the utility (i.e., the value function) 
and the prescribed
action.

```{r}
policy(sol)
```


We can visualize the value function.

```{r}
plot_value_function(sol)
```

# Additional Functions

The package provides several functions to work with models and policies. In the 
following, we will organize them into categories. More details about each
function, its parameters, and examples can be found in the manual pages.

## Access to Model Components

Components of the model are stored in a list and can be accessed directly.

```{r }
str(sol, max.level = 2)
```

A special list element in `"solution"` which is only available when the model
already contains a policy.

To access components more easily, several accessor functions are available:

* `actions()` find available actions for a state.
* `reward_matrix()` access the reward structure,
* `start_vector()` access the initial state probabilities.
* `transition_matrix()` access transition probabilities.
* `transition_graph()` converts the transition matrix into a 
   graph for visualization.
* `policy()` extracts the policy for a solved model.

These functions can efficiently retrieve individual values and convert 
the components into sparse and dense representations.

## Value Function

* `value_function()` extracts the value function from a solved MDP.
* `q_values()` calculates (approximates) Q-values for a given model and value function.

## Policy

Policies in this package are deterministic policies with one prescribed 
action per state. They are represented as a data.frame with columns for:

* `state`: The state.
* `U`: The state's value (discounted expected utility U) if the policy is followed.
* `action`: The prescribed action.

Policies are typically creates using `solve_MDP()` and then stored in the
`"solution"` component of the returned model. Policies can also be created
by the following functions:

* `random_policy()` create a random policy.
* `manual_policy()` specify a policy data.frame manually.
* `greedy_policy()` generates a greedy policy using Q-values.

A policy can be added to a model for the use in other functions using 
`add_policy()`.

The action prescribed by a model with a policy can be calculated
using `action()`. From a matrix with Q-values, `greedy_action()`
can be used to find the best action.

The value function for a policy applied to a model can be estimated 
using `policy_evaluation()`.

## Evaluation

MDP policies can be evaluated using:

* `reward()` calculates the expected reward of a policy
* `regret()` calculates the regret of a policy relative to a benchmark policy.

## Simulation

Trajectories through a MDPs are created using `simulate_MDP()`

The outcome of single actions can be calculated by
`act()`. 


## Acknowledgments

Development of this package was supported in part by
National Institute of Standards and Technology (NIST) under grant number
[60NANB17D180](https://www.nist.gov/ctl/pscr/safe-net-integrated-connected-vehicle-computing-platform).

# References
